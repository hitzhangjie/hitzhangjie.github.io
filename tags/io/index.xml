<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>io on</title><link>/tags/io/</link><description>Recent content in io on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 09 Sep 2021 14:40:13 +0800</lastBuildDate><atom:link href="/tags/io/index.xml" rel="self" type="application/rss+xml"/><item><title>常见的零拷贝优化技术</title><link>/blog/2021-09-09-%E5%B8%B8%E8%A7%81%E7%9A%84%E9%9B%B6%E6%8B%B7%E8%B4%9D%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/</link><pubDate>Thu, 09 Sep 2021 14:40:13 +0800</pubDate><guid>/blog/2021-09-09-%E5%B8%B8%E8%A7%81%E7%9A%84%E9%9B%B6%E6%8B%B7%E8%B4%9D%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/</guid><description>在关注IO性能时我们经常听到零拷贝，那么零拷贝到底是什么呢？为什么要做零拷贝？又有哪些方案？本文就一起来看下。
零拷贝技术一般可以分为两类 # devices（disk、nic）和kernel buffer之间的数据拷贝，一般可以通过DMA（直接存储器访问）来优化掉，既能够避免中断CPU减轻CPU负载，也能够直接读写内存减少数据从nic到cpu再到kernel buffer的拷贝动作； kernel buffer和application buffer之间的数据拷贝； 说优化拷贝一般是去优化cpu拷贝，DMA拷贝是无法避免的。零拷贝则强调的是kernel buffer和application buffer之间的拷贝，零拷贝并不是说整个过程中完全有没有数据拷贝，在kernel space还是发生拷贝，当然下面提到的sendfile+DMA硬件支持分散读/聚集写情况下能优化掉kernel buffer之间的拷贝。
然后明确下优化拷贝的原因 # 使用系统调用的次数影响到上下文切换次数，上下文切换会带来一定的开销，如read、write组合起来完成磁盘数据读取、网络发送，就要切换4次，而且read、write要重复很多次，上下文切换开销就不能完全忽视； 数据拷贝，主要是说利用cpu来拷贝，cpu势必要中断原来的任务去做拷贝的事情，move来move去，干了些杂活，理想情况下是希望尽可能做更多的事，当然不一定能完全避免cpu拷贝，但是能让拷贝的数据量减少点还是值得的； 零拷贝优化一方面是要优化掉kernel buffer和application buffer的数据拷贝问题，一方面也要考虑下如何尽可能减少cpu拷贝对程序停顿的影响。
常见的拷贝优化方案 # 这里解决kernel buffer和application buffer之间数据拷贝的常用办法有以下几种，以读取磁盘数据发送到socket为例说明：
memory mapping # mmap系统调用，read的时候，dma从磁盘发送数据到kernel buffer，mmap根据fd映射对应的kernel buffer和application buffer，省掉一次考拷贝。write的时候，数据从kernel buffer直接拷贝到socket buffer再到nic buffer;
shared buffers in kernel memory space # 这里希望能再次优化掉mmap方案中write时从kernel buffer到nic buffer的拷贝，在kernel space中建立一个共享内存区域buf，dma传送数据到这个buf的b_data指针指向的位置，write的时候使用dma从这个buf的m_data位置开始写，其实b_data、m_data共享了底层内存区域。相当于一个写指针、一个读指针。通过这种方式优化掉了kernel space到nic space的拷贝；
shared buffers between user and kernel space # linux sk_buffers结构，其中有个指针记录着要发送的数据（application buffer中）的地址，避免了从application buffer到kernel buffer的拷贝；
different system calls, sendfile, splice, etc # 先说sendfile，sendfile允许在fd之间直接传送数据，进出sendfile上下文切换只需两次，数据直接从源fd对应的kernel buffer（dma从设备上拷贝过去）到目的socketfd的socket buffer拷贝，完全绕过了应用程序buffer及其拷贝；</description></item><item><title>Linux常见IO模型</title><link>/blog/2017-05-02-linux-common-io-model/</link><pubDate>Tue, 02 May 2017 21:42:13 +0800</pubDate><guid>/blog/2017-05-02-linux-common-io-model/</guid><description>目前Linux下可用的IO模型有5种，分别为阻塞IO、非阻塞IO、IO多路复用、信号驱动IO、异步IO，其中较为成熟且高效、稳定的是IO多路复用模型，因此当前众多网络服务程序几乎都是采用这种IO操作策略。
当一个应用程序读写（以读为例）某端口数据时，选择不同IO模型的应用程序，其执行流程也将不同。下面将对选择这5种不同IO模型时的程序的执行情形进行分析，以便了解使用IO复用模型的运行情况和性能优势。
一个完整经典的应用程序的数据读取操作可以看做两步：
等待数据准备好； 将数据从内核复制到应用程序进程； 1. 阻塞IO模型 # 最流行的IO模型是阻塞IO（Blocking IO）模型，几乎所有刚开始学习IO操作的人员都是使用这个模型，虽然它存在一定的性能缺陷，但是它的确很简单。
如下图所示，是利用该模型读取IO端口数据的典型流程。在有些情况下，当系统调用发现用户请求的IO操作不能立刻完成时（比如对IO写操作，缓冲区没有空闲空间或者空闲空间少于待写的数据量；而对于读操作，缓冲区中没有数据可读或者可读数据少于用户请求的数据量），则当前的进程会进入睡眠，也就是进程被IO读写阻塞。但是当数据可以写出或者有数据可供读入时（其他进程或线程从缓冲区中读走了数据后或者向缓冲区写入了数据），系统将会产生中断，唤醒在缓冲区上等待相应事件的进程继续执行。
备注：
有必要在这里进一步解释一下“阻塞IO”的含义。通过阻塞IO系统调用进行IO操作时，以read为例，在内核将数据拷贝到用户程序完成之前，Linux内核会对当前read请求操作的缓冲区（内存中的特殊区域）进行加锁，并且会将调用read的进程的状态设置为 “uninterruptible wait”状态（不可中断等待状态），处于该状态的进程将无法参与进程调度。能够参与进程调度的进程的状态必须是处于running状态的进程或者有信号到达的处于interruptible wait状态（可中断等待状态）的进程。当read操作完成时，内核会将对应的缓冲块解锁，然后发出中断请求，内核中的中断服务程序会将阻塞在该缓冲块上的进程的状态修改为running状态以使其重新具备参与进程调度的能力。
2. 非阻塞IO模型 # 在有些时候并不希望进程在IO操作未完成时睡眠，而是希望系统调用能够立刻返回一个错误，以报告这一情况，然后进程可以根据需要在适当的时候再重新执行这个IO操作。这就是所谓的非阻塞IO模型。
如下图所示，应用程序前几次read系统调用时都没有数据可供返回，此时内核立即返回一个EAGAIN错误代码，程序并不睡眠而是继续调用read，当第四次调用read时数据准备好了，于是执行数据从内核到用户空间的复制操作并成功返回，应用程序接着处理数据。这种对一个非阻塞IO端口反复调用read进行数据读取的动作称为轮询，即应用程序持续轮询内核数据是否准备好。这里的持续轮询操作将导致耗费大量的CPU时间，因此该模型并不推荐使用。
3. IO多路复用模型 # 前面介绍了非阻塞IO模型的问题在于，尽管应用程序可以在当前IO操作不能完成的时候迫使系统调用立刻返回而不至于睡眠，但是却无法知道什么时候再次请求IO操作可以顺利完成，只能周期性地做很多无谓的轮询，每隔一段时间就要重新请求一次系统调用，这种轮询策略极大浪费了CPU时间。
IO多路复用模型就是在此之上的改进，它的好处在于使得应用程序可以同时对多个IO端口进行监控以判断其上的操作是否可以顺利（无阻塞地）完成，达到时间复用的目的。进程阻塞在类似于select、poll或epoll这样的系统调用上，而不是阻塞在真正的IO系统调用上，意思也就是说在这些select、poll或者epoll函数内部会代替我们做非阻塞地轮询，那么它的轮询策略是怎样地呢？稍后会进行介绍。
select、poll或epoll使得进程可以在多个IO端口上等待IO事件（可读、可写、网络连接请求等）的发生，当有事件发生时再根据发生事件的类型进行适当的IO处理。不过进程在等待IO事件发生时仍然在代码执行序上处于“阻塞”状态，应用程序“阻塞”在这里照样还是无法做其他的工作（尽管可以指定轮询时等待时间的长短）。因此如果希望进程在没有IO事件要处理时还能做其他的工作，这种模型是不可行的。
下图是IO多路复用模型的示例。
IO多路复用模型主要有3种实现形式，select、poll、epoll。
3.1. select # #include &amp;lt;sys/select.h&amp;gt; //返回值：readfds、writefds、exceptfds中事件就绪的fd的数量 int select(int nfds, // 最大文件描述符fd+1 fd_set *restrict readfds, // 等待读取的fds fd_set *restrict writefds, // 等待写入的fds fd_set *restrict exceptfds, // 异常fds struct timeval *restrict timeout); // 超时时间 //返回值：readfds、writefds、exceptfds中事件就绪的fd的数量 int pselect(int nfds, // 最大文件描述符fd+1 fd_set *restrict readfds, // 等待读取的fds fd_set *restrict writefds, // 等待写入的fds fd_set *restrict exceptfds, // 异常fds const struct timespec *restrict timeout, // 超时时间 const sigset_t *restrict sigmask); // 信号掩码 备注：</description></item></channel></rss>