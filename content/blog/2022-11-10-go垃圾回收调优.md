---
layout: post
title: go垃圾回收调优
description: "go1.19支持了内存软限制，这个内存调优带来了一种新的解决方案。在项目实践中，我们也从压舱石方案切换成了GOMEMLIMIT的方案，过程中遇到的问题、思考，也梳理分享下吧。"
date: 2022-11-10 10:54:26 +0800
tags: ["go","GC","GOGC", "ballast","GC tuner","GOMEMLIMIT"]
toc: true
---



### **相关背景**

在go1.19之前，go程序内存调优的方式主要是通过环境变量GOGC（或者debug.SetGCPercent(?)）来控制，

它的效果是影响go runtime计算nextGC heapGoal的大小：

- 较早的版本计算方式为：**heapGoal = heapMarked + (heapMarked) \* GOGC / 100**，
- 后续go迭代时发现非堆内存也是有影响的，于是go1.18完善了下 **heapGoal = heapMarked + (heapMarked + GCROOTs) \* GOGC/100**，这里的**GCROOTS=(gstacks+globals)**

GC pacer的目的就是为了根据上述公式计算下次GC的heapGoal，然后在必要时（比如malloc时）决定是否要GC。

默认初始heapGoal大小为4MB，如果靠GOGC来控制的话，会比较频繁触发GC，对绝大多数server程序而言频繁GC占比较多CPU，程序整体吞吐、响应延迟会受一定影响。



所以业界一般会通过两种方式来调优：

- ballast，利用一块不用的大内存（比如1GB），来推高下次GC的heapGoal，通过这种方式来降低GC频率
- GC tuner，动态设置GOGC，就是结合一些机器负载信息来动态设置，这个是uber搞的但是没开源，具体怎么调的未知。



### 项目以前的方案

项目以前使用的是go1.16.5，这个版本中也只有GOGC一个控制GC的选项，使用的是ballast的方案：

- 在服务初始化阶段去初始化一个大内存而推高下次GC时的heapGoal
- 不同程序可能对内存需求不同，配置文件中允许自定义ballast大小，默认为1GB

包括业界在内都是介绍了ballast如何使用，有的是全局变量声明，有的是在main函数中声明…好像只看到了一派祥和，但是我们使用时却遇到了问题。



#### 问题1：ballast占物理内存

在测试环境（很多套测试环境）都有比较大概率发现服务在几乎空闲时，物理内存占用竟然高达1.1g…这很不符合常理。

- 通过pprof跟踪内存分配，发现内存分配比较大的路径就是这个压舱石（pprof mem采样是看的虚拟内存）。

- 然后top、pmap等跟踪可疑进程发现其确实存在1GB左右的anon区域，且该区域为dirty**（其实gdb把内存dump一看全是0，就很容易联想到类似对象分配后memset的操作）**。



根据了解的go GC、内存分配器相关的知识，了解到**go向操作系统申请内存时通过mmap的方式**，**释放内存是通过madvise+MADV_DONTNEED/MADV_FREE**的方式。

- go1.12的时候改成了FREE默认代替DONTNEED，这两个选项是有区别的，详细的可以看下man手册（man 2 madvise），FREE的效率更好一点，但是也有一些不好的副作用。

- go1.16之后linux下又恢复成了DONTNEED，因为FREE不会立即让进程的RSS降下来，会误导很多监控、开发运维人员

内存分配器为了提高分配效率、GC效率会进行一定的组织，这些概念大家应该有听说过，mheap、spanClass、mcentral、mspan、markbits、allocbits，还有p.mcache。

- smallSize<=32K的分配，走mcache（tinySize的更特殊一点，略）
- largeSize>32K对分配，走mheap

这些对象不管是从mcache来的还是从mheap直接分配的，最终都是建立在从操作系统申请来的page里的，而这些page是由page allocator申请的。即使是很大的对象，最终也是以存在span描述的区域里的。

一个span可以很多pages，而在里面分配一个对象时，这些page是可以复用的，如果之前期中一个page用过了，但是现在申请一个更大的对象，之前这个旧的page也是所需pages中的一个，那么分配器会先

清零memclr...这段内存区域，这样在操作系统看来就是要真正分配内存，并且因为写了0，那就是全为dirty。



ps：可能我们以为我mmap新申请的内存页面，不写不也是0，为什么不只写之前复用过的page呢？嗯，道理归道理，现在的分配器实现就是这么干的，详见allocNeedsZero(base, npages) bool这个函数。



结论：就是通过ballast这种方案，不是很可靠：

- 虽然通过一些尽早分配的办法可以避免上述问题，但是不太可靠。因为不确定哪天import了一个包，里面干了些内存分配比较多的操作，说不定影响到ballast占物理内存。
- 另外ballast这种方案不可移植，它是否占内存与特定平台也有一定关系，并不总是说mmap了一段内存过来不读写就不占内存，windows下的分配就是立即分配。



#### 问题2：可能引发OOM问题

本来是想通过ballast来在程序内存负载低时尽量减少GC活动，但是当内存负载高了之后，还是希望它能多清理下GC的，如果清理不及时就容易OOM。

我们可能会通过GOGC设置的小一点来频繁触发GC，但是总不是一个很可靠的办法，该设置一个什么值呢？

默认的GOGC=100可能并非最佳，这意味着内存占用高时GC活动不及时。当然我们可以调节，多试试，这让人联想到Java垃圾回收的各种调节参数，多了并不见得对开发、运维、部署是好事。



### 项目现在的方案

社区呼吁go能提供内存软限制，目标就是内存占用在达到限制前尽量减少GC活动，当内存占用高了（甚至超了）限制就更及时地GC，以让内存占用保持在一个合理的限制内。

这个想法其实就和大家最初使用压舱石的初衷比较接近了，也可以用这个方案GOMEMLIMIT/GOGC组合来达到此效果（关掉GOGC=off，指定软限制GOMEMLIMIT=2GiB）。



#### 详细了解下GOMEMLIMIT

和GOGC类似，我们也可以通过环境变量GOMEMLIMIT来指定软限制，也可以通过debug.SetMemoryLimit(?)来设置。

背景部分我们提到了GOGC如何影响下次GC时heapGoal的计算，GOMEMLIMIT也是通过影响下次GC时的heapGoal计算来发挥作用的，并且是各自根据GOGC、GOMEMLIMIT计算。

```go
goal = memoryLimit                               // p0
       - ((mappedReady - heapFree - heapAlloc)   // p1
       + max(mappedReady - memoryLimit, 0))      // p2
       - memoryLimitHeapGoalHeadroom             // p3
```



**解释下：**

- p0：memoryLimit：就是指定的软限制

- p1：noheap overheads

- p2：超出限制的部分

- p3：1mb的headroom（留点buffer）



那这里的goal和GOGC算出的goal，以哪个为准呢？**可以简单理解为以小的为准**，真实情况是有一点点微调，可以不关注。这样当内存占用达到这个goal时，就会触发GC了。



#### **项目推荐的GC设置**

首先，可以参考项目以前的ballast设置，比如ballast=1gb设置，现在默认值可以采用GOMEMLIMIT=2gb、GOGC=off。

效果是：

- 和压舱石类似，内存占用不多时，尽量减少GC开销；
- 不同的是，当内存占用比较多（可以理解为接近或者超过GOMEMLIMIT）时，会（更频繁）触发GC，以尽量内存占用符合这个软限制



**如果特定服务对大内存占用有需求，项目中也可以允许通过自定义配置项来解决，如：**

```yaml
go:
  gogc: -1
  gomemlimit: <bytes>
```



另外由于go支持通过读取环境变量GOGC、GOMEMLIMIT的方式来在一开始gcinit的时候进行设置，所以我们应该遵循这样的原则，这两个变量都应该独立遵守下面的原则：

- 环境变量配置优先级最高，这样符合go使用习惯
- 环境变量未指定时，读取配置文件中的自定义值
- 如果配置文件没指定，则使用默认值



### 再说GC Tuner

关于动态GC调优，uber有一个实现方案，但是现在我们没有看到具体的方案。不过大致也有思路，综合系统、进程内存占用、cpu占用情况来动态设置GOGC，来平衡CPU、内存占用情况。

我们应该认识到，GOMEMLIMIT的方案应该是ballast的比较好的替代方案，但是和自定义的GC Tuner比，可能灵活性还是没有GC Tuner高的，但是这个自定义要怎么个灵活法，真实业务中如何在各种服务里面得到普适应、广泛的应用，以及带来的价值有多大，也是需要在实践中考虑的。

本人在项目中没有去做类似的实践，有相关经验的朋友可以评论或者邮件私我分享下 :)



### **相关的注意事项**

其实不管是通过以前GOGC这个唯一控制项，还是现在GOGC+GOMEMLIMIT组合的方式，开发人员都应该对自身服务性能、资源占用有个清晰的认识。这就是说，在必要的部署机型下做压测应该常态化，这样才能在服务部署运维时有更清晰的认识，内存占用多少算是正常、不正常，负载多高应该选择扩容、缩容。

现在很多都已经容器化部署了，但是对于扩缩容依赖的CPU、MEM阈值要有认识。容器化部署隔离性好一点，如果存在混部，那对这里GOMEMLIMIT=?+GOGC=off还会有更好的认识，因为GC不及时可能导致混部的其他服务申请不到内存资源。

不管用那种GC调优，对服务自身的认识都是每一个开发人员所应该关注、提高的。