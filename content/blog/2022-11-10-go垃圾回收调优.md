---
layout: post
title: go垃圾回收调优
description: "go1.19支持了内存软限制，这个内存调优带来了一种新的解决方案。在项目实践中，我们也从压舱石方案切换成了GOMEMLIMIT的方案，过程中遇到的问题、思考，也梳理分享下吧。"
date: 2022-11-10 10:54:26 +0800
tags: ["go","GC","GOGC", "ballast","GC tuner","GOMEMLIMIT"]
toc: true
---



### **相关背景**

在go1.19之前，go程序内存调优的方式主要是通过环境变量GOGC（或者debug.SetGCPercent(?)）来控制，

它的效果是影响go runtime计算nextGC heapGoal的大小：

- 较早的版本计算方式为：**heapGoal = heapMarked + (heapMarked) \* GOGC / 100**，
- 后续go迭代时发现非堆内存也是有影响的，于是go1.18完善了下 **heapGoal = heapMarked + (heapMarked + GCROOTs) \* GOGC/100**，这里的**GCROOTS=(gstacks+globals)**

GC pacer的目的就是为了根据上述公式计算下次GC的heapGoal，然后在必要时（比如malloc时）决定是否要GC。

默认初始heapGoal大小为4MB，如果靠GOGC来控制的话，会比较频繁触发GC，对绝大多数server程序而言频繁GC占比较多CPU，程序整体吞吐、响应延迟会受一定影响。



所以业界一般会通过两种方式来调优：

- ballast，利用一块不用的大内存（比如1GB），来推高下次GC的heapGoal，通过这种方式来降低GC频率
- GC tuner，动态设置GOGC，就是结合一些机器负载信息来动态设置，这个是uber搞的但是没开源，具体怎么调的未知。



### 项目以前的方案

项目以前使用的是go1.16.5，这个版本中也只有GOGC一个控制GC的选项，使用的是ballast的方案：

- 在服务初始化阶段去初始化一个大内存而推高下次GC时的heapGoal
- 不同程序可能对内存需求不同，配置文件中允许自定义ballast大小，默认为1GB

包括业界在内都是介绍了ballast如何使用，有的是全局变量声明，有的是在main函数中声明…好像只看到了一派祥和，但是我们使用时却遇到了问题。



#### 问题1：ballast占物理内存

在测试环境（很多套测试环境）都有比较大概率发现服务在几乎空闲时，物理内存占用竟然高达1.1g…这很不符合常理。

- 通过pprof跟踪内存分配，发现内存分配比较大的路径就是这个压舱石（pprof mem采样是看的虚拟内存）。

- 然后top、pmap等跟踪可疑进程发现其确实存在1GB左右的anon区域，且该区域为dirty**（其实gdb把内存dump一看全是0，就很容易联想到类似对象分配后memset的操作）**。



根据了解的go GC、内存分配器相关的知识，了解到**go向操作系统申请内存时通过mmap的方式**，**释放内存是通过madvise+MADV_DONTNEED/MADV_FREE**的方式。

- go1.12的时候改成了FREE默认代替DONTNEED，这两个选项是有区别的，详细的可以看下man手册（man 2 madvise），FREE的效率更好一点，但是也有一些不好的副作用。

- go1.16之后linux下又恢复成了DONTNEED，因为FREE不会立即让进程的RSS降下来，会误导很多监控、开发运维人员

内存分配器为了提高分配效率、GC效率会进行一定的组织，这些概念大家应该有听说过，mheap、spanClass、mcentral、mspan、markbits、allocbits，还有p.mcache。

- smallSize<=32K的分配，走mcache（tinySize的更特殊一点，略）
- largeSize>32K对分配，走mheap

这些对象不管是从mcache来的还是从mheap直接分配的，最终都是建立在从操作系统申请来的page里的，而这些page是由page allocator申请的。即使是很大的对象，最终也是以存在span描述的区域里的。

一个span可以很多pages，而在里面分配一个对象时，这些page是可以复用的，如果之前期中一个page用过了，但是现在申请一个更大的对象，之前这个旧的page也是所需pages中的一个，那么分配器会先

清零memclr...这段内存区域，这样在操作系统看来就是要真正分配内存，并且因为写了0，那就是全为dirty。



ps：可能我们以为我mmap新申请的内存页面，不写不也是0，为什么不只写之前复用过的page呢？嗯，道理归道理，现在的分配器实现就是这么干的，详见allocNeedsZero(base, npages) bool这个函数。



结论：就是通过ballast这种方案，不是很可靠：

- 虽然通过一些尽早分配的办法可以避免上述问题，但是不太可靠。因为不确定哪天import了一个包，里面干了些内存分配比较多的操作，说不定影响到ballast占物理内存。
- 另外ballast这种方案不可移植，它是否占内存与特定平台也有一定关系，并不总是说mmap了一段内存过来不读写就不占内存，windows下的分配就是立即分配。



#### 问题2：可能引发OOM问题

本来是想通过ballast来在程序内存负载低时尽量减少GC活动，但是当内存负载高了之后，还是希望它能多清理下GC的，如果清理不及时就容易OOM。

我们可能会通过GOGC设置的小一点来频繁触发GC，但是总不是一个很可靠的办法，该设置一个什么值呢？

默认的GOGC=100可能并非最佳，这意味着内存占用高时GC活动不及时。当然我们可以调节，多试试，这让人联想到Java垃圾回收的各种调节参数，多了并不见得对开发、运维、部署是好事。



### 项目现在的方案

社区呼吁go能提供内存软限制，目标就是内存占用在达到限制前尽量减少GC活动，当内存占用高了（甚至超了）限制就更及时地GC，以让内存占用保持在一个合理的限制内。

这个想法其实就和大家最初使用压舱石的初衷比较接近了，也可以用这个方案GOMEMLIMIT/GOGC组合来达到此效果（关掉GOGC=off，指定软限制GOMEMLIMIT=2GiB）。



#### 详细了解下GOMEMLIMIT

和GOGC类似，我们也可以通过环境变量GOMEMLIMIT来指定软限制，也可以通过debug.SetMemoryLimit(?)来设置。

背景部分我们提到了GOGC如何影响下次GC时heapGoal的计算，GOMEMLIMIT也是通过影响下次GC时的heapGoal计算来发挥作用的，并且是各自根据GOGC、GOMEMLIMIT计算。

```go
goal = memoryLimit                               // p0
       - ((mappedReady - heapFree - heapAlloc)   // p1
       + max(mappedReady - memoryLimit, 0))      // p2
       - memoryLimitHeapGoalHeadroom             // p3
```



**解释下：**

- p0：memoryLimit：就是指定的软限制

- p1：noheap overheads

- p2：超出限制的部分

- p3：1mb的headroom（留点buffer）



那这里的goal和GOGC算出的goal，以哪个为准呢？**可以简单理解为以小的为准**，真实情况是有一点点微调，可以不关注。这样当内存占用达到这个goal时，就会触发GC了。



#### **项目推荐的GC设置**

首先，可以参考项目以前的ballast设置，比如ballast=1gb设置，现在默认值可以采用GOMEMLIMIT=2gb、GOGC=off。

效果是：

- 和压舱石类似，内存占用不多时，尽量减少GC开销；
- 不同的是，当内存占用比较多（可以理解为接近或者超过GOMEMLIMIT）时，会（更频繁）触发GC，以尽量内存占用符合这个软限制



**如果特定服务对大内存占用有需求，项目中也可以允许通过自定义配置项来解决，如：**

```yaml
go:
  gogc: -1
  gomemlimit: <bytes>
```



另外由于go支持通过读取环境变量GOGC、GOMEMLIMIT的方式来在一开始gcinit的时候进行设置，所以我们应该遵循这样的原则，这两个变量都应该独立遵守下面的原则：

- 环境变量配置优先级最高，这样符合go使用习惯
- 环境变量未指定时，读取配置文件中的自定义值
- 如果配置文件没指定，则使用默认值



#### 上述方案的风险点

最开始确实是直接GOMEMLIMIT=1gb+GOGC=off这样来设置的，但是测试过程中发现，这个方案是有问题的。有经验的开发人员可能已经意识到问题在哪里了：

- ballast方案中，我们并不会GOGC=off直接关闭GC，这样虽然ballast推高了下次GC的阈值，但是sysmon还是能做到每隔两分钟（2min）强制GC一次的（forced GC）。所以如测试环境下观察的那样，大多数进程实际占用物理内存并不多，因为GC回收内存了；
- 而在我们GOMEMLIMIT+GOGC=off组合情况下，因为GOGC被关闭了，此时sysmon即使过了2min这个间隔期，也不会去触发forced GC（这个从源码中一看便知）。这样问题就来了，当内存占用小于1GB时基本上不会触发GC，因为非堆内存占用很少，按照GOMEMLIMIT计算出的下次的heapGoal跟GOMEMLIMIT差不多，所以基本上不会触发GC，这就会导致各个进程占用物理内存接近软限制，而如果混部的go进程多的话，就很容易导致机器内存占用率过高。

那我们应该如何进一步解决这个问题呢？解决问题前，首先要搞清楚我们追求什么。

1. 在进程内存占用很少时，尽量不触发GC，或者不要频繁GC；

2. 在进程内存占用较多时，要触发GC来回收内存，不要因为达到容器、虚拟机、物理机等分配的资源限制（cgroup来控制）被OS给OOM kill掉；

3. 进程steady状态时占用内存不要停留在GOMEMLIMIT附近，以避免频繁GC对服务性能产生不良影响、抖动；

我们目标明确了之后，方案自然也就多了起来：

- ~~方案1：GOGC=off+GOMEMLIMIT，就是上面存在风险点的方案。否掉；~~
- ~~方案2：GOGC=100+GOMEMLIMIT，这种就基本上走普通GC pacing，解决不了问题1，但是可以解决问题2。否掉；~~

- ~~方案3：GOGC=100+ballast方案，虽然用了go1.19，可以解决上述问题1，但是对2无能为力，被ballast推高的nextGoal很可能超过资源上限而存在被OOM kill的风险。否掉~~
- 方案4：GOGC=off+定时runtime.GC()+GOMEMLIMIT，这样可以兼顾问题1、2、3，在内存占用达到软限制之前，GC完全依赖定时；
- ~~方案5：GOGC=on+ballast+GOMEMLIMIT，这样可以兼顾问题1、2、3，但是ballast初始化方式有点依赖于初始化时机，我们在前面已经分析过ballast被GC和内存分配器影响而导致占用了物理内存的问题，并不是很想采纳这种方案。否掉；~~
- 方案6：采用一些动态GOGC设置的方式，以解决问题1、2，把问题3交给GOMEMLIMIT去解决，至于如何设置GOGC就可能要凭借一些经验值、测试效果、服务本身情况来调，可能实际部署运维的时候不见得是件容易的事情。但是也值得探索下。

最终搞下来，可能就只有几个方案值得去对比下，正在测试对比中……



### 关于GC Tuner

关于动态GC调优，uber有一个实现方案，根据其公开的技术文章 [Uber's Engineering Manages to Cut 70k CPUs by Tuning GO GC](https://www.infoq.com/news/2022/03/uber-go-gc-tuner)，大致是一个动态设置GOGC来让进程占用物理内存尽量不要超过设定的内存占用百分比的一个东西，从这个意义上来说，它的作用和GOMEMLIMIT很类似。github上有个参考实现，详见 [GC Tuner](https://github.com/cch123/gogctuner)。

不过既然是动态GC Tuner，当然有理由可以做的更灵活来满足各自的场景。但是嘛，就是前面说的，灵活意味着特殊化，难道要针对每个不同的服务做定制化配置？理想情况下是这样，但是实际部署运维中不见得是件简单的事情，可能会给我们带来很多额外的工作成本。



### **相关的注意事项**

其实不管是通过以前GOGC这个唯一控制项，还是现在GOGC+GOMEMLIMIT组合的方式，开发人员都应该对自身服务性能、资源占用有个清晰的认识。这就是说，在必要的部署机型下做压测应该常态化，这样才能在服务部署运维时有更清晰的认识，内存占用多少算是正常、不正常，负载多高应该选择扩容、缩容。

现在很多都已经容器化部署了，但是对于扩缩容依赖的CPU、MEM阈值要有认识。容器化部署隔离性好一点，如果存在混部，那对这里GOMEMLIMIT=?+GOGC=off还会有更好的认识，因为GC不及时可能导致混部的其他服务申请不到内存资源。

不管用那种GC调优，对服务自身的认识都是每一个开发人员所应该关注、提高的。