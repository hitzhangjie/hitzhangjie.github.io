---
layout: post
title: "json库性能对比及实现探究"
description: "JSON是一种轻量级的数据交换格式，易于阅读、解析、生成，应用十分广泛。如今在微服务通信中，JSON也是一种常见的序列化手段，比如json-rpc或者gRPC json、pb互转。因为读写场景的不同，对JSON序列化、反序列化（或者解析）的关注点也不一样，一个通用的JSON库不一定能满足性能要求，可以看到有非常多的JSON第三方库频频向标准库发起挑战。本文将从JSON解析的不同场景入手，来说明这些场景下对JSON生成、解析的一些诉求，以及对性能方面的考量，进一步介绍下业界在这方面一些优秀的实践。"
date: 2023-10-09 16:01:42 +0800
tags: ["serialization","json","encoding/json","bytedance/sonic","segmentio/encoding","jsonpb","reflection","simd"]
toc: true
reward: true
---

## 本文背景

JSON是一种轻量级的数据交换格式，易于阅读、解析、生成，应用十分广泛。如今在微服务通信中，JSON也是一种常见的序列化手段，比如json-rpc或者gRPC json、pb互转。因为读写场景的不同，对JSON序列化、反序列化（或者解析）的关注点也不一样，一个通用的JSON库不一定能满足性能要求，可以看到有非常多的JSON第三方库频频向标准库发起挑战。本文将从JSON解析的不同场景入手，来说明这些场景下对JSON生成、解析的一些诉求，以及对性能方面的考量，进一步介绍下业界在这方面一些优秀的实践。



## 回顾JSON标准

rfc8259是目前JSON事实上的标准https://datatracker.ietf.org/doc/html/rfc8259，一个合法的JSON value必须是一个object、array、number、string，或者以下字面量false、true、null。该规范定义了JSON grammar来说明如何表示上述数据。

rfc8259标准明确提出，如果JSON数据不是在一个封闭系统中使用，在不同系统中进行交换时，字符集应该明确使用UTF-8编码。旧的JSON标准并没有指出这点，但是为了保证不同系统的正常交互，大多数系统使用的正是UTF-8编码。标准还指出在编码时不应该在头部添加BOM字符（Byte Order Mark，U+FEFE），一些JSON解析器为了尽可能保证互操作性可能会忽略被错误添加的BOM字符，而不是报错。

> ps：rfc8259中还提及使用Unicode字符，Unicode是一种字符编码标准，定义了字符的唯一码点，而UTF-8是Unicode的一种可变长的具体编码方案，以对ASCII进行向后兼容。

JSON 解析器（parser）将JSON文本转换为另一种表示形式，比如go结构体struct。JSON 解析器必须接受所有符合 JSON grammar的文本，可以接受非 JSON 形式或其他扩展（比如vscode .devcontainer定义中支持注释）。解析器实现可能会对其文本的长度进行限制，也可以对数据的最大嵌套深度进行限制，也可以对数值的范围、精度进行限制。

> ps："A JSON parser MAY accept non-JSON forms or extensions." 这句话的意思是，JSON解析器可以接受非JSON形式或扩展。也就是说，解析器可以容忍一些不符合严格JSON语法的文本，或者支持一些扩展的语法或功能。这给了解析器一定的灵活性，使其能够处理一些非标准的JSON文本或具有扩展功能的JSON文本。这样做是为了在实际应用中提供更大的灵活性和兼容性，以满足不同的需求和场景。

JSON生成器（generator）用于生成JSON文本，生成的文本必须严格符合JSON grammar。比如`json.Marshal(v)`将v这个数据类型序列化成JSON文本，当然还有`json.MarshalIndent(v, "", "\\t")`，会在name前面增加一些缩进，tab、空格等空白字符在标准中也是允许的。

**小结：通过rfc8259我们了解了JSON是用来做什么的，有效的JSON数据是什么样的，为了互操作性、灵活性JSON的解析器、生成器又可以怎么做。**下面我们将介绍一些应用场景，从一般到特殊，对应的也会对标准库实现提出一些挑战，然后进一步介绍一些业界的实践、优化。

## 从标准库开始

go标准库中提供了对JSON编码、解析的支持，最常用的两个函数就是`json.Marshal`、`json.Unmarshal`。标准库的设计实现，对大多数数据类型、普通的编码解析场景、易用性方面提供了很不错的支持。

在指出标准库在哪些场景下会表现欠佳之前，需要先了解下标准库在编码、解析过程中的一些实现策略、细节。

这里简单总结一下：

- 标准库json.Marshal的过程，使用了大量的反射操作，比如确定map k、v的类型信息，struct字段的类型信息，匿名嵌套及字段的可见性分析，struct jsontag规则处理，而且是通过反射递归展开json.Marshal(v)中v的类型信息，才能知道如何encode，最后才是根据v及其内部各个组成部分对应的typeEncoder来完成encode输出。encode的过程中虽然它使用了一些caching（缓存）、pooling（池化）技术，但是前面的反射开销确实是比较大的，尤其是数据类型复杂、数据量比较大的时候。

  想了解详细过程的话，可以参考这篇总结，会对这个过程中的开销有更清晰的认识：https://www.notion.so/hitzhangjie/JSON-d278399b8092470985cbc423830115fb?pvs=4

- 标准库json.Unmarshal的过程，和json.Marshal的过程相比，其中涉及到的一些要点大差不差，这里就不展开了。

see: https://sourcegraph.com/github.com/golang/go@go1.19.10/-/tree/src/encoding/json

> ps：反射的开销主要在哪里？
>
> reflection trades performance for very dynamic and flexible code by pushing work from compile time to runtime. The runtime costs include indirect calls, type inspection, value conversions, and dynamic dispatch. But used judiciously, these costs can be worth it for certain programs.

我的个人看法是，标准库作为一种支持更广泛场景下的实现，使用反射并不是一件坏事，它牺牲了一定的性能来保证了运行时的更大的灵活性，而不用像某些三方库一样去做一些极致的优化，比如bytedance/sonic只支持amd64架构。

Well，但是当我们知道自己要做什么时，还是可以去做一些更加“极致”的优化的。比如bytedance团队知道自己用的是什么类型的机器，借助SIMD等一系列优化，单是JSON相关的序列化、反序列化优化带来的收益，性能提升了、CPU开销下降了，算下来为公司节省了几十万核，这种优化就是很值得的。

下面就带着这种“优化”的思路去看看“不同场景”下该如何优化来达到期望的效果。

## 细说业界实践

先列举几个不错的第三方实现。

| project           | repository                                 | functions        |
| ----------------- | ------------------------------------------ | ---------------- |
| fastjson          | https://github.com/valyala/fastjson        | parser           |
| jsonparser        | https://github.com/buger/jsonparser        | parser           |
| jsoniter          | https://github.com/simon-engledew/jsoniter | parser           |
| simdjson          | https://github.com/simdjson/simdjson       | parser           |
| simdjson-go       | https://github.com/minio/simdjson-go       | parser           |
| rapidjson         | https://github.com/Tencent/rapidjson       |                  |
| json-iterator/go  | https://github.com/json-iterator/go        | parser+generator |
| bytedance/sonic   | https://github.com/bytedance/sonic         | parser+generator |
| segmentio/encodin | https://github.com/segmentio/encoding      | parser+generator |
| goccy/go-json     | https://github.com/goccy/go-json           | parser+generator |

### fastjson

fastjson parse+get操作，和标准库encoding/json的unmarshal相比，效率是后者的15x，see [benchmark](https://github.com/valyala/fastjson#benchmarks)。

fastjson实现上消除了reflection，解析过程中也完全不需要schema，也不需要像其他某些三方库一样通过code generate来生成schema，它是怎么做的呢？

- 它解析JSON文本的过程，非常暴力直接，跳过空白字符直接对JSON value按照object、array、string、number、true、false、null进行解析，解析完后并不做任何“类型转换”的操作。比如object内部的字段名和值直接作为一个kvpairs数组存起来，并不关心它的类型是什么样的。
- 当真正去获取某个字段值时，调用方就知道类型是什么了，此时调用对应的GetInt或者GetString等包含类型信息的helper函数，函数内部将对应的string转换为具体的类型。

通过上面这种方法，彻底消除了反射。虽然parse+get的效率比标准库unmarshal效率高，但是还是要看具体场景，是否用起来方便，是否真的care性能。

另外，fastjson虽然提供了快速的解析操作，但是没有提供快速的编码操作，尽管它提供了value.MarshalTo方法，但是这个并不是大家日常编码时需要的将任意类型编码为JSON文本的操作。

关于快速的编码操作，fastjson建议通过[valyala/quicktemplate](https://github.com/valyala/quicktemplate)来执行快速编码。我看了下，它实际上是通过模板引擎来完成这个编码操作，但是要业务开发针对要marshal的自定义类型写好对应的go模板，有这个功夫，我还不如将自定义类型实现Marshaler接口了，干嘛非得用模板呢？同样可以避免大量反射，还不用调试go模板。

### jsonparser

只支持parser，号称效率是标准库encoding/json的10x，那么它为什么这么快呢？没fastjson快 :laughing:

- 它不依赖于encoding/json、reflection或interface{}，唯一真正的包依赖是bytes。 
- 它在字节级别上操作JSON payload，为您提供指向原始数据结构的指针：无需内存分配。
- 没有自动类型转换，默认一切都是[]byte，但它提供了值类型，您可以通过helper函数自行转换。 
- 它不解析完整的记录，只解析您指定的键。可以通过jsonpath来访问JSON中嵌套在内部的元素信息。

看下来，它的实现思路和fastjson类似。

### jsoniter

这个库的实现思路是，避免解析完整的JSON文本，而是遍历JSON数据的过程中path和指定的jsonpath匹配时才开始将当前path对应的jsonvalue进行decode。

在这个遍历过程中没有使用反射，也没有decode不必要的value，前面的几个实现方案是解析了对应的value的，尽管只是kvpairs的形式，不过开销也不大。

实际decode感兴趣的部分时还是使用了标准库encoding/json.Decoder的实现，这里面哈还是会走到反射部分。所以很难说这个库实现性能有多高。如果是数据量比较大，只解析其中部分数据时还是优势的。

```go
import (
    "encoding/json"
    "github.com/simon-engledew/jsoniter"
)

func main() {
    var found any
    matcher := jsoniter.Matcher("some", 0, "nested", "structure") // .some[0].nested.structure

    d := json.NewDecoder(os.Stdin)
    err := jsoniter.Iterate(d, func(path []json.Token) error {
        if matcher(path) {
            return d.Decode(&found)                               // decode感兴趣的部分数据
        }
        return nil
    })
}
```

### simdjson(-go)

simdjson是C++版本的实现，simdjson-go是用go重写后的版本，整体实现思路差不多，所以合在一起说了。simdjson是Daniel Lemire 和 Geoff Langdale实现的一个JSON解析库，它广泛使用了SIMD指令操作来获得高效的JSON解析操作，号称解析1GB JSON数据只需要1秒钟。

simdjson-go是使用go语言重写后的版本，它的性能大概是c++版本实现的40%~60%，是标准库性能的10x。

> ps：大名鼎鼎的[clickhouse](https://github.com/ClickHouse/ClickHouse)就使用了simdjson，可以在这里看到更多使用simdjson的知名项目https://github.com/simdjson/simdjson#real-world-usage。

那么simdjson或者simdjson-go为什么会在解析的时候有这么高的性能呢？最主要的技术已经在名字中了，SIMD！

那，什么是SIMD呢？SIMD在JSON编码、解析中可以用来做什么呢？让我们一点点说起。

#### 什么是SIMD

SIMD(Single Instruction Multiple Data) 是一种并行计算技术,可以同时对多个数据执行相同的操作。使用 SIMD 的主要目的是为了提升计算性能。

#### SIMD适用场景

适合使用 SIMD 的情况包括:

- 需要对大批量数据执行相同的数学运算或逻辑运算,如向量、矩阵运算、图像处理等。
- 需要对多媒体数据如音频、视频等进行处理,如编码、解码、滤波、变换等。
- 在数据库、科学计算、金融分析等需要处理大量数值计算的场景。
- 游戏开发中的物理模拟、人工智能等也可以使用 SIMD。

使用 SIMD 的好处有:

- 提高计算并行度,单次指令处理更多数据。
- 减少指令数,降低指令调度开销。
- 更高效利用处理器内部执行单元。
- 数据级并行,更易映射到多核架构。

一些常见使用 SIMD 的例子:

- 图像处理:模糊、锐化、色彩空间转换等算法可以用SIMD加速。
- 信号处理:FFT、FIR/IIR 滤波等用SIMD实现。
- 科学计算:向量矩阵运算都可以用 SIMD 优化。
- 数据压缩/解压:如音频视频编解码中的 SIMD 优化。
- 数据库操作:聚集函数、关系运算可用 SIMD 实现。
- 机器学习:神经网络中矩阵乘法、激活函数计算等使用 SIMD。

总之,SIMD 非常适合数据并行的场景,使用它可以显著提升计算性能。编译器和开发者都可以通过自动向量化和手动优化,利用 SIMD 使程序运行更快。

#### SIMD用于JSON

how simd works, https://www.youtube.com/watch?v=o_n4AKwdfiA

parsing json really quickly, https://www.youtube.com/watch?v=wlvKAT7SZIQ



### json-iterator/go

json-iterator/go是另一个标准库encoding/json的平替，默认配置下它的编码、解析性能已经是标准库的好几倍，还可以配置成高性能模式，如允许使用6digits来编码float允许损失精度。

那么这个库做了哪些优化呢？作为一个标准库的平替，应该可以多给些关注，看下它的benchmark数据：

| ns/op           | allocation bytes | allocation times |              |
| --------------- | ---------------- | ---------------- | ------------ |
| std decode      | 35510 ns/op      | 1960 B/op        | 99 allocs/op |
| easyjson decode | 8499 ns/op       | 160 B/op         | 4 allocs/op  |
| jsoniter decode | 5623 ns/op       | 160 B/op         | 3 allocs/op  |
| std encode      | 2213 ns/op       | 712 B/op         | 5 allocs/op  |
| easyjson encode | 883 ns/op        | 576 B/op         | 3 allocs/op  |
| jsoniter encode | 837 ns/op        | 384 B/op         | 4 allocs/op  |

那它是如何做的呢？看代码的结构的话，别说，bytedance/sonic跟这个还有点像……ok，回来继续看。

see：https://sourcegraph.com/github.com/json-iterator/go@71ac16282d122fdd1e3a6d3e7f79b79b4cc3b50e/-/blob/config.go?L296:26&popover=pinned

see：https://sourcegraph.com/github.com/json-iterator/go@71ac16282d122fdd1e3a6d3e7f79b79b4cc3b50e/-/blob/reflect.go?L87:23-87:31

看了下，和segmentio/encoding/json有点类似，尽可能消除反射逻辑、缓存技术、池化技术之类的优化。但是这里的测试应该没有那么充分，它的性能应该和segmentio/encoding/json差不多了太多。



### segmentio/encoding

内部实现使用了一些unsafe操作（无类型代码、指针运算等等）来尽可能避免使用反射，使用反射通常是序列化过程中CPU占用高、内存开销大的重要原因。这个包致力于实现零不必要的动态内存分配，并且热点代码路径中尽量避免使用反射包。

以json.Marshal(v)为例，来看下做了哪些优化？

- 构建encoderCache sync.Map的时候，它没有使用reflect.Type来作为key，而是使用了typeid(reflect.Type)，实际上是一个实现类型的地址，通过这种方式减轻了后续查询type对应的encoder的开销；
- map的keys是否排序，提供了一个选项进行控制，而不是像标准库那样有各种各样的排序，排序前获得map中的所有key、value还是通过反射的，所以这个排序的前置准备以及排序本身都有开销的，不能忽略；
- struct的fields排序也省略掉了，这个把struct及其内部嵌套struct通过反射获取index信息进行排序的逻辑也比较啰嗦，这里也省掉了；
- html转义也是可以控制的，这个和标准库一样都有选项进行控制，算不上什么明显优化；

所以，segmentio/encoding/json库是通过消除一些不必要的反射以及其他一些优化技术来改善了解析、编码的性能，可以作为标准库的平替。实测其性能是



### goccy/go-json





小结：see: https://www.notion.so/hitzhangjie/c89498a151a64cae80fc27ec875c6081?pvs=4







## 参考文献

1. JSON, https://en.wikipedia.org/wiki/JSON
2. Introducing JSON, https://www.json.org/json-en.html
3. The JavaScript Object Notation Data Interchange Format, https://datatracker.ietf.org/doc/html/rfc8259
4. 



## 附录

> Q：请列举JSON序列化、反序列化过程中的挑战点，并针对这些挑战点介绍下第三方库是如何进行优化的？

当涉及到JSON序列化和反序列化时，以下是一些常见的挑战点，以及第三方库是如何进行优化的：

1. 性能优化：JSON序列化和反序列化可能涉及大量的数据操作和字符串处理，对于大型数据集或频繁的操作，性能可能成为挑战。第三方库通常通过以下方式进行性能优化：
   - 采用更高效的算法和数据结构：一些库使用更快速的解析算法和数据结构，如基于流的解析器、缓冲区技术等，以提高处理速度。
   - 批量处理：一些库支持批量处理，将多个对象一次性序列化或反序列化，减少了数据操作和字符串处理的次数，提高了效率。
   - 延迟加载：一些库支持延迟加载，只在需要时才进行序列化或反序列化，避免了不必要的操作和资源消耗。
2. 复杂数据结构处理：处理嵌套、多层次或循环引用的复杂数据结构可能变得复杂。第三方库通过以下方式进行优化：
   - 自定义序列化和反序列化规则：一些库允许开发者定义自定义的序列化和反序列化规则，以处理复杂数据结构的特殊情况。
   - 引用跟踪和处理：一些库提供引用跟踪和处理机制，以处理循环引用和避免无限循环的问题。
3. 类型转换和映射：JSON和编程语言中的数据类型可能不完全匹配，需要进行类型转换和映射。第三方库通过以下方式进行优化：
   - 自动类型转换：一些库支持自动类型转换，根据JSON中的表示自动将其转换为适当的编程语言类型。
   - 自定义类型映射：一些库允许开发者定义自定义的类型映射规则，以处理特定类型的转换和映射需求。
4. 错误处理和容错性：处理JSON序列化和反序列化过程中的错误和异常情况是重要的。第三方库通过以下方式进行优化：
   - 错误处理机制：一些库提供丰富的错误处理机制，如异常抛出、错误码返回等，以便开发者能够更好地处理错误情况。
   - 容错性：一些库具有容错性，能够处理部分无效或不完整的JSON数据，以尽可能地恢复有效数据。

总的来说，第三方库通过采用更高效的算法和数据结构、提供自定义规则和映射、处理复杂数据结构、优化性能和错误处理等方式，来解决JSON序列化和反序列化过程中的挑战。这些优化措施使得第三方库能够更好地满足不同需求和场景下的JSON处理需求。



