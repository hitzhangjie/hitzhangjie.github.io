<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>loadbalance on</title><link>/tags/loadbalance/</link><description>Recent content in loadbalance on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 06 Aug 2022 17:16:57 +0800</lastBuildDate><atom:link href="/tags/loadbalance/index.xml" rel="self" type="application/rss+xml"/><item><title>一致性hash负载均衡方案的思考</title><link>/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Sat, 06 Aug 2022 17:16:57 +0800</pubDate><guid>/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>常见的负载均衡策略 # 客户端完成被调服务的服务发现后，获得了一批实例节点列表，现在要借助合适的负载均衡算法来选择一个实例完成请求处理。
常见的负载均衡算法包括：
轮询：每一次网络请求按照顺序发放给下节点列表中的下一个节点，这种情况适用于节点配置相同并且平均服务请求相对均衡的情况 加权轮询：考虑了不同节点的硬件配置情况，如节点a、b、c性能有低到高，权重设置为1、3、6，则按照权重分配10%、30%、60%的请求给到节点，这种可以避免高性能机器负载低、避免性能差机器过载 随机：随机选择一个节点来处理请求，这种在请求量比较大的情况下能达到相对均衡的分布，同样适用于机器配置相同的情况 加权随机：考虑了不同节点的硬件配置情况，类似加权轮询，只不过选择下一个节点时是基于随机选择，而非轮询的方式 余数hash：根据某个key对节点数做取模运算，比如节点数为n，根据请求中的m = uid % n，表示用节点列表中第m个节点来作为服务节点。当key分布范围比较广能达到相对均衡，选择key字段的时候要考虑下key分布情况。使用hash的场景，一般是因为后端节点有状态可复用（或者希望借此减少并发冲突），但真实环境中，节点故障是常态，尤其是在容器化部署场景下自动化扩缩容，hash会导致集群中所有节点状态无法被复用。一般会用一致性hash代替hash。 一致性hash：一致性hash是对hash的优化，一致性这里强调的就是节点加入、离开后尽量保证大多数请求仍然能路由到该路由的节点，而不是新加入的节点，同时为了避免新加入、离开节点导致的负载不均衡问题，引入了虚拟节点的概念，每个物理节点都对应着hash环上一定数量的虚拟节点，这些节点混在一起，能实现各个节点负载的相对均衡。节点数量该选择多少呢？一个比较直观的认识是可能虚拟节点越多越均衡，但是数量过多也会有开销，这与虚拟节点的hash计算、存储有关系，本文后面讨论。 按响应速度：有些负载均衡设备，会根据与后端服务节点间的ping延时来选择一个响应时间最短的。类似的也可以根据client、server之间的ping延时或者请求处理响应时间来选择。 按最少连接数：对于某些请求处理时间比较长的场景，如ftp传输等，一个tcp连接存在的时间可能比较长，连接数比较多的可能代表该节点负载比较重，因此会开率选择连接数比较少的来提供服务。 其他 负载均衡算法有很多，之所以这么多也是因为应用场景的差异，根据合适的场景选择适用的负载均衡算法。
调研一致性hash策略及其可替代方案 # 对一致性hash方案及其可替代方案进行调研、对比。
余数hash # 余数hash，简单讲就是那个key去算出hash值，然后对节点数量取模，m = hash(key) % n，用节点列表中的第m个节点去做请求处理。 如果节点数变化非常不频繁，或者说key remapping（rebalancing）过程中带来的开销不大、影响不大，那用余数hash也无所谓。
但是现实场景中，比如一些有状态服务，如果remapp后映射到了与先前不同的节点，或者容器化部署时节点数经常变更，不满足适用余数hash的条件。
比较常见的对策，就是采用一致性hash。
一致性hash # 简要介绍 # 一致性hash能够缓解节点加入、离开时rebalancing导致的一些hash节点改变的问题，在以下场景中就更有优势：
服务是有状态的，这样大多数路由仍然能路由到原来的节点，状态可以复用；
即使服务不是有状态的，将原来路由到节点n的请求及其后续请求继续路由到该节点，也可能存在更好的局部性处理（locality），
举个例子（可能不很恰当哈）： 比如有个个人展示页要展示头像昵称、最近游戏记录，假设之前有个什么客户端请求uid=xxx的请求路由到了节点n拉取过了昵称头像并cache，后面该展示页也路由到该节点的话就可以复用该cache。
假设key空间中值数量为k，节点数为n，那么当发生remapping时，笼统估算有k/n不命中原来的节点。
关于实现 # 关于一致性hash的实现：
构建一个一致性hash环，一个数组就可以实现 选定节点的key，如ip，hash(key)，然后再hash环上对应位置存储该节点信息，考虑到hash环大小需要适当取模 考虑到各节点的负载平衡，引入虚节点，每个物理节点对应为k各虚节点（k多大？），各个虚节点的hash值计算就分不同方法： key多大？兼顾计算效率和负载均衡性，因为节点数提前无法预估，可能要选择一个更好的经验值 引入k个hash函数，hash1(key), hash2(key), hash3(key)&amp;hellip;.hashK(key)，分别设置到hash环上 针对key，构造key_1, key_2, key_3..，keyK，使用同一个hash函数分别计算上述key的hash，并在hash环上设置其节点信息 TODO 这里的计算方式的选择，虚节点数多大（过少还是会不均衡），过大计算效率慢（多次计算hash），另外多个hash还是构造多个key也可能会影响到负载的均衡性，需要针对性的测试。 现在有个请求，比如用玩家userid作key，hash(key)得到值之后，因为一致性hash环是个收尾相接的有序数组实现的，可通过二分查找（查找第一个大于等于该hash(key) )的节点，复杂度O(logn) 一致性hash，对于带权重的也能支持到：比如a机器比b机器性能高一倍，希望其处理两倍于b的请求，那么就可以让a机器的虚节点多一倍。但是如果管理的节点数量成千上万之后，hash环上存储这些虚节点的开销就不能忽略了。</description></item></channel></rss>