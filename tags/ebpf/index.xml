<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ebpf on</title><link>/tags/ebpf/</link><description>Recent content in ebpf on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 22 Nov 2023 01:57:47 +0800</lastBuildDate><atom:link href="/tags/ebpf/index.xml" rel="self" type="application/rss+xml"/><item><title>eBPF开发环境搭建</title><link>/blog/2023-11-22-ebpf%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</link><pubDate>Wed, 22 Nov 2023 01:57:47 +0800</pubDate><guid>/blog/2023-11-22-ebpf%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</guid><description>问题背景 # 如果读者朋友使用的是Linux机器，而且系统是Ubuntu这些社区比较活跃的发行版，那么你遇到问题的时候，一般可以比较快地解决，或者很少遇到这种被他人反复才坑已经修复的问题。所以本篇文章并不一定适合你，不过看看也说不定有其他方面收获。
我使用的开发环境如下：
处理器：i9 13900K (x86_64) 操作系统：Windows 11 WSL版本：v2 Linux发行版：RedHat 8.5 Linux内核版本：5.15.90.1-microsoft-standard-WSL2+ 2023年9月份已经在阅读lizrice的learning-ebpf一书了，并且自己还跑了下书中的用例，并对测试时遇到的环境设置问题进行了解决，但是隔了一段时间，因为执行了 yum update吧，clang、llvm、kernel-headers、bcc相关包，它们之间的依赖没有明显问题，但是整合到一起编译构建、运行ebpf程序的时候，开始报错。
于是2023.11.21日这天花费了大量时间来重新解决eBPF的开发环境设置问题，先记录下，供大家以及自己日后参考。
环境设置 # 内核配置 # 1、git clone https://github.com/kernel-newbies/WSL2-Linux-Kernel
2、cd WSL2-Linux-Kernel &amp;amp;&amp;amp; git checkout linux-msft-wsl-5.15.90.1
​ 选择版本5.15.90.1，与lizrice/learning-ebpf中推荐版本5.15.x.y尽可能对齐
3、执行 make config 配置编译构建选项
​ 直接使用这里的.config ，这个已经是配置好了必要的ebpf选项的配置了
4、执行 make -j8 进行内核构建，内核输出到了vmlinuz文件
5、执行 sudo make headers_install 进行内核头文件安装
工具链配置 # 1、sudo yum install clang clang-devel llvm llvm-devel
​ 注意llvm不同版本兼容性有些问题，可能在低版本上编译ok升级后反而失败了，
​ 我就是遇到的这样的坑，原本bcc 0.26可以在llvm 16上编过，升级到llvm 17失败
2、不使用yum源中的bcc 0.25.0，有bug未修复，直接从源码安装
​ git clone https://github.</description></item><item><title>观测Go应用函数调用：go-ftrace</title><link>/blog/2023-09-25-%E8%A7%82%E6%B5%8Bgo%E5%BA%94%E7%94%A8%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8go-ftrace/</link><pubDate>Thu, 28 Sep 2023 12:42:47 +0800</pubDate><guid>/blog/2023-09-25-%E8%A7%82%E6%B5%8Bgo%E5%BA%94%E7%94%A8%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8go-ftrace/</guid><description>go-ftrace # go-ftrace 是一个基于Linux bpf(2) 的类似内核工具 ftrace(1) 的函数调用跟踪、耗时统计工具，它主要是面向go应用程序的。
限制: 因为设计实现的原因，当前go-ftrace只支持满足如下限制条件的go程序跟踪、统计：
Linux内核：支持 bpf(2) 和 uprobe 的Linux内核 处理器架构: x86-64架构（little-endian字节序） 二进制程序：只能是go ELF可执行程序（非PIE模式），未剔除符号表.symtab，未剔除调试信息.(z)debug_info， 使用方式 # 项目中提供了测试程序 examples/main.go ，可以执行如下几种测试来了解go-ftrace的使用:
示例1: 跟踪一个自定义函数 main.add: ftrace -u main.add ./main 示例2: 跟踪所有的匹配函数 main.add*: ftrace -u 'main.add*' ./main 示例3: 跟踪多个模式匹配的函数 main.add* 或 main.minus*: ftrace -u 'main.add*' -u 'main.minus*' ./main 示例4: 跟踪一个自定义函数 &amp;quot;main.add 以及 内置函数 runtime.chan*: ftrace -u 'main.add' -u 'runtime.chan*' ./main 示例5: 跟踪一个自定义类型的方法: ftrace -u 'main.(*Student).String ./main 示例6: 跟踪一个自定义类型的方法，并试图提取关心的参数: ftrace -u 'main.</description></item><item><title>eBPF案例及分析：gofuncgraph</title><link>/blog/2023-09-15-ebpf%E6%A1%88%E4%BE%8B%E5%8F%8A%E5%88%86%E6%9E%90gofuncgraph/</link><pubDate>Fri, 15 Sep 2023 23:00:22 +0800</pubDate><guid>/blog/2023-09-15-ebpf%E6%A1%88%E4%BE%8B%E5%8F%8A%E5%88%86%E6%9E%90gofuncgraph/</guid><description>前言 # 可观测性（observability）是这几年开始被频繁提及的一个词，特别是在微服务领域可观测性已经成为了微服务治理的一项关键的平台化技术手段，在CNCF孵化的项目中我们看到Opentelemetry如火如荼的发展背后也逐渐出现了一些成熟的解决方案。在腾讯内部也有类似天机阁、蓝鲸、wxg等不同的解决方案。这些往往配合框架解决了微服务RPC层面 的可观测性问题，实际上借助eBPF这项革命性技术，我们还可以做更多。
背景 # 不久前，在做一个关于序列化方面的优化工作，先说下项目情况：项目中使用的go框架采用了pb+protoc-gen-gogofast来生成桩代码，RPC通信的时候使用pb序列化。另外呢，为了方便开发人员查看pb message对应的log信息，项目的日志库使用了pbjson将pb message格式化为json后输出到log，RPC interceptor也会使用相同的方式序列化req、rsp后将其上报到链路跟踪系统。
大致就是这样一个问题，当时对比了pbjson序列化、stdlib encoding/json序列化，segmentio/encoding/json序列化，以及bytedance/sonic序列化。哈哈，这个顺序其实就是由慢到快的一个顺序，bytedance/sonic凭借优化反射、simd等技术“遥遥领先”其他集中方案。除了benchmark的手段，我还想看看上线前后的一些详细的优化效果，比如不同包大小（比如按1KB分桶）的序列化耗时（纳秒）分布。
摆在我面前有两个办法：
改源码，统计下序列化前后的执行耗时，然后打log，写个工具分析下log； 改源码，统计下序列化前后的执行耗时，然后上报到监控，看看统计直方图； 其实都可以，但是我有点懒，我既不想去改源码（更不用说改很多）去写log、报监控，分析完了还需要再把这堆代码删掉。改完代码我还需要编译、发布，我们每次编译发布流程都要10min左右，我很不想去干这些事。
总之我既想要灵活的分析工具（能灵活指定函数名称），又不侵入业务代码，调研之后发现有开发者实现了这样的工具，jschwinger233/gofuncgraph，它借鉴了内核函数图跟踪工具ftrace的设计，执行效果大致如下。借助funcgraph，很快解决了我的问题。
工具介绍 # gofuncgraph是借鉴了Linux内核函数图工具ftrace（function tracer）的功能，然后为go程序开发的一个函数图工具，如上图所示，你可以指定要跟踪的函数的匹配模式，然后该工具会将程序中匹配的函数名全部作为uprobe去注册，并注册上对应的回调处理函数。
处理函数中会根据是进入函数、退出函数来生成一些这样的events，每个event都有时间，这样就可以准确统计出函数的执行耗时了。然后利用调用栈信息，也可以绘制出函数调用图。最终输出上述函数图。
一个小插曲，help: how to use gofuncgraph，最开始我以为是要用这个工具去启动个程序才可以执行测试，是我理解有误。和作者沟通过程中，作者提到之前阅读过我写的调试器相关的电子书，并说质量很高。大家互相分享互相学习，挺好的。现在我也来学习作者的gofuncgraph，除了学习ebpf程序的写法外，我也想了解下为什么调试器的知识会用在这个程序里。
剖析实现 # 本节先介绍该工具的用户界面设计实现，然后再介绍其内部的工作逻辑，工作逻辑中会层层深入把必要的DWARF、eBPF、编译链接加载等相关的关键内容都逐一介绍下。
为了后续方便自己学习、维护、定制，我fork了作者的项目并做了一些优化、重构，如使用spf13/cobra来代替了原先的命令行框架，spf13/cobra支持长、短选项，对用户更友好。另外也对项目代码进行了一些可读性方面的优化。后续介绍将继续我修改的这个版本介绍 hitzhangjie/gofuncgraph (dev)。
命令行界面 # 执行 gofuncgraph help 查看帮助信息，简要介绍了它的用途，你可以执行gofuncgraph --help来查看更完整的帮助信息。
简要帮助信息：
$ ./gofuncgraph bpf(2)-based ftrace(1)-like function graph tracer for Go! for now, only support following cases: - OS: Linux (always little endian) - arch: x86-64 - binary: go ELF executable built with non-stripped non-PIE mode Usage: gofuncgraph [-u wildcards|-x|-d] &amp;lt;binary&amp;gt; [fetch] [flags] Flags: -d, --debug enable debug logging -x, --exclude-vendor exclude vendor (default true) -h, --help help for gofuncgraph -t, --toggle Help message for toggle -u, --uprobe-wildcards strings wildcards for code to add uprobes 详细帮助信息：</description></item><item><title>eBPF BCC框架：helloworld</title><link>/blog/2023-09-15-ebpf_bcc%E6%A1%86%E6%9E%B6helloworld/</link><pubDate>Fri, 15 Sep 2023 16:36:33 +0800</pubDate><guid>/blog/2023-09-15-ebpf_bcc%E6%A1%86%E6%9E%B6helloworld/</guid><description>怎么写eBPF程序 # 目前写eBPF程序的话，一般要通过C语言来写，python、golang写的都是用户态的部分，涉及到内核部分的操作都是要借助C语言来写，然后通过编译器将C部分编译成字节码，用户态部分只是借助bpf()系统调用将字节码程序提交给了eBPF子系统去运行。
实际上任何高级语言都可以写用户态部分，但是写内核态部分的eBPF程序需要写C语言，编译器会将C语言部分编译成target=ebpf的字节码，所以现在有很多框架比如BCC+python以及Cilium+golang等，都是对eBPF字节码操作、系统调用操作的一些封装。
ps：如果你是用Rust的话，那么确实可以直接写eBPF程序，不用依靠C，一般常用的是Rust aya这个框架。
从0开始写eBPF程序 # 前面多次提到了eBPF程序编写、执行的大致过程，但是介绍的还是太粗略了，也不打算在这么几个简单的总结性文档中，把细节都介绍清楚。
我们可以先看下，如果手把手从0开始写eBPF程序，大致需要经历哪些操作，看图：
需要被简化的一些操作：
用C语言先写eBPF程序，然后使用编译器（如clang）将其编译为target为bpf的字节码程序，然后通过系统调用将其提交给eBPF子系统执行。这一步如果没有BCC这样的框架封装下的话，那么操作起来就有一点啰嗦。 还有你编译eBPF程序时要用到的很多头文件之类的设置，可能就比较麻烦。 还有eBPF程序执行时，那些结果存储到不同的数据结构，和不同语言的类型系统如何对接，如何方便的读取，全部自己从0开始搞也很麻烦。 其他的； 所以现在有BCC、Cilium、Aya这样的一些eBPF框架来简化这一些工作，我们可以先从BCC开始，这个项目比较早、成熟，用的人也多，也被集成到了Linux YUM源中，可以直接安装bcc、bcc-tools包来尝鲜。
从helloworld开始 # 现在就开始使用BCC来写几个helloworld，让大家了解下一个简单的eBPF程序大致是如何写的，熟悉下其结构，后面虽然不一定自己写，但是了解已有的这些工具的实现细节，以及如何调整来满足自己需要，还是有帮助的。
file: helloworld.py
#!/usr/bin/python3 from bcc import BPF program = r&amp;quot;&amp;quot;&amp;quot; int hello(void *ctx) { bpf_trace_printk(&amp;quot;Hello World!&amp;quot;); return 0; } &amp;quot;&amp;quot;&amp;quot; b = BPF(text=program) syscall = b.get_syscall_fnname(&amp;quot;execve&amp;quot;) b.attach_kprobe(event=syscall, fn_name=&amp;quot;hello&amp;quot;) b.trace_print() 分析下其结构：
导入bcc中的bpf program是一段c语言程序，b = BPF(text=program)，我们执行这个脚本时bcc框架会自动将其编译为字节码 它定义了一个hello函数，bpf_trace_printk会向ebpf子系统中的一个临时文件或者什么数据结构中打印hello world字符串 syscall = b.get_syscall_fnname是获得exeve函数调用的一个hook point b.attach_kprobe是在execve这个系统调用入口处通过kprobe系统调用创建一个探针，当执行到这里时会触发trap，内核会回调函数hello去执行，这里的hello就是上面C语言中定义的函数 b.trace_print会从取出前面打印的hello world字符串取出来打印出来。 这就是一个极简的helloworld的示例，当我们执行它时，它就会跟踪所有的execve的系统调用，每次触发这个系统调用时，就会打印上述helloworld字符串信息。
ps：执行ebpf程序时，需要使用root权限。</description></item><item><title>eBPF原理及实践：什么是eBPF</title><link>/blog/2023-09-15-ebpf%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E8%B7%B5%E4%BB%80%E4%B9%88%E6%98%AFebpf/</link><pubDate>Fri, 15 Sep 2023 11:57:13 +0800</pubDate><guid>/blog/2023-09-15-ebpf%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E8%B7%B5%E4%BB%80%E4%B9%88%E6%98%AFebpf/</guid><description>eBPF是一项革命性的内核技术，它允许开发人员编写自定义的代码，然后被内核动态加载后执行，以此来改变内核的执行行为。它的这个特点能够帮助实现高性能网络、更好的可观测性、更细致的安全分析工具。
eBPF的前身：bpf # 1、eBPF的前身是bpf（BSD Packet Filter），最早它在1993年论文中有Lawrence Berkeley National Laboratory的Steven McCanne和Van Jacobson提出，它是一种类似字节码虚拟机的东西，有自己的指令集，你可以通过它来编写程序然后交给这个小的虚拟机去执行，这个指令集非常像汇编。比如你可以用它来写包过滤的逻辑（接受或者拒绝一个网络分组）。在这篇论文中可以找到其他一些更加复杂的示例程序，论文名：The BSD Packet Filter: A New Architecture for User-level Packet Capture。
2、1997年，Linux内核版本2.1.75首次引入了BPF，BPF也就开始成为了Berkeley Packet Filter的简称，主要用在tcpdump这些工具中来实现高效的网络包的跟踪。
3、时间快进到2012年，Linux内核版本3.5中引入了seccomp-bpf，它能够控制是否允许用户态应用程序执行系统调用，举个例子，我们启动一个docker容器，如果不添加特殊的选项控制，在docker容器内部去调试程序的时候是执行不了的，因为Linux系统中程序调试需要利用系统调用ptrace，但是ptrace往往都是被默认不允许的，发挥作用的就是seccomp-bpf，这里有一篇文章介绍了seccomp+ptrace调试原理的文章：https://zhuanlan.zhihu.com/p/606317619。seccomp-bpf是首次开始将bpf从包过滤这个范畴开始向其他范畴扩展。到今天发展到eBPF这个阶段，其实与最早的“包过滤”已经没有多大关系了。
从BPF到eBPF # 随着BPF在Linux内核中的演进，到了2014年，从版本3.18开始可以使用eBPF将来称呼这项技术，全程就是extended BPF，这包含了几个比较明显的改变：
BPF指令级对64位机器做了高度的优化，解释器也基本上重写了； eBPF中增加了maps，BPF程序执行时可以访问它记录一些数据，这些数据可以在BPF程序间共享，也可以允许用户态程序访问它获取结果； 增加了bpf()系统调用，用户态程序通过它可以和eBPF程序进行交互，比如加载到内核、从内核卸载、访问maps数据等； 增加了bpf_这样的一些helper函数； 增加了eBPF程序验证器，验证安全的程序才可以被执行； 这是eBPF首次正式放出，但是不是结束，此后就开始了它的快速发展之路。
eBPF到生产系统 # 这里介绍下eBPF技术演进过程中的一些关键事件：
2005年Linux中就引入了特性kprobe，它允许在任意指令地址处设置trap，当执行到此处时允许回调用户自定义的函数。开发人员可以编写内核模块，将其中的函数设置为kprobe的回调以执行调试。 ps: 调试器一般也是使用这种指令patch的方式，区别在于kprobe回调函数是内核处理的，而调试器tracee执行时触发断点是内核通过信号通知tracer由tracer来执行的。 2015年的时候允许将eBPF程序连接到kprobe，kprobe可以回调eBPF程序了，这使得在Linux中tracing变得简单，为了更好的追踪Linux内核网络栈的各类事件，Linux中开始增加各种hooks允许eBPF程序进行更细致的观测。 2016年，Netflix的工程师Gregg大佬公开了他和团队在eBPF基础上的大量性能观测工具及实践，让基础设施、运维领域认识到了eBPF在这方面的巨大潜力。 2017年，Facebook开源了Katran这个基于eBPF的高性能L4负载均衡器，也是这一年，Liz Rice这位女强人对此也产生了浓厚的兴趣，并开始研究。 ps: Liz Rice 经常做些技术方面的分享，目前是 the chief open source officer with eBPF specialists at Isovalent, 也是 the Cilium cloud native networking, security and observability project 的创建者.</description></item><item><title>Linux性能问题排查60s</title><link>/blog/2023-09-08-linux%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A560s/</link><pubDate>Fri, 08 Sep 2023 16:30:30 +0800</pubDate><guid>/blog/2023-09-08-linux%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A560s/</guid><description>简介 # 最近在阅读Gregg大佬著作《BPF Performance Tools》，其中一小节作者提到了其在Netflix工程团队中践行的一个性能排查checklist，当遇到Linux性能问题时，前60s往往是借助这个checklist来进行排查，如果有必要，缩小范围后再借助其他工具进行进一步排查。我觉得这个简短的checklist还挺实用的，特地摘录出来分享下。
问题背景 # 这个checklist可以用来指导排查任意Linux性能问题，当我们知道有台机器性能（疑似）有问题时，我们就可以登录这台机器，按照这个checklist来进行前60s的快速分析。这也是Gregg自己以及Netflix工程团队实践中总结出来的。
对于很多刚入行后台开发的同学而言，我觉得这个还是比较有价值的，应该在日常工作中不断实践、不断加深对性能影响因素的理解。有位技术扎实的同事曾经这样说，一切都是可计算的、可量化的，比如判断对特定工作负载瓶颈是什么，cpu、内存、网卡？链路长短，网络延迟，然后大致的系统吞吐量是什么样的？他大致就能推算出来。
其实，Jeff Dean曾经在论文里给出过一些开发人员应该知晓的latency数据：
L1 cache reference ......................... 0.5 ns Branch mispredict ............................ 5 ns L2 cache reference ........................... 7 ns Mutex lock/unlock ........................... 25 ns Main memory reference ...................... 100 ns Compress 1K bytes with Zippy ............. 3,000 ns = 3 µs Send 2K bytes over 1 Gbps network ....... 20,000 ns = 20 µs SSD random read ........................ 150,000 ns = 150 µs Read 1 MB sequentially from memory .</description></item></channel></rss>