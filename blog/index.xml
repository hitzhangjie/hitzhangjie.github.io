<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on</title><link>/blog/</link><description>Recent content in Blog on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 06 Oct 2020 08:49:55 +0000</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>检测并解决数值计算溢出问题</title><link>/blog/2023-03-03-%E6%A3%80%E6%B5%8B%E5%B9%B6%E8%A7%A3%E5%86%B3%E8%AE%A1%E7%AE%97%E6%BA%A2%E5%87%BA%E9%97%AE%E9%A2%98/</link><pubDate>Fri, 03 Mar 2023 02:04:11 +0800</pubDate><guid>/blog/2023-03-03-%E6%A3%80%E6%B5%8B%E5%B9%B6%E8%A7%A3%E5%86%B3%E8%AE%A1%E7%AE%97%E6%BA%A2%E5%87%BA%E9%97%AE%E9%A2%98/</guid><description>问题背景 # 数据类型是个好东西，类型定义了一种数据组成以及允许在其上进行的操作。 数据类型是个好东西，它定义了一种最基础的“安全”，类型安全。
我们在进行数值运算时，有可能会“超出”类型本身的值域，但是受限于位宽限制，进而表现为“上溢出”。以a+b为例：
如果a、b都是有符号数，且其符号相同，有可能超过最大值、最小值而在值域空间中轮转； 两个正数相加，结果却是负数；两个负数相加，结果却是正数. 如果a、b都是无符号数，也有可能超过最大值而在值域空间中轮转。 这个很容易理解，今天我们想看下如何解决此类问题。
如何解决溢出问题 # 升级32位到64位？ # 这通常是第一反应，它可能是有效的，也可能无效。
有效：如果输入int32 a、b是有明确约束保证的，比如任意一个都必须在[-1*1&amp;laquo;31,(1&amp;laquo;31)-1]， a+b可能对int32可能会溢出，但是如果提升成int64则可以解决问题，前提有这样的约束保证； 无效：没有任何输入约束做保证，只是简单提升成int64 a、b是没有用的，极端情况，a=b=1&amp;laquo;36-1， a+b很明显就溢出了，这种就需要其他方法做保证。 设计上应该有上限？ # 在设计上就要有这方面的“数据”上的“安全”的意识，比如：
玩家每赛季的经验应该是有上限的，满经验后就提示玩家满经验，后续就不给加了； 比如用uint32表示经验值，那么加之前先测一下是否发生了溢出(v=orig+delta, 如果v小于任意一个则溢出) 这很好理解，正常情况下，v应该大于orig、delta，就是逻辑反嘛。 ps：不好理解？把值域想象成一个转盘，delta不可能让v在值域范围内“环绕 (wraparound)”/“转到”orig，反之orig也不能让v转到delta。 如果发生了溢出，则直接将v=maxUint32完事，多出来的就扔掉，提示玩家满经验。 或者，这里的满经验不一定要maxUint32，可以是认为设计好的一个小值，比如99999； 如果输入有约束，比较小比如int8 a, int8 b，那么至少可以保证 if a+b &amp;gt; 99999 then v=999999 是ok的， 也不会触及累积量v达到uint32最大值的情况。可能这种情况比较理想化了。 检查是否发生溢出？ # 言归正传，还是要有办法来比较可靠地检查运算结果a+b是否发生了溢出？
可以用大数计算来避免溢出，比如golang里面的math/big包。 比如int32 a,b相加，按int c=a+b的方式，c有可能是个溢出后的错误结果。 但是如果用大数计算，位宽充足可以算出正确结果，只要将其和maxInt32比较下即可知道是否发生了溢出。 如果确实发生了溢出，应该如何处理，如fallback到满经验值不再加经验。 也可以不用大数计算，通过一些有趣的副作用也可以知道是否发生了溢出。 比如在x86汇编中，可以通过 test OF,OF 来判断是否发生了溢出。 高级语言中，就没那么直接，比如go，得借助一些其他办法来判断，这就是这个math_test.go要测试的东西。 代码测试：运算时检测溢出 # 测试代码，请移步：https://github.com/hitzhangjie/codemaster/blob/master/math/math_test.go。
math_test.go中定义了两个函数safeSignedAdd、safeUnsignedAdd来对有符号数、无符号数加法进行安全的计算：</description></item><item><title>how go.mod works?</title><link>/blog/2022-11-24-how-go.mod-works/</link><pubDate>Thu, 24 Nov 2022 15:31:49 +0800</pubDate><guid>/blog/2022-11-24-how-go.mod-works/</guid><description>go.mod/go.sum内容 # go.mod里面包含的信息包括：
当前module构建要求的最小go版本 依赖的module及校验和信息 为了方便本地开发测试的一些replace信息 这里不讨论vendor相关的modules.txt中的内容。
最小go版本号 # 我们举个例子来描述下。
如果当前module的go.mod是go 1.16，等价于编译的时候go build -gcflags &amp;lsquo;-lang=1.16&amp;rsquo; / go tool compile -lang=1.16。
假设我们现在安装的go版本是go1.19。
这种情况下执行编译测试：
如果我们用了范型（go1.18开始支持），go编译器编译时会检查， 本来go1.19肯定能编译1.18的范型代码，但是它会报错出来，因为go.mod里声明的go版本，是当前项目支持的最小go版本，有可能别人不是1.19而是1.15,1.17，所以要报错提示下
我们还没有用那些1.16.5以后的新特性非得要新版本的go来编，所以之前能正常编。
如果我们安装的go1.15，go.mod里面的1.16高了，也会先尝试编译，编过了就编过了，编不过就报错最小版本是1.16.5 比如机器上现在是1.19，可以go.mod改成1.20正常编过
如果因为-lang编译导致的编不过，如果go.mod里面的版本比当前安装的版本高，还会打印出来 module requires go 1.21，提示安装新版本
依赖信息 # 依赖的module，除了指明importPath，还要指明version，才能完整指明一个依赖。这个应该没什么疑问，所以大家都会提交go.mod文件。
再说下校验和，有什么用呢？防止包内容被篡改。有些同学因为什么原因导致校验和经常冲突，需要解决冲突，所以直接不提交go.sum文件了，这是十分错误的。
有同学可能会觉得这些繁琐的步骤很荒唐，其实并不是，可重复的制品构建，是一门非常重要的工程上的保证手段，为了达到此目的，甚至还有封闭构建、构建容器等其他方法来提供进一步的保证。
本文小结 # 本文简单记录了下go.mod/go.sum相关的知识点，可能对刚接触这块的同学比较有价值 :)</description></item><item><title>go如何触发垃圾回收的</title><link>/blog/2022-11-20-go%E5%A6%82%E4%BD%95%E8%A7%A6%E5%8F%91%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</link><pubDate>Sun, 20 Nov 2022 20:39:22 +0800</pubDate><guid>/blog/2022-11-20-go%E5%A6%82%E4%BD%95%E8%A7%A6%E5%8F%91%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</guid><description>前言 # go触发GC有这么几个时机，内存分配时触发mallogc，定时触发sysmon，手动触发runtime.GC、debug.FreeOSMemory，其中内存分配时触发是go是重中之重，go runtime以此来平衡好内存分配、内存回收的节奏以让内存占用维持在一个合适的水准。本文对内存分配过程中触发GC的一些设计考量进行了总结梳理。
应该听过“mark assist” # gc过程中mutator分配内存时可能会被搞去做assistg,去辅助扫描标记一些对象是否该回收的工作,当我的辅助标记内存数量减去要申请的内存数,如果为负数时,相当于我申请的比辅助标记的多,相当于欠债了,这个时候我就得去做些辅助标记的工作 gcAssistBytes:
然后根据当前内存使用情况\扫描情况\下次GC的heapgoal,计算出我应该辅助标记多少,才能保证达到堆大小时GC标记工作恰好能完成,让我去干活 这个时候干活之前会先检查下bgMarkWorker因为扫描工作贡献的信用分,然后我可以借用这个信用分来偿还债务,以减少扫描工作,或者完全避免扫描工作 如果依旧欠债,那就干活呗,后面会执行到gcDrainN,去执行一些标记类的工作 这些标记类的工作从何而来呢,比如写屏障记录下来的一些需要去扫描的对象 执行完了这个扫描之后,这个assistG.gcAssistBytes就会加上扫描的字节数,相当于攒的一点信用分 干完这些之后,才允许你申请内存\分配对象,哈哈哈! goroutine可以去做些mark assist之类的工作的前提是，GC已经进入了GCMark阶段，那内存分配期间GC是什么如何被触发的呢？
GC什么情况下被触发的 # 关于什么时候触发GC，严谨一点，内存分配期间何时触发的GC，这里不考虑sysmon触发、手动runtime.GC()触发，ok。
GOGC\GOMEMLIMIT\heapGoal # 我们应该都这样的认识阶段，通过GOGC、GOMEMLIMIT可以计算出下次GC时的heapGoal，等堆内存占用达到这个heapGoal时会触发GC。
但是严格来讲，理解成接下来内存占用达到heapGoal才触发GC，是不正确的。
引入GC Trigger # 为了触发GC，还有一个概念，叫GC trigger，它的值heapGoal要小些，在GCOff阶段，内存分配过程中会检查当前heapLive大小是否超过了这个trigger，是则启动gc（gcStart） 那个协程来负责检查是否启动gc，可以理解成所有的协程，协程如果是申请大内存（&amp;gt;32K）则一定会做上述检查，小内存为了效率则不一定每次检查，当如果申请小内存（tiny or small）如果过程中span不足发生了refill也会做上述检查（shouldhelpgc） 当启动了GC之后，接下来goroutines如果涉及到内存分配，就会转入markAssist阶段，要分配多少，先要干一定量的标记扫描的活才行（内存debt/assist设计） 那么heapGoal干嘛用的呢，前面提到的内存debt/assist设计，就是为了在当前堆大小达到heapGoal时尽量完成内存的标记扫描，将markbits改成allocbits，未使用的就可以复用或者等下个GC cycle阶段回收 所以从GC trigger到heapGoal，这中间是有一些考量的，如果只认为GC heapGoal控制GC的触发，其实是认识不到位的。ps：可能在这这个提案 GC pacer redesign 实现之前确实是根据heapGoal来触发的，但是这会导致内存的不受限制的增长。
GC Trigger计算 # 那么这个GC trigger是如何计算的呢？
首先它不能比heapGoal小很多，那可能会导致GC启动过早，写屏障打开后程序latency会上升，而且如果内存分配比较快GC一直触发运行，期间分配的对象会一直标记为black，Rss会上升 也不能过晚触发，可能导致标记扫描阶段assistG的工作量过大，latency会比较明显，而且会堆大小增长会超出预期。 至于如何计算的，可以先看下上面这个提案中关于GC trigger的设计，然后翻下源码瞧瞧……额，还是简单总结下吧：
明确下目标，GC trigger是用来确定何时触发GC的一个值，当内存分配导致堆大小变化时会检查当前heapLive&amp;gt;trigger来决定是否触发GC（申请大内存必检查，申请小内存为了效率一般不检查，但在span不足refill后检查） GC trigger如何计算出来的： 首先根据GOGC、GOMEMLIMIT算出下次GC的heapGoal， 然后根据minTrigger=heapMarked+(heapGoal-heapMarked)*0.7， 然后maxTrigger=heapMarked+(heapGoal-heapMarked)*.0.95，如果堆比较小就用这里算出的值意味着总有一个buffer来赶在内存占用达到heapGoal之前启动GC。如果堆比较大但是有没有多少扫描工作，就用heapGoal-defaultHeapMinimum(4MB)来作为maxTrigger，这也是一种优化。 ps: 这里的heapMarked表示上轮GC标记结束时堆大小。这两个值，相当于确定了一个候选的触发GC的heapLive范围，最终trigger值一定要大于等于minTrigger，一定要小于等于maxTrigger。 确定trigger： 确定runway，根据上轮GC过程记录的consMark（程序分配内存、扫描内存量的比值）、实际的扫描内存的量（heap+stack+global）以及并发标记执行阶段mutator:collector的CPU执行时间的比值3:1，可以大致算出下一轮GC期间内存使用量能涨到多少，这个源码中选了个词叫runway，意思是我们内存使用量能走多远。 很明显如果这个值如果大于heapGoal说明我们很可能会让堆占用走高，此时需要更激进地触发GC，所以此时的trigger就选下界minTrigger。 如果这个值比比heapGoal小，那就用goal-runway作为trigger，但是这个值表示的时啥？如果这个值比minTrigger小就用minTrigger。 前面还算了个最大trigger，如果这里的trigger值比maxTrigger还大，那trigger要改成maxTrigger。 Put it together # OK，现在知道了trigger值是怎么详细计算的了，好，我们继续串一下：</description></item><item><title>go1.18泛型支持</title><link>/blog/2022-11-10-go1.18%E6%B3%9B%E5%9E%8B%E6%94%AF%E6%8C%81/</link><pubDate>Fri, 11 Nov 2022 01:00:44 +0800</pubDate><guid>/blog/2022-11-10-go1.18%E6%B3%9B%E5%9E%8B%E6%94%AF%E6%8C%81/</guid><description>go1.18 泛型支持 # 关于泛型编程 # 首先什么是泛型呢？ # Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters.
泛型编程有啥好处呢？ # cleaner code and simpler API (not always) improve code exectution performance (not always) 没有泛型的日子 # 如何应付的 # go1.18之前苦于没有范型编程，开发人员一般会这么做：
go编译器对内置类型有一定的范型支持，比如new、make、len、cap go支持reflection和interace，通过这两个一定程度上可以模拟范型的能力 go支持//go:generate，通过自定义工具可以生成一些“重复”代码 痛点依然在 # 即便是通过反射、interface来模拟也把风险从编译时类型安全推到了运行时检查部分，生成代码也会有大量重复性代码……所以痛点依然存在。
go1.18中终于解决了这个问题，虽然现在来看还没那么尽善尽美，但是总算在路上了。
go泛型知识点 # go1.</description></item><item><title>go垃圾回收调优</title><link>/blog/2022-11-10-go%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E8%B0%83%E4%BC%98/</link><pubDate>Thu, 10 Nov 2022 10:54:26 +0800</pubDate><guid>/blog/2022-11-10-go%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E8%B0%83%E4%BC%98/</guid><description>相关背景 # 在go1.19之前，go程序内存调优的方式主要是通过环境变量GOGC（或者debug.SetGCPercent(?)）来控制，
它的效果是影响go runtime计算nextGC heapGoal的大小：
较早的版本计算方式为：heapGoal = heapMarked + (heapMarked) * GOGC / 100， 后续go迭代时发现非堆内存也是有影响的，于是go1.18完善了下 heapGoal = heapMarked + (heapMarked + GCROOTs) * GOGC/100，这里的GCROOTS=(gstacks+globals) GC pacer的目的就是为了根据上述公式计算下次GC的heapGoal，然后在必要时（比如malloc时）决定是否要GC。
默认初始heapGoal大小为4MB，如果靠GOGC来控制的话，会比较频繁触发GC，对绝大多数server程序而言频繁GC占比较多CPU，程序整体吞吐、响应延迟会受一定影响。
所以业界一般会通过两种方式来调优：
ballast，利用一块不用的大内存（比如1GB），来推高下次GC的heapGoal，通过这种方式来降低GC频率 GC tuner，动态设置GOGC，定义一个对象为其设置finalizer，每轮GC结束时触发它并检查当前进程当前的内存占用情况，并与允许的最大内存占用进行比较，并计算出达到最大内存占用才触发GC时GOGC应该设定的值（其思路和go1.19 GOMEMLIMIT类似） 项目以前的方案 # 项目以前使用的是go1.16.5，这个版本中也只有GOGC一个控制GC的选项，使用的是ballast的方案：
在服务初始化阶段去初始化一个大内存而推高下次GC时的heapGoal 不同程序可能对内存需求不同，配置文件中允许自定义ballast大小，默认为1GB 包括业界在内都是介绍了ballast如何使用：
全局变量声明，垃圾回收器会认为其在整个进程生命周期内reachable 局部变量声明，通过runtime.KeepAlive(&amp;hellip;)来欺骗垃圾回收器这之前对象reachable 但是，好像只看到了一派祥和，我们使用时却遇到了Rss占用问题。
问题1：ballast占物理内存 # 在测试环境（很多套测试环境）都有比较大概率发现服务在几乎空闲时，物理内存占用竟然高达1.1g…这很不符合常理。
通过pprof跟踪内存分配，发现内存分配比较大的路径就是这个压舱石（pprof mem采样是看的虚拟内存）。
然后top、pmap等跟踪可疑进程发现其确实存在1GB左右的anon区域，且该区域为dirty**（其实gdb把内存dump一看全是0，就很容易联想到类似对象分配后memset的操作）**。
根据了解的go GC、内存分配器相关的知识，了解到go向操作系统申请内存时通过mmap的方式，释放内存是通过madvise+MADV_DONTNEED/MADV_FREE的方式。
go1.12的时候改成了FREE默认代替DONTNEED，这两个选项是有区别的，详细的可以看下man手册（man 2 madvise），FREE的效率更好一点，但是也有一些不好的副作用。
go1.</description></item><item><title>裁员带给我的思考</title><link>/blog/2022-10-01-%E8%A3%81%E5%91%98%E5%B8%A6%E7%BB%99%E6%88%91%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Sat, 01 Oct 2022 22:03:43 +0800</pubDate><guid>/blog/2022-10-01-%E8%A3%81%E5%91%98%E5%B8%A6%E7%BB%99%E6%88%91%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>本文简介 # 今天是国庆节，没抢到合适的高铁票去武汉，今天索性在深圳休息一天再走。回头看今年各互联网大厂大范围裁员的事情，看法也更全面了许多，就不讨论那些企业家有没有责任感的事情，就从家庭、个体工作生活角度谈下自己的一点看法。
裁员事件 # 听到一点风声 # 从2019.7之后，我一直在从事PCG微服务框架的支持工作，我们会每两周开一次PMC迭代例会同步下规划及进展，有一次会议上，我们评估各语言版本微服务框架今年可能新增的实例数的时候，大家按照过去21年的新增趋势评估，结果有个领导说我们太乐观了，说今年PCG有很多业务要面临下线。
开始看到瞄头 # 这是我当时听到的最早的“小道消息”了，但是没有当回事。后面过了一阵子，突然听说各大厂开始裁员了，但是各种消息、公关消息夹杂在一起，事情没发生在自己身边，也没觉得有多大影响。后面就听说同“幸福线（信服线）”隔壁几个中心开始裁员了，后面还是开例会的时候，有个子项目trpc-dart负责人说他们那边人员变动比较大，暂时没人力支持需求开发了。我当时开始想这么严重么，后面陆续开始关注这方面消息，涉及到的人是比较多。
我也难逃此劫 # 再后面，就直接裁到我们这边了，先是组里面的工作年限短的收到毕业通知，裁完一波后，据说裁了很多人没降什么成本，于是开始裁高T、裁组长…后面就到我头上了。
其实，我们技术总监等其他同事有跟我通过气，意思就是说裁到后面实在没办法了，当时收到消息的时候，虽然有点难过，但是很快就释然了。难过是因为自己好歹也为业务技术支撑、为公司微服务框架做了不少贡献，结果因为业务不景气、为了降“成本”就把工资高（贡献也多啊）的老员工直接裁掉，虽然说赔偿比较良心，但是你懂得心理上还是不认同这种粗暴的裁员方式的，那时候真的准备离开腾讯了。
我的看法 # 业务调整是迟早的事 # 业务不景气已经不是一天两天了，只不过以前公司在各方面压力比较小，像这些不挣钱的业务也就这样活下来了，还养活了一部分人。
只是心理上难接受些 # 像我是一毕业就SP进入了腾讯，6年时间先后做了几个业务，也做了些公线支持的工作，框架、规范、工具、培训分享等，内心对公司的归属感是很强的，这次简单粗暴的大范围裁员，让人感觉这些年算是瞎了眼。公司确实是家好公司，但真的不见得每个人都优秀。行业可能竞争比较激烈，可能业务发展遇到了瓶颈，但是你能感觉到有些领导在犯错，光嘴说的好听，动作搞的很大，但是树立一个个山头、做的事实少、能有高价值产出的就更少，业务也没有好的起色。
整体形势差换工作难 # 已经是山头、嫡系林立的丛林社会了，这种情况下即便是想留下有能力的员工也近乎不可能，谁来评判呢。而且这个时候，领导们只想留下自己的嫡系，而非有能力的员工。有能力的员工，自然不用担心找不到工作机会，理论上是这样，前提是得有这么多就业岗位。今年大形势不好，很多公司都在裁员，最后这被裁的一两万同事要到外面去找机会，大厂普遍裁员，没这么多机会，很多同事只能去第二梯队的企业（业务不一定不行哈）。
我旁边的一些同事，从接到通知就开始准备找工作，等到lastday离职哪天，有的也还没有拿到offer，当然最后他们找到offer了，但是这确实能看出来就业形式比较差。
对公司不再有归属感 # 这种事情发生之后，咱也不哭不闹，不过是看清了这个丛林社会中，谁也靠不住，公司再好它也不是家，自己首先要考虑的还是家庭、家人、自己，摆工作的重要性往后放放。工作上，没了你随便找个人就能代替，再说了，就算是没人代替你大不了短期内工作没法推进了，但是依旧还可以维持。说白了，自己对公司业务来说没那么重要。没必要掏心掏肺的，为了点鸡毛蒜皮的小事牺牲自己那么多精力去忠心耿耿地工作，影响到家人就更蠢了。
现在对腾讯已经没有任何“家”的感觉了，也不在KM上讨论问题了，公司内发生的事情也不再关心了，公司外的形象也不想去维护了，努力做好自己分内的事，拿钱干活。但是努力做好本职工作、提升自己、提供更好的产品服务，这个目标不能打折，程序员的自我修养。
要明白来工作是为啥 # 人活着，时间精力有限，心里能装的东西有限，归根结底，我们还是要多体验些美好的东西，少为这些乱七八糟的事情浪费心神。裁员就裁员，要么活水去好的业务团队，无论学习、沉淀、挣钱都还有保障，要么出去找更好的机会。我来腾讯不就是为了来锻炼长经验的吗，多挣点钱更好。
还是要坚持自己的想法，提升技术的同时，多看看有哪些小成本的试错机会能致富的，比如羊了个羊这样的小游戏，搞一个成功了，说不定就财务自由了，即便是没这么成功，多几种收入来源也是不错的。
本文小结 # 最后，祝愿所有一起奋斗过的小伙伴们保重身体，找到合适的工作机会。</description></item><item><title>分布式锁方案的思考</title><link>/blog/2022-09-25-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Sun, 25 Sep 2022 15:31:53 +0800</pubDate><guid>/blog/2022-09-25-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>问题背景 # 在微服务架构下，经常面临一些事务性处理需要考虑一致性的场景，笔者工作过程中，很多场景最终都采取了可用性优先、数据最终一致的方案。最后我们可能也会结合一些对账之类的手段来发现异常并修复不一致的数据。
归根结底，是因为微服务架构下上述事务性处理方案没有保证刚性事务的ACID原则，其弱化了对A、C的控制，OK，这种方案并非不可接受的，只要业务场景适用即可。这里还有一点就是I（Isolation）的原则，如何保证微服务架构下事务处理的隔离性呢？
分布式锁，就是大家常用的方案，只不过关于对分布式锁的认识，可能大家认识的程度并没有自己认为的那么到位。
常见方案分类 # 分布式锁，归根究底是为了保证任务的排他性执行。但是为了排他性执行的初衷却可能是不同的，所以我们接下来会先按照要解决问题进行分类。然后呢，考虑到可用性、正确性，实现分布式锁的具体方法也是不同的，然后我们也可以按照实现方式进行分类。
按解决问题分类 # 解决效率类问题：为了避免资源浪费，如每天统计下业务下所有服务接口成功率数据，这类定时任务也是多机部署的避免单点问题，但是只要一台机器执行就行了，属于解决效率类问题。没有必要多台机器执行，但是即便都执行了也没啥影响，只是后面执行的覆盖掉前面的执行了，仅此而已。 解决正确性问题：任务必须排他性执行，如果并发执行则存在正确性问题。比如用户购买游戏道具时需要读取玩家金币数、扣金币、写回，这里涉及到典型的Read、Modify、Write的问题，如果这个操作时序不是排他性的，就掺杂着重置、送礼等各种可能修改金币的操作时序，则会导致正确性问题。 按实现方式分类 # 基于缓存实现：比如利用redis、memcache等实现，分布式缓存一般提供了get、set的能力，允许给key、value指定版本、排他性、过期时间属性，来实现分布式锁。 基于共识算法实现：比如etcd、zk这类底层有raft、zab共识算法支撑的组件，借助他们可以比较可靠的实现分布式锁，至少能保证分配锁时不会导致client错误持有锁。 在实际实现、使用分布式锁时，我们多数时候是冲着正确性去的，但是方案本身其实是不完备的，但是我们却将其当做了“正确的”。
常见方案介绍 # 基于redis单机版 # 比较常见的就是单机版的redis实现版本，如下所示：
# 加锁操作 SET resource_name my_random_value NX PX 30000 # 解锁操作 if redis.call(&amp;quot;get&amp;quot;,KEYS[1]) == ARGV[1] then return redis.call(&amp;quot;del&amp;quot;,KEYS[1]) else return 0 end 实际情况是单机版redis存在单点问题，为了解决这个问题，通常又会给redis master挂个slave来备份数据，但是redis的备份机制是异步的，所以仍然存在主备切换时丢失锁数据而导致的错误加锁情况，解决不了正确性。
基于redis集群版 # 集群版的redis，用的比较多的是redlock算法。redlock算法主要是解决单点故障问题，它的主要思想是，假设集群中有N个（建议值5）master节点，这些master节点及其replicas各自维护一些不相干的keyslots。加锁时，client先获取本地时间，然后串行地向N个节点发起请求，是串行的。
至于详细的实现，redis官网上有这些推荐实现的github repo链接，可以自己去找找看。
其实这个算法也解决不了网络波动、分区极端场景下，依然会导致client错误持有锁的情况，比如1、2、3、4、5个节点，一开始网络正常client1持有了1、2、3上的锁，后面网络波动导致client只能访问3、4、5，并且3发生了主备切换而备份上缺少数据，client依然能在3、4、5上获得相同锁。
尽管redlock算法提出了一些可以缓解正确性被破坏的想法，但是仍然不能保证分配锁时的正确性。
基于etcd实现 # etcd本身是基于raft算法实现的副本的状态复制，是有可靠的共识理论支撑的工程实现，另外etcd号称其raft算法实现有着比paxos算法更好的性能（这个没求证，多数情况下paxos算法可能性能更优点，也不一定非得有master节点），感兴趣的可以自行了解。
基于etcd的分布式锁实现，已经内置在etcd中了，直接使用即可。
因为示例代码的篇幅原因、go.mod设置等，我们就不在这里占用太多篇幅了，感兴趣的可以直接查看这里的示例代码：https://github.com/hitzhangjie/codemaster/blob/master/distlock/main.go。
思考下 # 我们从解决效率问题的分布式锁，到解决正确性问题的分布式锁，对锁分配正确性的要求提升了一档，关于其实现方案，也从基于redis、redis集群版的方案，也过渡到了基于raft算法的etcd实现（其他的基于paxos、zab共识算法的类似就不介绍了），我们解决了锁服务分配锁时的正确性问题，但是这样就能保证任务排他性执行了吗？
不能！client在使用锁的时候，可能会出现如下情况：
client出现了崩溃、逻辑bug，导致锁没有被正确地释放掉，如果锁没有过期时间，将导致其他client加锁时出现死锁； client加锁时设置过期时间，但是过期时间可能设置的过短，锁过期被etcd清理然后又重新分配给了其他client，然后旧client还以为自己别锁保护的临界区内还可以肆意妄为，导致并发执行错误； client加锁时设置了合理的过期时间，但是自身因为其他原因出现了一定时间阻塞，恢复后继续执行，但是锁实际已经过期被释放； client执行操作时锁确实是有效的，但是在其发起对下游的请求后，下游继续处理期间锁过期，其他client持有了锁并发继续执行操作； 看到没，即便是锁分配是正确的，client使用锁时依然无法100%保证正确性，这个问题能100%解决吗？不能，但是可以尽可能缓解，比如合理设置锁过期时间，比如请求方调用下游服务时，把锁信息带给下游让下游能够去锁服务校验锁有效性。</description></item><item><title>一致性hash负载均衡方案的思考</title><link>/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Sat, 06 Aug 2022 17:16:57 +0800</pubDate><guid>/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>常见的负载均衡策略 # 客户端完成被调服务的服务发现后，获得了一批实例节点列表，现在要借助合适的负载均衡算法来选择一个实例完成请求处理。
常见的负载均衡算法包括：
轮询：每一次网络请求按照顺序发放给下节点列表中的下一个节点，这种情况适用于节点配置相同并且平均服务请求相对均衡的情况 加权轮询：考虑了不同节点的硬件配置情况，如节点a、b、c性能有低到高，权重设置为1、3、6，则按照权重分配10%、30%、60%的请求给到节点，这种可以避免高性能机器负载低、避免性能差机器过载 随机：随机选择一个节点来处理请求，这种在请求量比较大的情况下能达到相对均衡的分布，同样适用于机器配置相同的情况 加权随机：考虑了不同节点的硬件配置情况，类似加权轮询，只不过选择下一个节点时是基于随机选择，而非轮询的方式 余数hash：根据某个key对节点数做取模运算，比如节点数为n，根据请求中的m = uid % n，表示用节点列表中第m个节点来作为服务节点。当key分布范围比较广能达到相对均衡，选择key字段的时候要考虑下key分布情况。使用hash的场景，一般是因为后端节点有状态可复用（或者希望借此减少并发冲突），但真实环境中，节点故障是常态，尤其是在容器化部署场景下自动化扩缩容，hash会导致集群中所有节点状态无法被复用。一般会用一致性hash代替hash。 一致性hash：一致性hash是对hash的优化，一致性这里强调的就是节点加入、离开后尽量保证大多数请求仍然能路由到该路由的节点，而不是新加入的节点，同时为了避免新加入、离开节点导致的负载不均衡问题，引入了虚拟节点的概念，每个物理节点都对应着hash换上一定数量的虚拟节点，这些节点混在一起，能实现各个节点负载的相对均衡。节点数量该选择多少呢？一个比较直观的认识是可能虚拟节点越多越均衡，但是数量过多也会有开销，这与虚拟节点的hash计算、存储有关系，本文后面讨论。 按响应速度：有些负载均衡设备，会根据与后端服务节点间的ping延时来选择一个响应时间最短的。类似的也可以根据client、server之间的ping延时或者请求处理响应时间来选择。 按最少连接数：对于某些请求处理时间比较长的场景，如ftp传输等，一个tcp连接存在的时间可能比较长，连接数比较多的可能代表该节点负载比较重，因此会开率选择连接数比较少的来提供服务。 其他 负载均衡算法有很多，之所以这么多也是因为应用场景的差异，根据合适的场景选择适用的负载均衡算法。
调研一致性hash策略及其可替代方案 # 对一致性hash方案及其可替代方案进行调研、对比。
余数hash # 余数hash，简单讲就是那个key去算出hash值，然后对节点数量取模，m = hash(key) % n，用节点列表中的第m个节点去做请求处理。 如果节点数变化非常不频繁，或者说key remapping（rebalancing）过程中带来的开销不大、影响不大，那用余数hash也无所谓。
但是现实场景中，比如一些有状态服务，如果remapp后映射到了与先前不同的节点，或者容器化部署时节点数经常变更，不满足适用余数hash的条件。
比较常见的对策，就是采用一致性hash。
一致性hash # 简要介绍 # 一致性hash能够缓解节点加入、离开时rebalancing导致的一些hash节点改变的问题，在以下场景中就更有优势：
服务是有状态的，这样大多数路由仍然能路由到原来的节点，状态可以复用；
即使服务不是有状态的，将原来路由到节点n的请求及其后续请求继续路由到该节点，也可能存在更好的局部性处理（locality），
举个例子（可能不很恰当哈）： 比如有个个人展示页要展示头像昵称、最近游戏记录，假设之前有个什么客户端请求uid=xxx的请求路由到了节点n拉取过了昵称头像并cache，后面该展示页也路由到该节点的话就可以复用该cache。
假设key空间中值数量为k，节点数为n，那么当发生remapping时，笼统估算有k/n不命中原来的节点。
关于实现 # 关于一致性hash的实现：
构建一个一致性hash环，一个数组就可以实现 选定节点的key，如ip，hash(key)，然后再hash换上对应位置存储该节点信息，考虑到hash环大小需要适当取模 考虑到各节点的负载平衡，引入虚节点，每个物理节点对应为k各虚节点（k多大？），各个虚节点的hash值计算就分不同方法： key多大？兼顾计算效率和负载均衡性，因为节点数提前无法预估，可能要选择一个更好的经验值 引入k个hash函数，hash1(key), hash2(key), hash3(key)&amp;hellip;.hashK(key)，分别设置到hash环上 针对key，构造key_1, key_2, key_3..，keyK，使用同一个hash函数分别计算上述key的hash，并在hash环上设置其节点信息 TODO 这里的计算方式的选择，虚节点数多大（过少还是会不均衡），过大计算效率慢（多次计算hash），另外多个hash还是构造多个key也可能会影响到负载的均衡性，需要针对性的测试。 现在有个请求，比如用玩家userid作key，hash(key)得到值之后，因为一致性hash环是个收尾相接的有序数组实现的，可通过二分查找（查找第一个大于等于该hash(key) )的节点，复杂度O(logn) 一致性hash，对于带权重的也能支持到：比如a机器比b机器性能高一倍，希望其处理两倍于b的请求，那么就可以让a机器的虚节点多一倍。但是如果管理的节点数量成千上万之后，hash环上存储这些虚节点的开销就不能忽略了。</description></item><item><title>内核中的内存检查工具</title><link>/blog/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%A3%80%E6%9F%A5/</link><pubDate>Fri, 08 Jul 2022 00:52:25 +0800</pubDate><guid>/blog/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%A3%80%E6%9F%A5/</guid><description>kmemcheck.txt # kmemcheck用于内核的未初始化内存的动态检测，它工作在内核态，与工作在用户态的 memcheck实现机制不同。虽然kmemcheck不如memcheck精确，但是已经足够使用的了。此外，kmemcheck会使用更多的内存，增加系统负载，仅适合用于内核的调试。
kmemleak.txt # kmemleak是一个工作在内核态，用于检测内核中内存泄漏的工具，与工作在用户态的内存泄漏检测工具memcheck加参数&amp;ndash;leak-check工作时效果类似。
为了加深对内存管理的理解，应该查看下这两个工具的源代码。</description></item><item><title>内核探针kprobe工作原理</title><link>/blog/%E5%86%85%E6%A0%B8%E6%8E%A2%E9%92%88/</link><pubDate>Fri, 08 Jul 2022 00:45:45 +0800</pubDate><guid>/blog/%E5%86%85%E6%A0%B8%E6%8E%A2%E9%92%88/</guid><description>内核探针 # 内核中用来方便调试的探针（probe）主要有以下几种：
kprobe，可以对任意指令地址处安装探针 jprobe，可以对函数入口地址处安装探针，方便获取参数信息 rprobe，或者称为retprobe，顾名思义，主要用来观察retvalue 这几种探针的实现原理大同小异，详细的工作原理可以参考 kprobes.rst。
kprobe工作原理 # 联想下调试器中断点的工作方式，kprobe可以通过断点的形式来实现：
记录目标地址addr处的原始一字节指令 将目标地址处的指令替换为0xcc（int3就是软件断点），并注册该地址处对应的kprobe，kprobe应该包含了pre_handler/post_handler int3对应的中断服务处理程序中，有一段代码是要执行对应的kprobe的pre_handler； 将原addr处的一字节指令恢复，然后改成singlestep执行完下条指令； 执行完这个函数中的所有指令，执行完返回后继续执行post_handler 然后直接continue 我说的这个过程不一定精确，但是大致可以这么实现。这种方法可能效率会有点低下，kprobes.rst中也有些优化的思路，我这里没有仔细去看。
我感兴趣的就是，内核里面的kprobe和调试器中的大致跟踪tracee执行过程，很类似。
相同点：都是通过指令patch的方式
不同点：
kprobe是利用了int3指令触发trap服务程序走到了kprobe的处理逻辑去执行pre_handler/post_handler， 而调试器是tracee执行到断点时区通过trap服务程序走到了通知tracer继续控制tracee这个逻辑。 总结 # 简要总结了下kprobe这种内核探针的工作原理。</description></item></channel></rss>