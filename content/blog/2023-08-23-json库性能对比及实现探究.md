---
layout: post
title: "json库性能对比及实现探究"
description: "JSON是一种轻量级的数据交换格式，易于阅读、解析、生成，应用十分广泛。如今在微服务通信中，JSON也是一种常见的序列化手段，比如json-rpc或者gRPC json、pb互转。因为读写场景的不同，对JSON序列化、反序列化（或者解析）的关注点也不一样，一个通用的JSON库不一定能满足性能要求，可以看到有非常多的JSON第三方库频频向标准库发起挑战。本文将从JSON解析的不同场景入手，来说明这些场景下对JSON生成、解析的一些诉求，以及对性能方面的考量，进一步介绍下业界在这方面一些优秀的实践。"
date: 2023-10-09 16:01:42 +0800
tags: ["serialization","json","encoding/json","bytedance/sonic","segmentio/encoding","jsonpb","reflection","simd"]
toc: true
reward: true
---

## 本文背景

JSON是一种轻量级的数据交换格式，易于阅读、解析、生成，应用十分广泛。如今在微服务通信中，JSON也是一种常见的序列化手段，比如json-rpc或者gRPC json、pb互转。因为读写场景的不同，对JSON序列化、反序列化（或者解析）的关注点也不一样，一个通用的JSON库不一定能满足性能要求，可以看到有非常多的JSON第三方库频频向标准库发起挑战。本文将从JSON解析的不同场景入手，来说明这些场景下对JSON生成、解析的一些诉求，以及对性能方面的考量，进一步介绍下业界在这方面一些优秀的实践。



## 回顾JSON标准

rfc8259是目前JSON事实上的标准https://datatracker.ietf.org/doc/html/rfc8259，一个合法的JSON value必须是一个object、array、number、string，或者以下字面量false、true、null。该规范定义了JSON grammar来说明如何表示上述数据。

rfc8259标准明确提出，如果JSON数据不是在一个封闭系统中使用，在不同系统中进行交换时，字符集应该明确使用UTF-8编码。旧的JSON标准并没有指出这点，但是为了保证不同系统的正常交互，大多数系统使用的正是UTF-8编码。标准还指出在编码时不应该在头部添加BOM字符（Byte Order Mark，U+FEFE），一些JSON解析器为了尽可能保证互操作性可能会忽略被错误添加的BOM字符，而不是报错。

> ps：rfc8259中还提及使用Unicode字符，Unicode是一种字符编码标准，定义了字符的唯一码点，而UTF-8是Unicode的一种可变长的具体编码方案，以对ASCII进行向后兼容。

JSON 解析器（parser）将JSON文本转换为另一种表示形式，比如go结构体struct。JSON 解析器必须接受所有符合 JSON grammar的文本，可以接受非 JSON 形式或其他扩展（比如vscode .devcontainer定义中支持注释）。解析器实现可能会对其文本的长度进行限制，也可以对数据的最大嵌套深度进行限制，也可以对数值的范围、精度进行限制。

> ps："A JSON parser MAY accept non-JSON forms or extensions." 这句话的意思是，JSON解析器可以接受非JSON形式或扩展。也就是说，解析器可以容忍一些不符合严格JSON语法的文本，或者支持一些扩展的语法或功能。这给了解析器一定的灵活性，使其能够处理一些非标准的JSON文本或具有扩展功能的JSON文本。这样做是为了在实际应用中提供更大的灵活性和兼容性，以满足不同的需求和场景。

JSON生成器（generator）用于生成JSON文本，生成的文本必须严格符合JSON grammar。比如`json.Marshal(v)`将v这个数据类型序列化成JSON文本，当然还有`json.MarshalIndent(v, "", "\\t")`，会在name前面增加一些缩进，tab、空格等空白字符在标准中也是允许的。

**小结：通过rfc8259我们了解了JSON是用来做什么的，有效的JSON数据是什么样的，为了互操作性、灵活性JSON的解析器、生成器又可以怎么做。**下面我们将介绍一些应用场景，从一般到特殊，对应的也会对标准库实现提出一些挑战，然后进一步介绍一些业界的实践、优化。

## 从标准库开始

go标准库中提供了对JSON编码、解析的支持，最常用的两个函数就是`json.Marshal`、`json.Unmarshal`。标准库的设计实现，对大多数数据类型、普通的编码解析场景、易用性方面提供了很不错的支持。

在指出标准库在哪些场景下会表现欠佳之前，需要先了解下标准库在编码、解析过程中的一些实现策略、细节。

这里简单总结一下：

- 标准库json.Marshal的过程，使用了大量的反射操作，比如确定map k、v的类型信息，struct字段的类型信息，匿名嵌套及字段的可见性分析，struct jsontag规则处理，而且是通过反射递归展开json.Marshal(v)中v的类型信息，才能知道如何encode，最后才是根据v及其内部各个组成部分对应的typeEncoder来完成encode输出。encode的过程中虽然它使用了一些caching（缓存）、pooling（池化）技术，但是前面的反射开销确实是比较大的，尤其是数据类型复杂、数据量比较大的时候。

  想了解详细过程的话，可以参考这篇总结，会对这个过程中的开销有更清晰的认识：https://www.notion.so/hitzhangjie/JSON-d278399b8092470985cbc423830115fb?pvs=4

- 标准库json.Unmarshal的过程，和json.Marshal的过程相比，其中涉及到的一些要点大差不差，这里就不展开了。

see: https://sourcegraph.com/github.com/golang/go@go1.19.10/-/tree/src/encoding/json

> ps：反射的开销主要在哪里？
>
> reflection trades performance for very dynamic and flexible code by pushing work from compile time to runtime. The runtime costs include indirect calls, type inspection, value conversions, and dynamic dispatch. But used judiciously, these costs can be worth it for certain programs.

我的个人看法是，标准库作为一种支持更广泛场景下的实现，使用反射并不是一件坏事，它牺牲了一定的性能来保证了运行时的更大的灵活性，而不用像某些三方库一样去做一些极致的优化，比如bytedance/sonic只支持amd64架构。

Well，但是当我们知道自己要做什么时，还是可以去做一些更加“极致”的优化的。比如bytedance团队知道自己用的是什么类型的机器，借助SIMD等一系列优化，单是JSON相关的序列化、反序列化优化带来的收益，性能提升了、CPU开销下降了，算下来为公司节省了几十万核，这种优化就是很值得的。

下面就带着这种“优化”的思路去看看“不同场景”下该如何优化来达到期望的效果。

## 细说业界实践

先列举几个不错的第三方实现。

| project           | repository                                 | functions        |
| ----------------- | ------------------------------------------ | ---------------- |
| fastjson          | https://github.com/valyala/fastjson        | parser           |
| jsonparser        | https://github.com/buger/jsonparser        | parser           |
| jsoniter          | https://github.com/simon-engledew/jsoniter | parser           |
| simdjson          | https://github.com/simdjson/simdjson       | parser           |
| simdjson-go       | https://github.com/minio/simdjson-go       | parser           |
| rapidjson         | https://github.com/Tencent/rapidjson       | parser           |
| json-iterator/go  | https://github.com/json-iterator/go        | parser+generator |
| bytedance/sonic   | https://github.com/bytedance/sonic         | parser+generator |
| segmentio/encodin | https://github.com/segmentio/encoding      | parser+generator |
| goccy/go-json     | https://github.com/goccy/go-json           | parser+generator |

### fastjson

fastjson parse+get操作，和标准库encoding/json的unmarshal相比，效率是后者的15x，see [benchmark](https://github.com/valyala/fastjson#benchmarks)。

fastjson实现上消除了reflection，解析过程中也完全不需要schema，也不需要像其他某些三方库一样通过code generate来生成schema，它是怎么做的呢？

- 它解析JSON文本的过程，非常暴力直接，跳过空白字符直接对JSON value按照object、array、string、number、true、false、null进行解析，解析完后并不做任何“类型转换”的操作。比如object内部的字段名和值直接作为一个kvpairs数组存起来，并不关心它的类型是什么样的。
- 当真正去获取某个字段值时，调用方就知道类型是什么了，此时调用对应的GetInt或者GetString等包含类型信息的helper函数，函数内部将对应的string转换为具体的类型。

通过上面这种方法，彻底消除了反射。虽然parse+get的效率比标准库unmarshal效率高，但是还是要看具体场景，是否用起来方便，是否真的care性能。

另外，fastjson虽然提供了快速的解析操作，但是没有提供快速的编码操作，尽管它提供了value.MarshalTo方法，但是这个并不是大家日常编码时需要的将任意类型编码为JSON文本的操作。

关于快速的编码操作，fastjson建议通过[valyala/quicktemplate](https://github.com/valyala/quicktemplate)来执行快速编码。我看了下，它实际上是通过模板引擎来完成这个编码操作，但是要业务开发针对要marshal的自定义类型写好对应的go模板，有这个功夫，我还不如将自定义类型实现Marshaler接口了，干嘛非得用模板呢？同样可以避免大量反射，还不用调试go模板。

### jsonparser

只支持parser，号称效率是标准库encoding/json的10x，那么它为什么这么快呢？没fastjson快 :laughing:

- 它不依赖于encoding/json、reflection或interface{}，唯一真正的包依赖是bytes。 
- 它在字节级别上操作JSON payload，为您提供指向原始数据结构的指针：无需内存分配。
- 没有自动类型转换，默认一切都是[]byte，但它提供了值类型，您可以通过helper函数自行转换。 
- 它不解析完整的记录，只解析您指定的键。可以通过jsonpath来访问JSON中嵌套在内部的元素信息。

看下来，它的实现思路和fastjson类似。

### jsoniter

这个库的实现思路是，避免解析完整的JSON文本，而是遍历JSON数据的过程中path和指定的jsonpath匹配时才开始将当前path对应的jsonvalue进行decode。

在这个遍历过程中没有使用反射，也没有decode不必要的value，前面的几个实现方案是解析了对应的value的，尽管只是kvpairs的形式，不过开销也不大。

实际decode感兴趣的部分时还是使用了标准库encoding/json.Decoder的实现，这里面哈还是会走到反射部分。所以很难说这个库实现性能有多高。如果是数据量比较大，只解析其中部分数据时还是优势的。

```go
import (
    "encoding/json"
    "github.com/simon-engledew/jsoniter"
)

func main() {
    var found any
    matcher := jsoniter.Matcher("some", 0, "nested", "structure") // .some[0].nested.structure

    d := json.NewDecoder(os.Stdin)
    err := jsoniter.Iterate(d, func(path []json.Token) error {
        if matcher(path) {
            return d.Decode(&found)                               // decode感兴趣的部分数据
        }
        return nil
    })
}
```

### simdjson(-go)

simdjson是C++版本的实现，simdjson-go是用go重写后的版本，整体实现思路差不多，所以合在一起说了。simdjson是Daniel Lemire 和 Geoff Langdale实现的一个JSON解析库，它广泛使用了SIMD指令操作来获得高效的JSON解析操作，号称解析1GB JSON数据只需要1秒钟。

simdjson-go是使用go语言重写后的版本，它的性能大概是c++版本实现的40%~60%，是标准库性能的10x。

> ps：大名鼎鼎的[clickhouse](https://github.com/ClickHouse/ClickHouse)就使用了simdjson，可以在这里看到更多使用simdjson的知名项目https://github.com/simdjson/simdjson#real-world-usage。

那么simdjson或者simdjson-go为什么会在解析的时候有这么高的性能呢？最主要的技术已经在名字中了，SIMD？

2019年QCon大会上Daniel Lemire做了一个分享 [Parsing JSON Really Quickly: Lessons Learned](https://www.youtube.com/watch?v=wlvKAT7SZIQ)，其中提到了JSON解析过程中的主要任务，以及在实现时可能会遇到的一些挑战。作者还发表过一篇论文 [Parsing Gigabytes of JSON per Second](https://r.jordan.im/download/technology/langdale2019.pdf)，感兴趣也可以看下。

#### JSON解析中的主要任务

- 读取完整的JSON内容
- 检查是否是一个有效的JSON
- 检查Unicode编码
- 解析number
- 构建JSON DOM（document-object-model）

#### 实现JSON解析时要注意

- 考虑分支预测失效的影响，避免难以预测的分支

  分享中给出了一个示例代码，无判断分支时每次迭代3cycle，加了奇偶判断分支每次迭代增加到了15cycle，通过branchless programming消除奇偶判断分支每次迭代重新回到了4cycle。为什么会给这个例子，因为在JSON解析时需要对字符{}[]:,"等进行分类，而分类是基于相等判断的，需要在设计实现时避免分支判断。

- 使用更宽字长，避免1个字节1个字节处理，很慢

- 如果可能（硬件支持的话），使用SIMD

  目前在大多数现代主流ARM、x64处理器上都支持SIMD，最初Pentium支持SIMD是为了更好地对多媒体（声音）进行处理，现代处理器增加了位宽更大的寄存器（128-bit、256-bit、512-bit），也增加了一些高效的指令，比如一次性做32个表查询。那这个和JSON解析有什么关系呢？JSON解析中需要对字符进行分类，如分类成{}[]:,"，通过巧妙的表设计，可以一次性对很多字符进行分类，而且代码还能避免不必要的分支预测。

  老的x64（Intel、AMD）平台可以用SSE2...SSE4.2（128-bit），主流的x64（Intel、AMD）可以用AVX、AVX2（256-bit），最新的x64（Intel）可以用AVX-512（512-bit），其他平台可以自行检索下。

- 避免内存（对象）分配，这个很好理解了

  JSON解析库解析过程中解析出的object、array等最终都会转换成一个个的内存对象，如何合理地减少或者避免内存分配，就很重要，尤其是JSON数据量比较大的时候这个内存开销问题就会比较明显。对于支持GC的语言，因为GC也会导致CPU开销。

- 对解析性能做benchmark，并进行合理的优化

  比如在后续不停的优化、维护过程中，需要注意做好benchmark，一旦发现性能下降，就应该当做BUG来跟进。

#### 从0到1实现simdjson

分享最后给出了几个examples，分别介绍了SIMD技术在UTF-8编码检测、字符分类、检测转义字符（检测、移除转义字符、确定字符串范围）的应用，并不是那么好理解，建议多看几遍分享好好体会下，看了也不一定能理解那些branchless programming的写法，分享人明确指出这些玩意是经过lots of hard work总结出来的计算式。

到这里，整个JSON文档的结构就可以非常高效地解析出来，没有任何分支判断。分享最后提到了将字符串转换为number时的一点复杂性、计算你开销。`man 3 strtod`，strtod是一个库函数实现将字符串转换为浮点数double的逻辑，性能的话：吞吐量90MB/s、每个字节耗时38cycles、每个浮点数转换大约有10次分支预测失败。在simdjson作者看来这个转换太慢了，所以自己实现了相关的转换逻辑。

最后，不同平台有不同SIMD指令及对应的实现细节，simdjson会检测当前平台，并通过runtime dispatch选择使用匹配的实现代码。将上述这些思路放在一起，就是simdjson的全部核心思想了。

[simdjson](https://github.com/simdjson/simdjson)的benchmark数据显示，其性能吊打yyjson、rapidjson、json for m. c++等其他json parser。但是它只支持parser，不支持generator，后面再介绍go标准库encoding/json的平替方案 [bytedance/sonic](https://github.com/bytedance/sonic) 时我们会继续详细介绍下sonic的一些优化思路，当然SIMD相关的部分就可以省略掉了。

小结：读者可能看到这里，可能恍然大悟，也可能仍旧一头雾水，感觉让自己迷惑的东西越来越多了？那是正常的，懂的越多之后会发现自己不懂的也越来越多。作者个人水平有限，也不打算在此补充更多SIMD的内容，读者感兴趣的话可以自行去学习。

### rapidjson

rapidjson虽然号称支持parser+generator，肯定是支持parser的，但是对于generator的支持是比较有限的，它和前面的个别JSON库有点类似，就是你得parse完后再修改再将这部分解析后的DOM生成为JSON数据。和我们日常应用时理解的将自定义数据结构、类型编码为JSON数据是有差异的。鉴于此，前面表格里没有将其归为parser+generator这一类。

从simdjson的benchmark数据来看，虽然其性能吊打rapidjson，但是从实现思路上来看，rapidjson其实也是考虑使用了SIMD技术（仅限于SSE, SSE4.2？）来加速的，但是可能没有simdjson做的完善。

下面是rapidjson项目介绍中，主要提到下面几点：

- 支持SAX和DOM两种风格的API；

- 它的性能近似strlen()，也支持SSE/SSE4.2加速；
- 不依赖外部库Boost，也不依赖STL；
- 内存友好，对于大多数32/64位机器，每个JSON值占用精确的16字节（不包括字符串）。默认情况下，它使用快速内存分配器，并且解析器在解析过程中紧凑地分配内存；

- 对Unicode支持友好。它内部支持UTF-8、UTF-16、UTF-32（LE和BE）以及它们的检测、验证和转码；

对性能方面的优化细节，我们就不再想想展开看了，毕竟是被simdjson吊打了 :laughing:

### json-iterator/go

json-iterator/go是另一个标准库encoding/json的平替，默认配置下它的编码、解析性能已经是标准库的好几倍，还可以配置成高性能模式，如允许使用6digits来编码float允许损失精度。

那么这个库做了哪些优化呢？作为一个标准库的平替，应该可以多给些关注，看下它的benchmark数据：

| ns/op           | allocation bytes | allocation times |              |
| --------------- | ---------------- | ---------------- | ------------ |
| std decode      | 35510 ns/op      | 1960 B/op        | 99 allocs/op |
| easyjson decode | 8499 ns/op       | 160 B/op         | 4 allocs/op  |
| jsoniter decode | 5623 ns/op       | 160 B/op         | 3 allocs/op  |
| std encode      | 2213 ns/op       | 712 B/op         | 5 allocs/op  |
| easyjson encode | 883 ns/op        | 576 B/op         | 3 allocs/op  |
| jsoniter encode | 837 ns/op        | 384 B/op         | 4 allocs/op  |

那它是如何做的呢？看代码的结构的话，别说，bytedance/sonic跟这个还有点像……ok，回来继续看。

see：https://sourcegraph.com/github.com/json-iterator/go@71ac16282d122fdd1e3a6d3e7f79b79b4cc3b50e/-/blob/config.go?L296:26&popover=pinned

see：https://sourcegraph.com/github.com/json-iterator/go@71ac16282d122fdd1e3a6d3e7f79b79b4cc3b50e/-/blob/reflect.go?L87:23-87:31

看了下，和segmentio/encoding/json有点类似，尽可能消除反射逻辑、缓存技术、池化技术之类的优化。但是这里的测试应该没有那么充分，它的性能应该和segmentio/encoding/json差不多了太多。

### bytedance/sonic

该JSON库支持parser+generator，可以作为go标准库encoding/json的平替，API方面和标准库提供的接口一样也比较友好，对JSON标准的支持层面也OK。支持和标准库对齐的默认配置、快速模式等，联想下标准库中的各种操作的开销，这里的快速模式就是通过调整一些开关选项来关闭对应的操作，更加追求极致的性能。

好，除了上面这些，对于sonic而言，它在改进性能方面，有哪些比较亮眼的地方呢？下面介绍下。

> bytedance/sonic: A blazingly fast JSON serializing & deserializing library, accelerated by JIT (just-in-time compiling) and SIMD (single-instruction-multiple-data).

sonic的性能优化，主要源自JIT即时编译和SIMD，SIMD在JSON解析中的作用，我们前面介绍simdjson的时候重点介绍过了，这里就不再详细展开了，我们来看下为什么会用到JIT及时编译。

大家前面看这些parser的时候，不知道有没有注意到，这些parser在使用的时候，基本都是通过指定JSONPATH或者至少是这样的层级结构`twitter["xxx"]["yyy"][0]`这种方式去访问嵌套在内部的元素数据的，而不是将JSON数据解析成一个完整的`go struct`然后再通过字段去访问的。对于前者往往是在helper函数（函数签名中指定了类型）去读取时才会解析或者强制类型转换（simdjson好像也是这样，see: class document, class tape_ref）。

标准库encoding/json会将JSON unmarshal成一个有名类型，这种我们称为有schema的，因为name对应着struct中的jsontag，名字匹配上了就知道了value应该解析成字段对应的类型，而不用去根据value的内容再去额外推断类型长啥样。unmarshal也可以解析成一个any，这个时候就称为无schema的，需要根据JSON中name对应的value去推断类型，一般都会搞成一个map来存name，value的pairs。

相比较而言，unmarshal时有schema的会比无schema的性能要好些，因为反射方面的任务少了。marshal时其实也类似，因为我们前面介绍parser时介绍的主要是解析相关的，为了不让内容出现太大跳跃，我们还是以解析逻辑的介绍为主。

> 在讨论这个问题之前，先需要明确一点，因为前面的库包括simdjson的给用户提供的API的差异，它们不用反序列化成一个完整的类型，就不需要有没有schema这类问题，所以就可以省掉很多这种类型推断的过程，性能自然高。但是sonic是要成为一个标准库的平替，它就得考虑schema这些的优化问题。

那，sonic如何解决这个问题的呢？AST……啊这，我们还是系统性的了解下sonic都做了哪些优化比较好。

### segmentio/encoding

内部实现使用了一些unsafe操作（无类型代码、指针运算等等）来尽可能避免使用反射，使用反射通常是序列化过程中CPU占用高、内存开销大的重要原因。这个包致力于实现零不必要的动态内存分配，并且热点代码路径中尽量避免使用反射包。

以json.Marshal(v)为例，来看下做了哪些优化？

- 构建encoderCache sync.Map的时候，它没有使用reflect.Type来作为key，而是使用了typeid(reflect.Type)，实际上是一个实现类型的地址，通过这种方式减轻了后续查询type对应的encoder的开销；
- map的keys是否排序，提供了一个选项进行控制，而不是像标准库那样有各种各样的排序，排序前获得map中的所有key、value还是通过反射的，所以这个排序的前置准备以及排序本身都有开销的，不能忽略；
- struct的fields排序也省略掉了，这个把struct及其内部嵌套struct通过反射获取index信息进行排序的逻辑也比较啰嗦，这里也省掉了；
- html转义也是可以控制的，这个和标准库一样都有选项进行控制，算不上什么明显优化；

所以，segmentio/encoding/json库是通过消除一些不必要的反射以及其他一些优化技术来改善了解析、编码的性能，可以作为标准库的平替。实测其性能是优于标准库的，但是没有sonic好。



### goccy/go-json





## 参考文献

1. JSON, https://en.wikipedia.org/wiki/JSON
2. Introducing JSON, https://www.json.org/json-en.html
3. The JavaScript Object Notation Data Interchange Format, https://datatracker.ietf.org/doc/html/rfc8259
4. Parsing Gigabytes of JSON per Second, https://r.jordan.im/download/technology/langdale2019.pdf
5. Parsing JSON Really Quickly: Lessons Learned, https://www.youtube.com/watch?v=wlvKAT7SZIQ
6. rapidjson, https://github.com/Tencent/rapidjson
7. rapidjson features, https://github.com/Tencent/rapidjson/blob/master/doc/features.md
8. 



## 附录

> Q：请列举JSON序列化、反序列化过程中的挑战点，并针对这些挑战点介绍下第三方库是如何进行优化的？

当涉及到JSON序列化和反序列化时，以下是一些常见的挑战点，以及第三方库是如何进行优化的：

1. 性能优化：JSON序列化和反序列化可能涉及大量的数据操作和字符串处理，对于大型数据集或频繁的操作，性能可能成为挑战。第三方库通常通过以下方式进行性能优化：
   - 采用更高效的算法和数据结构：一些库使用更快速的解析算法和数据结构，如基于流的解析器、缓冲区技术等，以提高处理速度。
   - 批量处理：一些库支持批量处理，将多个对象一次性序列化或反序列化，减少了数据操作和字符串处理的次数，提高了效率。
   - 延迟加载：一些库支持延迟加载，只在需要时才进行序列化或反序列化，避免了不必要的操作和资源消耗。
2. 复杂数据结构处理：处理嵌套、多层次或循环引用的复杂数据结构可能变得复杂。第三方库通过以下方式进行优化：
   - 自定义序列化和反序列化规则：一些库允许开发者定义自定义的序列化和反序列化规则，以处理复杂数据结构的特殊情况。
   - 引用跟踪和处理：一些库提供引用跟踪和处理机制，以处理循环引用和避免无限循环的问题。
3. 类型转换和映射：JSON和编程语言中的数据类型可能不完全匹配，需要进行类型转换和映射。第三方库通过以下方式进行优化：
   - 自动类型转换：一些库支持自动类型转换，根据JSON中的表示自动将其转换为适当的编程语言类型。
   - 自定义类型映射：一些库允许开发者定义自定义的类型映射规则，以处理特定类型的转换和映射需求。
4. 错误处理和容错性：处理JSON序列化和反序列化过程中的错误和异常情况是重要的。第三方库通过以下方式进行优化：
   - 错误处理机制：一些库提供丰富的错误处理机制，如异常抛出、错误码返回等，以便开发者能够更好地处理错误情况。
   - 容错性：一些库具有容错性，能够处理部分无效或不完整的JSON数据，以尽可能地恢复有效数据。

总的来说，第三方库通过采用更高效的算法和数据结构、提供自定义规则和映射、处理复杂数据结构、优化性能和错误处理等方式，来解决JSON序列化和反序列化过程中的挑战。这些优化措施使得第三方库能够更好地满足不同需求和场景下的JSON处理需求。



> Q: 那，接下来需要看下下面几个问题：什么是SIMD呢？SIMD最初是用来解决什么问题的呢？SIMD在JSON编码、解析中可以用来做什么呢？simdjson正确使用SIMD还需要注意些什么呢？

**什么是SIMD**

SIMD(Single Instruction Multiple Data) 是一种并行计算技术,可以同时对多个数据执行相同的操作。使用 SIMD 的主要目的是为了提升计算性能。

**SIMD适用场景**

适合使用 SIMD 的情况包括:

- 需要对大批量数据执行相同的数学运算或逻辑运算,如向量、矩阵运算、图像处理等。
- 需要对多媒体数据如音频、视频等进行处理,如编码、解码、滤波、变换等。
- 在数据库、科学计算、金融分析等需要处理大量数值计算的场景。
- 游戏开发中的物理模拟、人工智能等也可以使用 SIMD。

使用 SIMD 的好处有:

- 提高计算并行度,单次指令处理更多数据。
- 减少指令数,降低指令调度开销。
- 更高效利用处理器内部执行单元。
- 数据级并行,更易映射到多核架构。

一些常见使用 SIMD 的例子:

- 图像处理:模糊、锐化、色彩空间转换等算法可以用SIMD加速。
- 信号处理:FFT、FIR/IIR 滤波等用SIMD实现。
- 科学计算:向量矩阵运算都可以用 SIMD 优化。
- 数据压缩/解压:如音频视频编解码中的 SIMD 优化。
- 数据库操作:聚集函数、关系运算可用 SIMD 实现。
- 机器学习:神经网络中矩阵乘法、激活函数计算等使用 SIMD。

总之,SIMD 非常适合数据并行的场景,使用它可以显著提升计算性能。编译器和开发者都可以通过自动向量化和手动优化,利用 SIMD 使程序运行更快。

**SIMD用于JSON**



