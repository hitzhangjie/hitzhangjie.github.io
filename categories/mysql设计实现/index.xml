<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MySQL设计实现 on</title><link>/categories/mysql%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0/</link><description>Recent content in MySQL设计实现 on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 27 Apr 2021 12:34:00 +0800</lastBuildDate><atom:link href="/categories/mysql%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>09普通索引和唯一索引：如何选择</title><link>/blog/09%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9/</link><pubDate>Tue, 27 Apr 2021 12:34:00 +0800</pubDate><guid>/blog/09%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9/</guid><description>对比两种类型的索引 # 普通索引，允许多条记录中组成索引的字段值出现重复的情况； 唯一索引，不允许…… 两种类型索引实现 # 肯定都是一样的啊
两种类型索引效率 # 我们以表user为例：
create table `user` ( id int auto_increment, id_card varchar(64), name varchar(32), primary key(id), [uique|index] (id_card) -- 创建索引：唯一索引或者普通索引 ) 其中id_card可能是唯一索引，也可能是普通索引。
查询效率 # 以这条查询语句为例：select name from user where id_card=?
普通索引的查询
顺着B+树根据id_card查询，查询到第一条记录之后，回表查询对应的name，加入结果集。继续遍历向右的指针对应的记录，直到找到第一条id_card不匹配的记录为止。因为id_card肯定是不重复的，所以这里向右的匹配开销顶多也就是多比较一次。
当然如果匹配到的这条记录如果是page的最后一条记录的话，那么可能向右的查找需要加载另一个page，这是最坏的情况了。
实际情况是B+树种一个节点可以存储非常多的key和指针，真的出现匹配记录出现在最后一个的情况非常少。
唯一索引的查询
查找过程也是顺着B+树根据id_card查询，然后再回表。区别是它找到第一个匹配的节点之后就停止向右的查找了，因为它知道是唯一索引，不可能有重复的记录存在。
性能对比
看上去唯一索引查询性能会高一点，但是前面也分析了id_card本身具备唯一性，普通查询中这种继续向右查找的操作对性能影响开销并不大，微乎其微。所以对于这两种索引，建议使用普通索引来代替唯一索引。
更新效率 # 更新语句以这个为例：update user set name=&amp;quot;xxxx&amp;quot; where id_card=?
change buffer
在mysql执行数据更新时，会先写redo log，然后收到ok后准备更新数据。这个要更新的行对应的页数据如果在内存中，则直接更新内内存中的相应字段就可以了。</description></item><item><title>08事务隔离：事务到底是隔离的还是不隔离的</title><link>/blog/08%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/</link><pubDate>Sun, 25 Apr 2021 22:30:00 +0800</pubDate><guid>/blog/08%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%BA%8B%E5%8A%A1%E5%88%B0%E5%BA%95%E6%98%AF%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%98%E6%98%AF%E4%B8%8D%E9%9A%94%E7%A6%BB%E7%9A%84/</guid><description>启动事务 # 启动事务的方式有哪些：
autocommit=1，每条语句是一个独立的事务，比如select、update、delete； 通过begin/start transaction来启动一个事务，但是该语句并不是事务的起点，起点是在后面的第一条sql语句执行的时候； start transaction with consistent snapshot，立即启动一个新的事务，和begin/start transaction不同，该语句是一个事务的起点； 视图的概念 # 在mysql里，视图，有两种意思：
一个是“view”，它是一个用查询语句定义的虚拟表，如执行create view select * from table，该语句执行的时候执行查询语句获得结果并创建视图，可以在视图上执行查询操作，查询语法与在表上的查询方式类似；
另一个是InnoDB在实现MVCC时用到的“一致性读视图”，即consistent read view，用于支持RC（read commited，读提交）和RR（repeatable read，可重复读）隔离级别的实现；
它没有物理结构，作用是事务执行期间用来定义“当前事务能看到什么数据”。
“快照”在MVCC里是怎么工作的 # 在可重复读隔离级别下，事务在启动的时候就“创建了个快照”，这个快照是基于整库的。
但是这里的创建快照，并不是复制一份完整的数据作为只读，肯定不能这样实现，想想一下一个数据库如果数量很大，复制的存储开销也太大了。
mysql MVCC里实现的这个快照非常聪明：
InnoDB里每个事务都有一个唯一的事务ID，叫transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。
每行数据也是有多个版本的，这里的版本就用transaction id来表示。哪个数据版本更加新一点旧一点，还是根据生成该版本时的顺序来决定的，每行数据的transaction id则用来维护一个一致性读视图；
当对某行数据进行更新操作时，会申请一个新的事务id，并插入新行数据，并更新字段trx_id为事务id，此时，插入了新的数据并不会删除旧的，旧的还是保存着的。但是新版的行数据有办法能找到旧版本的数据；
注意新生成一个版本数据时，也会插入一行undo log，一个事务可以借助其事务id，从当前数据版本开始读，然后结合每行数据的trx_id和undo log，来读取到当前事务可见的数据版本，来实现一致性读视图，也就实现了可重复读；
就是当前事务id可能是100，现在对应行的数据当前版本是102，100这个事务就顺着数据行的当前版本开始找，直到发现一个版本&amp;lt;=100时才行，也就保证了一致性读，这里就是根据数据行102版本的undo log找到前一条数据行，重复这个过程，直到发现一个版本&amp;lt;=100。
通过这种方式，实现了秒级快照的能力！
当前读（current read） # 如果事务中涉及到一些更新类的操作的话，这里的更新是在数据“最新版本”上进行的更新，也就是说在“当前读”的版本上进行更新。后续的读，看上去读取到的就是最新值。
这可能会让我们觉得，与我们之前MVCC里面一致性读时说的一些有矛盾。其实没有矛盾的，只是更新操作的时候是在当前读的最新数据上进行更新。而后续读取的时候依然是按照MVCC里一致性读的方式来的。
如果更新时不是按照当前读来更新，那么就会造成以前已经提交的事务更新操作丢失了。
有几种办法可以实现当前读：
更新操作肯定是当前读了； select + lock in share mod，也是当前读； select + for update，也是当前读；</description></item><item><title>07行锁功过：怎么减少行锁对性能的影响</title><link>/blog/07%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/</link><pubDate>Sun, 25 Apr 2021 16:00:00 +0800</pubDate><guid>/blog/07%E8%A1%8C%E9%94%81%E5%8A%9F%E8%BF%87%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/</guid><description>行锁 # 行锁，顾名思义就是对表的行进行加锁，是存储引擎层来设计实现的：
MyISAM没有行锁，对表执行更新时只能加表锁，并发度就比较低 InnoDB支持行锁，并发度就比MyISAM高，所以一般用InnoDB代替MyISAM ps: 减少行锁的冲突，有助于进一步提高并发处理能力。
两阶段封锁协议 # 在一个事务中：
加锁，是在需要的时候按需加锁； 释放锁，是在事务提交的时候释放锁； 知道这个后，我们可以在编码时进行一点优化。
如果事务涉及到锁定多个行的情况，尽量将可能导致锁冲突的行操作往后放，这样减少了锁持有时间，从而降低锁冲突。
举个例子，现在有个顾客A从电影院B买电影票，需要执行：
1：扣账户A余额的操作；
2：需要给电影院B增加余额的操作；
3：记录一条交易日志；
那这几个操作该如何排序呢？因为会有很多人买电影票，所以操作2的冲突概率是比较大的，所以将2排在最后，而操作3是在额外的表中追加记录，基本不存在行冲突，所以不如放在最前面，A可能除了买电影票还可能买其他，冲突概率次之。
所以排序为3、1、2比较合理。
死锁和死锁检测 # 调整上面的操作顺序，只能尽量减少锁冲突，提高并发度，但是不能完全保证避免死锁。
死锁原因 # 造成死锁的原因，就是多个事务中加锁顺序不一致，造成了循环依赖：比如事务t1已经持有了锁a，现在申请锁b，但是锁b呢已经被事务t2持有，事务t2还在申请锁a，但是a已经被t持有。这样事务t1、t2相互等待对方，都拿不到锁，就造成了死锁。
死锁检测 # 解决死锁问题，有这么几种方法，一种是死锁避免，一种是死锁检测。
死锁避免，可以让获取锁的操作有一个最大超时时间，超过这个时间就返回获取锁失败，让事务退出，事务退出的时候释放掉已经持有的锁，这样就避免了死锁。
mysql中可以通过设置变量innodb_lock_wait_timeout的值来设定这个超时时间，默认值是50s，这个时间还是很长的，一旦真的发生了死锁，对业务不可用时间也比较长，50s啊！
如果把这个变量设为1s呢，也不行，可能会有很多的锁获取失败的情况，但是可能是正常获取锁操作，非死锁，会造成很多误伤，也不好！
死锁检测，通过死锁检测算法来检测是否会出现死锁操作，比如获取一个锁之前，先检查这个锁被哪个线程持有，没有也就正常拿到锁了，如果被线程t2持有，继续检查这个线程t2有没有要申请的锁被当前线程持有，如果有，那么当前线程发起的加锁请求将会导致一个循环依赖，会发生死锁。
这个时候，可以直接让当前事务失败，释放锁，或者干掉另一个事务t2让它释放锁，也就避免了死锁。
死锁检测默认是开启的，innodb_deadlock_detect，通过这个变量来设置。
相关开销 # 死锁避免虽然效果不怎么令人满意，一般还是会开启死锁检测的，但是死锁检测的过程前面也简单描述了，实际上这个死锁检测的过程会更复杂，假如有1000个线程，当前线程t1可能希望获得线程t2上的锁，t2可能希望获得t3上的锁，&amp;hellip;.，t1000可能希望获得当前线程t1的锁……就是要分析做很多分析才能判断出会不会导致死锁。
有的时候线程数多了之后，死锁检测开销也会比较高，表现就是CPU占用率很高，比如100%，但是每秒并没有执行几个事务。
热点记录 # 对于某些热点记录，更新频繁的记录，这样的锁冲突的情况会比较多，而且线程数也比较多的情况下，问题更明显，CPU占用很高，但是执行不了几个事务，尽管没有真的发生死锁。
对于热点记录如何解决呢？
方法一：将对一条记录的操作拆分成对多个记录，每次更新时随机选一条，降低锁冲突的概率，比如改为随机更新10条记录中的一条，冲突概率就下降为原来的1/10； 方法二：改成用写增量流水日志的方式，定期地取合并日志中的操作更新到原来的那一条记录； 这里的思想，很分布式缓存热key的处理方式也是类似的，要么就是通过写多个key来解决，要么就是记录流水异步更新来解决。</description></item><item><title>06全局锁和表锁：给表加个字段怎么这么难</title><link>/blog/06%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE/</link><pubDate>Fri, 23 Apr 2021 19:00:00 +0800</pubDate><guid>/blog/06%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%E7%BB%99%E8%A1%A8%E5%8A%A0%E4%B8%AA%E5%AD%97%E6%AE%B5%E6%80%8E%E4%B9%88%E8%BF%99%E4%B9%88%E9%9A%BE/</guid><description>根据加锁的范围，mysql中的锁可以分为：全局锁、表锁、行锁 3类。
全局锁 # 全局锁，是对整个数据库实例进行加锁，如通过命令Flush tables with read lock (FTWRL)加全局读锁，锁定后，数据更新（增删改）、数据定义（建表、修改表等）都会被阻塞。
其作用，主要是做全库逻辑备份，也就是把全表select出来存成文本。
加全局读锁之后，再开始备份，但是有风险：
如果是对主库备份，开了全局读锁之后，库不能写入，意味着业务基本不可用； 如果是对从库备份，开了全局读锁之后，从库新同步过来的binlog假如有表结构修改的操作，会导致因为拿不到MDL（metadata lock）而阻塞，无法修改表结构这一个阻塞还好，更严重的是会导致后续所有的拿MDL读锁的操作失败，包括正常的更新数据。因此这种方法容易造成主从同步延迟； **备份数据，为什么要加锁，能不能不加锁？**不能！数据一致性，这个很好理解，不解释！
有没有不加全局锁的方法，有，但是要看引擎是否支持事务：
MyISAM引擎，不支持事务，只能用加全局读锁的方式锁定之后再开始备份
InnoDB引擎，支持事务，主库备份的时候通过&amp;ndash;single-transaction，开启独立的事务进行备份：
因为备份时候设定的事务隔离级别是RR（可重复读），一致性问题不用担心了； 备份过程中也会拿MDL lock读锁，如果备份过程中有表结构更新操作，也可能会因为拿不到MDL写锁而阻塞，也会阻塞后续的所有数据更新动作； 针对上面问题，AliSQL提了个PR已经合入MariaDB，即尝试修改表的时候加个超时时间，如果过了超时时间还没有拿到MDL锁，则失败，等后续重试，这样至少不会阻塞正常的数据更新操作； 这里涉及了表锁中的一种：MDL锁
加全局读锁，可以将数据库设置为只读，还有一种办法，设置全局变量set global readonly=true，但是这种方式，风险更高：
通常这个属性还用来区分一个数据库是主库还是从从库，如果贸然修改这个变量，可能会造成一些其他应用的误判； 假如客户端申请了加全局锁且成功之后，如果客户端崩溃了，这个全局锁还是可以自动被释放掉的，库还是可以写入的。但是，如果客户端通过全局变量将库设置为了只读，那么客户端崩溃后，库也是只读的，不可写入的； 表锁 # 加表锁，主要有两种形式，lock tables&amp;hellip; 和 MDL lock。
lock tables &amp;hellip; with read/write # 这种加锁方式，对应的解锁方式是 unlock tables
这种加锁方式，对其他线程能否读写、当前线程能否读写都做了明确的限制。假定当前线程p读表t1加读锁、对表t2加写锁，那么：
其他线程q是不能对t1执行写操作的，对t2也不能执行读操作，这个好理解； 当前线程p也是不能对t1执行写操作的，也不能对t2执行读操作； 可以理解成没有考虑锁的重入、读写排他性；</description></item><item><title>05索引原理：深入浅出索引（下）</title><link>/blog/05%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95%E4%B8%8B/</link><pubDate>Sat, 17 Apr 2021 17:00:00 +0800</pubDate><guid>/blog/05%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95%E4%B8%8B/</guid><description>前面介绍了常见的查询类型，支持查询的常用数据结构和算法，然后介绍了InnoDB执行引擎B+树的优势，主要是和机械硬盘的特性结合起来实现高性能的读写，也描述了B+树层高与可存储的数据量的计算方式，等等吧。
这里重点介绍下索引的设计、使用方面的一些知识点，下面讨论的都是InnoDB B+树。
user(id,name,age), pk(id),index(name)
主键索引
b+树中索引节点存储的是关键字以及指针，指向指向索引节点或者数据节点的page。索引中的关键字，可以是一个列字段，也可以是多个列字段，如果是多个列字段的话，它们出现的顺序就是定义索引时写的顺序。
innodb主键索引是聚集索引，叶子节点中的数据直接包含了记录行的数据。
普通索引
其他普通索引，比如id为主键，在name上创建索引，这种索引的叶子节点中记录的主键的id，如果要通过name去查age之类的其他字段信息，要先通过这里的name索引查到id，然后回表，也就是通过主键索引查找到对应的记录后（实际上是主键索引查找到指针对应的page，然后遍历page里面的各个记录找到的），再去拿到age等其他信息。
所以普通索引这样查起来会多一次回表的操作。
覆盖索引
假如说现在我想通过name，直接查找到对应的age行不行啊，不想先查到id，再回表查。不回表的话，就可以考虑索引覆盖，比如创建个索引index(name,age)，这样就会先通过这个组合索引中的name找到对应的叶子节点，叶子节点中也包含了age这个关键字，就可以直接返回age。
当然name有可能重复，所以可能查询结果不是单条记录，那还得沿着这个第一次找到的节点，向右遍历叶子节点列表，知道name不满足为止。
由于索引中包含了age，也就不需要回表了。
最左前缀原则
创建组合（联合）索引之后，查询的时候一定要注意，要用最左匹配原则才能应用索引，否则用不上索引，为什么呢？比如我们定义索引的时候是index(name,age)，那么你查询到时候where age=xxx and name=yyy，这种就没有优先用name，查询比较的时候就用不上索引，但是where name=yyy and age=xxx就能用上索引。
为什么会这样呢？b+数索引节点中关键字，是按照定义索引时字段的顺序设置的，比较的时候也是按照这个顺序来比较。
如何安排组合索引内的字段顺序？将更容易用到的放前面，这样可以提高复用的程度。 由于建立了索引index(a,b)，最左前缀可以在a上引用索引，也就不需要再单独为a建立索引index(a)了。 另外就是要考虑空间原则，是不是一定要(name，age)建联合索引，那就得考虑用的频率了，如果这种查询场景不多，查询效率要求也不高，那么确实不适合建立联合索引，浪费存储空间啊。但是也不能全表扫描那么慢吧，这个时候为name建个索引，回表查age，还是可以接受的。 索引下推
对于组合索引index(a,b,c)，我们建议使用最左前缀匹配的方式来应用索引，那么如果查询的时候第一列是匹配的，第二列不配的，这种情况下会怎么处理呢？
5.6以前的话，会直接根据匹配到的a回表，查出记录后再对比b是否匹配，不匹配再过滤掉，很明显这种效率是比较低的。
5.6以后的话，引入了索引下推，什么意思呢，就是在组合索引上遍历的时候就直接比较其他几个索引列字段是否匹配，不匹配直接过滤掉，也不用回表了，减少了回表次数，效率自然也就高了。
范围查询：说下范围查询大致是怎么工作的？
最后说下范围查询，比如 user表上的索引有pk(id), age(name)，现在查询select name,age from user where age&amp;gt;25 and age&amp;lt;30 ，这个时候会现在index(age)这个索引上找到age&amp;gt;25的一个叶子节点，然后从这个节点开始，沿着叶子节点链表，直接向右遍历，因为都是按照age有序的嘛，每遍历一个叶子节点，回表查询name假如结果集，直到发现age&amp;lt;30不成立结束。
g</description></item><item><title>04索引原理：深入浅出索引（上）</title><link>/blog/04%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95%E4%B8%8A/</link><pubDate>Sat, 17 Apr 2021 16:00:00 +0800</pubDate><guid>/blog/04%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%B4%A2%E5%BC%95%E4%B8%8A/</guid><description>索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。直接在几百页的书找一个关键词可能要找很久，但是通过附录中先通过首字母找到对应的关键词，再通过关键词对应页码找到书中对应页、对应内容，就会比较快。查词典、查电话本，都是类似的思想。
查询类型：
等值查询； 区间查询； 几种索引模型：
hash
这种只适合等值查询，接近O(1)的时间复杂度，不适合范围查询（比如key介于[k1,k2]之间的就得全量扫描了）。这类存储，主要有memcached及其他一些nosql存储；
有序数组
在等值查询、范围查询场景中，性能都比较好，基本可以在O(log(n))时间复杂度内搞定。当然数据有序、没有重复等情况下，平均复杂度要坏一点。但是考虑到插入的场景，插入点位置及以后的数据要移动的，代价比较高。
有序数组，只适用于静态存储引擎，不怎么变化的那种存储。
搜索树
根据对树中索引节点key的数量以及对树的高度的不同，可以分为好几种，比较常见的有二叉搜索树、二叉平衡树、红黑树，这些都是二叉的，时间复杂度基本都是O(log(n))，但是考虑到平均时间复杂度二叉平衡树是最好的，但是它的插入操作涉及到大量的树调整步骤，开销较大，普通二叉搜索树的话又有可能退化为一个单支的链表，查询性能退化为O(n)。红黑树是一个比较好的选择，使用也比较广，它通过施加一些约束限制了左、右子树高度不会成为另一个的两倍，这比二叉平衡树左右子树高度差最多为1可松多了，减轻了树调整的开销。红黑树是用的比较多的。
树的搜索效率取决于树的高度，如果树高度很高，那么查询效率自然就会比较低。考虑到机械硬盘随机访问慢的特性，每个索引节点都要取机械硬盘里面去加载的，这个很慢的。数据库设计出来是要存储大量数据的，索引关键字区分度再高，二叉树出度太低，树还是会很高的，这对存储大量数据（几千万上亿）的数据库系统来说，二叉树有点吃不消，所以B树、B+树出现了。
试想下，二叉树的情况下，节点多了之后，树高度会很高的，每个索引节点都可能存储在机械硬盘上离散的位置，读取每个索引节点会很耗时的。
B树是m叉树，但是B树中的非叶子节点既可能是索引节点，也可能是数据节点，数据节点之间没有形成一个有序链表，没有充分考虑到机械硬盘顺序读取效率高的特点，B+树考虑到了，所有非叶子节点都是索引节点，所有，叶子节点均为数据节点，且构成了一个有序的链表，正好能解决机械硬盘随机访问慢的问题，也能利用硬盘顺序读取快的优势。
m叉树中的这个m应该多大呢？这个取决于数据块的大小，以InnoDB的一个整数字段索引为例，这个N差不多是1200。树高4层的时候，就差不多可以存储17亿条数据了。
ps：类似的怎么计算呢？
学习下怎么计算的：https://www.programmersought.com/article/65874297377/
现代机械硬盘最大不知道多少，innodb里面定义的是索引中指针大小是6个字节，意味着2^(6*8)/2^40=256TB，这里的指针大小表示的是数据在表空间中的偏移量，可以理解成可以寻址256TB的硬盘空间？
innodb默认的pagesize是16KB，ok！
先算一个索引可以存多少指针：假定我们用bigint作为主键，8个字节，指针6字节，那一个索引节点可以存储16KB/14B=1170个关键字和指针。 再算一个叶子节点可以存多少记录：聚集索引里面叶子节点中，索引关键字是数据记录的一部分，至少大于8个字节了，我们就假定一行记录大约么为1KB吧。那么一个叶子节点可以存储记录数量为16KB/1KB=16。 现在我们笼统算法下：
假如树最大高度为2，那么就是根节点指针数量*每个叶子节点记录数量，可以存储1170x16=18720
假如数最大高度为3，那么就是1170^2*16=21902400=2190w
假如数最大高度为4，那么就是1170^3*16=25625808000=256亿
实际情况一般3层就可以满足绝大部分场景使用了，数据量大的话，也极少有业务会超过4层。所以存储很多的数据，查询效率也基本上是有保证的。
每次查询索引节点，都代表了一次磁盘IO，所以通过主键索引查询，会比借助其他辅助索引查询、再回表的方式要快一些。
指针6个字节，意味着可以寻址的索引地址空间很大的，256TB，哪有这么大的内存，这个东西也可以理解成磁盘上索引的偏移量，256TB的硬盘。
但是看得出来，索引可能也很大，不一定能完全在硬盘中存储的下，也是按需加载的！
ps：为什么要用自增id作为主键，有什么好处，主要是为了避免插入时页分裂，b+树调整导致的效率低下：https://zhuanlan.zhihu.com/p/71022670
ps：innodb b+tree索引结构：https://www.programmersought.com/article/1411118316/</description></item><item><title>03事务隔离：为什么你改了我还看不见</title><link>/blog/03%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/</link><pubDate>Sat, 17 Apr 2021 15:00:00 +0800</pubDate><guid>/blog/03%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E6%94%B9%E4%BA%86%E6%88%91%E8%BF%98%E7%9C%8B%E4%B8%8D%E8%A7%81/</guid><description>数据库事务（刚性事务）
事务特性：ACID
Atomic Consistency Isolation Durability 事务隔离性及事务隔离级别：
read uncommited read committed repeatable read serializable 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。
在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。 在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。 这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念； 而“串行化”隔离级别下直接用加锁（读写冲突、写写冲突）的方式来避免并行访问。 事务隔离性级别通过变量 transaction_isolation 来设置。
MVCC：
在执行更新操作的时候，也会对应的插入一行“回滚记录”，用于MVCC（多版本并发控制）中构建不同时间启动的事务的视图，这主要是为了维持一个可重复读的视图。
比如现在执行操作假如现在c=1，执行update c=2, c=3, c=4的操作，那么对应的就会插入四行回滚记录，将2回滚为1，将3回滚为2，将4回滚为3。
三个更新操作的时刻t1、t2、t3，相当于确立了三个边界，边界时间点前后可能会有不同的事务启动、结束。那些依然还未结束的事务，通过他们的启动时间与回滚记录插入时的时间做对比，就可以找到应该依次执行哪些回滚记录来恢复到启动事务时的数据库状态，通过这种方式来重建一个可重复读的视图。
这就是MVCC的要义。
这些回滚日志，多了之后也会占用存储空间，浪费资源，需要清除？但是假如有个事务是时刻t启动的，那么时刻t之后的回滚日志都是要保留的，如果这个事务执行时间很长，就会导致回滚日志积累很多，浪费资源。因此不建议使用长事务。
查找执行时间超过60s的长事务：
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&amp;gt;60 了解下这个事务统计表几个比较有用的字段：
mysql&amp;gt; desc innodb_trx; +----------------------------+-----------------+------+-----+---------+-------+ | Field | Type | Null | | +----------------------------+-----------------+------+-----+---------+-------+ | trx_id | bigint unsigned | NO | 事务id | | trx_state | varchar(13) | NO | 事务状态，如RUNNING | | trx_started | datetime | NO | 事务启动时间 | .</description></item><item><title>02一条SQL更新语句是如何执行的</title><link>/blog/02%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/</link><pubDate>Sat, 17 Apr 2021 13:44:00 +0800</pubDate><guid>/blog/02%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E4%B8%80%E6%9D%A1sql%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/</guid><description>这里涉及的日志类型：
执行引擎层：innodb redolog
mysql服务层：mysql binlog
买东西赊账为例，老板通常有个账本，上面记录了所有人总的赊账情况，但是忙的时候是来不及查、计算的，可能就会在一个粉板上记录当前次的赊账情况，等打烊之后再去算，算完更新到账本上。
mysql的设计者，采用了类似老板记账的方式，来提高更新效率。
WAL：write ahead log，关键点就是：
先写日志，再写磁盘 也就是先写粉板，不忙的时候再写账本； WAL log写操作基本只是追加，磁盘顺序写，效率高；写记录到磁盘还要考虑B+树特性、磁盘特性，要找到在哪里插入，涉及到多次随机读，效率是比较差的。
所以说先写WAL这里是提高更新效率是没有问题的，当然了，也提供了崩溃后恢复的一种保证。
具体说，就是innodb引擎会：
先把记录写到redo log里面； 再更新内存，（这个时候就算更新完成了）； 然后比较空闲的时候再写回磁盘。 但是如果粉板写满了怎么办呢？老板只能停下手中的活，先把粉板上的赊账记录算完腾到账本上，然后擦掉粉板腾出新的空间，然后再继续赊账。
类似地，innodb的redo log也是固定大小的（和粉板类似），从头到尾写满了，就得再从头写。redolog维护了两个指针：
write pos，写最新赊账记录的位置，++，到头后再开始； checkpoint，表示已经将对应操作同步到磁盘数据文件的位置，相当于腾空的粉板位置，可以继续记录赊账位置。 当writepos追上checkpoint的时候，表示写满了，这时候mysql就得和老板一样停下来算账，不能接受新的更新请求，这样把checkpoint推进以下之后，再继续接受更新请求。
这样即使数据库运行期间崩溃了，但是有了这个redolog，就可以将之前的操作全部恢复，不会丢失，这个能力称之为crash-safe。
这里的write pos、checkpoint的作用，是为了提高更新效率，延时写入磁盘用的。
一个更新操作是如何执行的？
当执行一个更新操作时，执行器找到记录对应的行，请求执行引擎返回行数据，如果行数据在内存中，执行引擎就从内存直接返回，反之还需要从磁盘上读回来返回。执行器拿到行数据之后完成更新，比如某列N=N+1，并请求执行引擎更新行数据。
执行引擎将数据更新到内存中，然后写redo log，然后返回给执行引擎成功，表示进入prepare状态，随时可提交。
为什么不先写redolog，再写内存？
没有实质区别吧，不都写了内存嘛。是担心数据不一致问题吗？别担心，mysql用了MVCC的，（可重复读级别）不会出现不可重复读的。
执行器收到正常响应后，生成binlog并写入磁盘binlog文件，然后对刚才的操作继续请求置引擎发起commit操作。
如果写binlog失败会怎样？mysql中有个选项binlog_error_action，用来控制如果binlog写失败：
上述变量，其默认值是ABORT_SERVER，即mysqld退出。需要排除binlog写失败原因（如磁盘满、inode耗光等）后再启动起来。
还可以将上述变量设置为IGNORE_ERROR，就是写binlog失败就失败，继续执行，此时就会导致没有生成binlog，无法同步给slave，master-slave数据就会变得不一致。而且也会影响到数据备份。一般是不太能接受的。
重启后，innodb中有prepare阶段的redo log（未commited），这个时候binlog中又没有对应的binlog，此时就会rollback掉。</description></item><item><title>01一条SQL查询语句是如何执行的</title><link>/blog/01%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/</link><pubDate>Sat, 17 Apr 2021 13:08:00 +0800</pubDate><guid>/blog/01%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/</guid><description>MySQL基础架构
mysql基础架构示意图，及主要流程介绍
mysql 连接及内存管理
mysql 8.0删除了查询缓存，为什么：https://mysqlserverteam.com/mysql-8-0-retiring-support-for-the-query-cache/
优化器：存在多个索引，应该用哪一个？
mysql select语句中不存在的列，是在哪个阶段分析出来的呢？分析器
mysqld程序入口:
main: https://sourcegraph.com/github.com/mysql/mysql-server/-/blob/sql/main.cc#L23:12 mysqld_main: https://sourcegraph.com/github.com/mysql/mysql-server/-/blob/sql/mysqld.cc#L7680:5</description></item></channel></rss>