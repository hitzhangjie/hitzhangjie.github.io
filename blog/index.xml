<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on</title><link>/blog/</link><description>Recent content in Blog on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 06 Oct 2020 08:49:55 +0000</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>json库性能对比及实现探究</title><link>/blog/2023-08-23-json%E5%BA%93%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%E5%8F%8A%E5%AE%9E%E7%8E%B0%E6%8E%A2%E7%A9%B6/</link><pubDate>Mon, 09 Oct 2023 16:01:42 +0800</pubDate><guid>/blog/2023-08-23-json%E5%BA%93%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%E5%8F%8A%E5%AE%9E%E7%8E%B0%E6%8E%A2%E7%A9%B6/</guid><description>本文背景 # JSON是一种轻量级的数据交换格式，易于阅读、解析、生成，应用十分广泛。如今在微服务通信中，JSON也是一种常见的序列化手段，比如json-rpc或者gRPC json、pb互转。因为读写场景的不同，对JSON序列化、反序列化（或者解析）的关注点也不一样，一个通用的JSON库不一定能满足性能要求，可以看到有非常多的JSON第三方库频频向标准库发起挑战。本文将从JSON解析的不同场景入手，来说明这些场景下对JSON生成、解析的一些诉求，以及对性能方面的考量，进一步介绍下业界在这方面一些优秀的实践。
回顾JSON标准 # rfc8259是目前JSON事实上的标准https://datatracker.ietf.org/doc/html/rfc8259，一个合法的JSON value必须是一个object、array、number、string，或者以下字面量false、true、null。该规范定义了JSON grammar来说明如何表示上述数据。
rfc8259标准明确提出，如果JSON数据不是在一个封闭系统中使用，在不同系统中进行交换时，字符集应该明确使用UTF-8编码。旧的JSON标准并没有指出这点，但是为了保证不同系统的正常交互，大多数系统使用的正是UTF-8编码。标准还指出在编码时不应该在头部添加BOM字符（Byte Order Mark，U+FEFE），一些JSON解析器为了尽可能保证互操作性可能会忽略被错误添加的BOM字符，而不是报错。
ps：rfc8259中还提及使用Unicode字符，Unicode是一种字符编码标准，定义了字符的唯一码点，而UTF-8是Unicode的一种可变长的具体编码方案，以对ASCII进行向后兼容。
JSON 解析器（parser）将JSON文本转换为另一种表示形式，比如go结构体struct。JSON 解析器必须接受所有符合 JSON grammar的文本，可以接受非 JSON 形式或其他扩展（比如vscode .devcontainer定义中支持注释）。解析器实现可能会对其文本的长度进行限制，也可以对数据的最大嵌套深度进行限制，也可以对数值的范围、精度进行限制。
ps：&amp;ldquo;A JSON parser MAY accept non-JSON forms or extensions.&amp;rdquo; 这句话的意思是，JSON解析器可以接受非JSON形式或扩展。也就是说，解析器可以容忍一些不符合严格JSON语法的文本，或者支持一些扩展的语法或功能。这给了解析器一定的灵活性，使其能够处理一些非标准的JSON文本或具有扩展功能的JSON文本。这样做是为了在实际应用中提供更大的灵活性和兼容性，以满足不同的需求和场景。
JSON生成器（generator）用于生成JSON文本，生成的文本必须严格符合JSON grammar。比如json.Marshal(v)将v这个数据类型序列化成JSON文本，当然还有json.MarshalIndent(v, &amp;quot;&amp;quot;, &amp;quot;\\t&amp;quot;)，会在name前面增加一些缩进，tab、空格等空白字符在标准中也是允许的。
**小结：通过rfc8259我们了解了JSON是用来做什么的，有效的JSON数据是什么样的，为了互操作性、灵活性JSON的解析器、生成器又可以怎么做。**下面我们将介绍一些应用场景，从一般到特殊，对应的也会对标准库实现提出一些挑战，然后进一步介绍一些业界的实践、优化。
从标准库开始 # go标准库中提供了对JSON编码、解析的支持，最常用的两个函数就是json.Marshal、json.Unmarshal。标准库的设计实现，对大多数数据类型、普通的编码解析场景、易用性方面提供了很不错的支持。
在指出标准库在哪些场景下会表现欠佳之前，需要先了解下标准库在编码、解析过程中的一些实现策略、细节。
这里简单总结一下：
标准库json.Marshal的过程，使用了大量的反射操作，比如确定map k、v的类型信息，struct字段的类型信息，匿名嵌套及字段的可见性分析，struct jsontag规则处理，而且是通过反射递归展开json.Marshal(v)中v的类型信息，才能知道如何encode，最后才是根据v及其内部各个组成部分对应的typeEncoder来完成encode输出。encode的过程中虽然它使用了一些caching（缓存）、pooling（池化）技术，但是前面的反射开销确实是比较大的，尤其是数据类型复杂、数据量比较大的时候。
想了解详细过程的话，可以参考这篇总结，会对这个过程中的开销有更清晰的认识：https://www.notion.so/hitzhangjie/JSON-d278399b8092470985cbc423830115fb?pvs=4
标准库json.Unmarshal的过程，和json.Marshal的过程相比，其中涉及到的一些要点大差不差，这里就不展开了。
see: https://sourcegraph.com/github.com/golang/go@go1.19.10/-/tree/src/encoding/json
ps：反射的开销主要在哪里？
reflection trades performance for very dynamic and flexible code by pushing work from compile time to runtime.</description></item><item><title>观测Go应用函数调用：go-ftrace</title><link>/blog/2023-09-25-%E8%A7%82%E6%B5%8Bgo%E5%BA%94%E7%94%A8%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8go-ftrace/</link><pubDate>Thu, 28 Sep 2023 12:42:47 +0800</pubDate><guid>/blog/2023-09-25-%E8%A7%82%E6%B5%8Bgo%E5%BA%94%E7%94%A8%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8go-ftrace/</guid><description>go-ftrace # go-ftrace 是一个基于Linux bpf(2) 的类似内核工具 ftrace(1) 的函数调用跟踪、耗时统计工具，它主要是面向go应用程序的。
限制: 因为设计实现的原因，当前go-ftrace只支持满足如下限制条件的go程序跟踪、统计：
Linux内核：支持 bpf(2) 和 uprobe 的Linux内核 处理器架构: x86-64架构（little-endian字节序） 二进制程序：只能是go ELF可执行程序（非PIE模式），未剔除符号表.symtab，未剔除调试信息.(z)debug_info， 使用方式 # 项目中提供了测试程序 examples/main.go ，可以执行如下几种测试来了解go-ftrace的使用:
示例1: 跟踪一个自定义函数 main.add: ftrace -u main.add ./main 示例2: 跟踪所有的匹配函数 main.add*: ftrace -u 'main.add*' ./main 示例3: 跟踪多个模式匹配的函数 main.add* 或 main.minus*: ftrace -u 'main.add*' -u 'main.minus*' ./main 示例4: 跟踪一个自定义函数 &amp;quot;main.add 以及 内置函数 runtime.chan*: ftrace -u 'main.add' -u 'runtime.chan*' ./main 示例5: 跟踪一个自定义类型的方法: ftrace -u 'main.(*Student).String ./main 示例6: 跟踪一个自定义类型的方法，并试图提取关心的参数: ftrace -u 'main.</description></item><item><title>eBPF案例及分析：gofuncgraph</title><link>/blog/2023-09-15-ebpf%E6%A1%88%E4%BE%8B%E5%8F%8A%E5%88%86%E6%9E%90gofuncgraph/</link><pubDate>Fri, 15 Sep 2023 23:00:22 +0800</pubDate><guid>/blog/2023-09-15-ebpf%E6%A1%88%E4%BE%8B%E5%8F%8A%E5%88%86%E6%9E%90gofuncgraph/</guid><description>前言 # 可观测性（observability）是这几年开始被频繁提及的一个词，特别是在微服务领域可观测性已经成为了微服务治理的一项关键的平台化技术手段，在CNCF孵化的项目中我们看到Opentelemetry如火如荼的发展背后也逐渐出现了一些成熟的解决方案。在腾讯内部也有类似天机阁、蓝鲸、wxg等不同的解决方案。这些往往配合框架解决了微服务RPC层面 的可观测性问题，实际上借助eBPF这项革命性技术，我们还可以做更多。
背景 # 不久前，在做一个关于序列化方面的优化工作，先说下项目情况：项目中使用的go框架采用了pb+protoc-gen-gogofast来生成桩代码，RPC通信的时候使用pb序列化。另外呢，为了方便开发人员查看pb message对应的log信息，项目的日志库使用了pbjson将pb message格式化为json后输出到log，RPC interceptor也会使用相同的方式序列化req、rsp后将其上报到链路跟踪系统。
大致就是这样一个问题，当时对比了pbjson序列化、stdlib encoding/json序列化，segmentio/encoding/json序列化，以及bytedance/sonic序列化。哈哈，这个顺序其实就是由慢到快的一个顺序，bytedance/sonic凭借优化反射、simd等技术“遥遥领先”其他集中方案。除了benchmark的手段，我还想看看上线前后的一些详细的优化效果，比如不同包大小（比如按1KB分桶）的序列化耗时（纳秒）分布。
摆在我面前有两个办法：
改源码，统计下序列化前后的执行耗时，然后打log，写个工具分析下log； 改源码，统计下序列化前后的执行耗时，然后上报到监控，看看统计直方图； 其实都可以，但是我有点懒，我既不想去改源码（更不用说改很多）去写log、报监控，分析完了还需要再把这堆代码删掉。改完代码我还需要编译、发布，我们每次编译发布流程都要10min左右，我很不想去干这些事。
总之我既想要灵活的分析工具（能灵活指定函数名称），又不侵入业务代码，调研之后发现有开发者实现了这样的工具，jschwinger233/gofuncgraph，它借鉴了内核函数图跟踪工具ftrace的设计，执行效果大致如下。借助funcgraph，很快解决了我的问题。
工具介绍 # gofuncgraph是借鉴了Linux内核函数图工具ftrace（function tracer）的功能，然后为go程序开发的一个函数图工具，如上图所示，你可以指定要跟踪的函数的匹配模式，然后该工具会将程序中匹配的函数名全部作为uprobe去注册，并注册上对应的回调处理函数。
处理函数中会根据是进入函数、退出函数来生成一些这样的events，每个event都有时间，这样就可以准确统计出函数的执行耗时了。然后利用调用栈信息，也可以绘制出函数调用图。最终输出上述函数图。
一个小插曲，help: how to use gofuncgraph，最开始我以为是要用这个工具去启动个程序才可以执行测试，是我理解有误。和作者沟通过程中，作者提到之前阅读过我写的调试器相关的电子书，并说质量很高。大家互相分享互相学习，挺好的。现在我也来学习作者的gofuncgraph，除了学习ebpf程序的写法外，我也想了解下为什么调试器的知识会用在这个程序里。
剖析实现 # 本节先介绍该工具的用户界面设计实现，然后再介绍其内部的工作逻辑，工作逻辑中会层层深入把必要的DWARF、eBPF、编译链接加载等相关的关键内容都逐一介绍下。
为了后续方便自己学习、维护、定制，我fork了作者的项目并做了一些优化、重构，如使用spf13/cobra来代替了原先的命令行框架，spf13/cobra支持长、短选项，对用户更友好。另外也对项目代码进行了一些可读性方面的优化。后续介绍将继续我修改的这个版本介绍 hitzhangjie/gofuncgraph (dev)。
命令行界面 # 执行 gofuncgraph help 查看帮助信息，简要介绍了它的用途，你可以执行gofuncgraph --help来查看更完整的帮助信息。
简要帮助信息：
$ ./gofuncgraph bpf(2)-based ftrace(1)-like function graph tracer for Go! for now, only support following cases: - OS: Linux (always little endian) - arch: x86-64 - binary: go ELF executable built with non-stripped non-PIE mode Usage: gofuncgraph [-u wildcards|-x|-d] &amp;lt;binary&amp;gt; [fetch] [flags] Flags: -d, --debug enable debug logging -x, --exclude-vendor exclude vendor (default true) -h, --help help for gofuncgraph -t, --toggle Help message for toggle -u, --uprobe-wildcards strings wildcards for code to add uprobes 详细帮助信息：</description></item><item><title>eBPF BCC框架：helloworld</title><link>/blog/2023-09-15-ebpf-bcc%E6%A1%86%E6%9E%B6helloworld/</link><pubDate>Fri, 15 Sep 2023 16:36:33 +0800</pubDate><guid>/blog/2023-09-15-ebpf-bcc%E6%A1%86%E6%9E%B6helloworld/</guid><description>怎么写eBPF程序 # 目前写eBPF程序的话，一般要通过C语言来写，python、golang写的都是用户态的部分，涉及到内核部分的操作都是要借助C语言来写，然后通过编译器将C部分编译成字节码，用户态部分只是借助bpf()系统调用将字节码程序提交给了eBPF子系统去运行。
实际上任何高级语言都可以写用户态部分，但是写内核态部分的eBPF程序需要写C语言，编译器会将C语言部分编译成target=ebpf的字节码，所以现在有很多框架比如BCC+python以及Cilium+golang等，都是对eBPF字节码操作、系统调用操作的一些封装。
ps：如果你是用Rust的话，那么确实可以直接写eBPF程序，不用依靠C，一般常用的是Rust aya这个框架。
从0开始写eBPF程序 # 前面多次提到了eBPF程序编写、执行的大致过程，但是介绍的还是太粗略了，也不打算在这么几个简单的总结性文档中，把细节都介绍清楚。
我们可以先看下，如果手把手从0开始写eBPF程序，大致需要经历哪些操作，看图：
![Go](assets/2023-09-15-eBPF BCC框架：helloworld/go.png)
需要被简化的一些操作：
用C语言先写eBPF程序，然后使用编译器（如clang）将其编译为target为bpf的字节码程序，然后通过系统调用将其提交给eBPF子系统执行。这一步如果没有BCC这样的框架封装下的话，那么操作起来就有一点啰嗦。 还有你编译eBPF程序时要用到的很多头文件之类的设置，可能就比较麻烦。 还有eBPF程序执行时，那些结果存储到不同的数据结构，和不同语言的类型系统如何对接，如何方便的读取，全部自己从0开始搞也很麻烦。 其他的； 所以现在有BCC、Cilium、Aya这样的一些eBPF框架来简化这一些工作，我们可以先从BCC开始，这个项目比较早、成熟，用的人也多，也被集成到了Linux YUM源中，可以直接安装bcc、bcc-tools包来尝鲜。
从helloworld开始 # 现在就开始使用BCC来写几个helloworld，让大家了解下一个简单的eBPF程序大致是如何写的，熟悉下其结构，后面虽然不一定自己写，但是了解已有的这些工具的实现细节，以及如何调整来满足自己需要，还是有帮助的。
file: helloworld.py
#!/usr/bin/python3 from bcc import BPF program = r&amp;quot;&amp;quot;&amp;quot; int hello(void *ctx) { bpf_trace_printk(&amp;quot;Hello World!&amp;quot;); return 0; } &amp;quot;&amp;quot;&amp;quot; b = BPF(text=program) syscall = b.get_syscall_fnname(&amp;quot;execve&amp;quot;) b.attach_kprobe(event=syscall, fn_name=&amp;quot;hello&amp;quot;) b.trace_print() 分析下其结构：
导入bcc中的bpf program是一段c语言程序，b = BPF(text=program)，我们执行这个脚本时bcc框架会自动将其编译为字节码 它定义了一个hello函数，bpf_trace_printk会向ebpf子系统中的一个临时文件或者什么数据结构中打印hello world字符串 syscall = b.get_syscall_fnname是获得exeve函数调用的一个hook point b.attach_kprobe是在execve这个系统调用入口处通过kprobe系统调用创建一个探针，当执行到这里时会触发trap，内核会回调函数hello去执行，这里的hello就是上面C语言中定义的函数 b.trace_print会从取出前面打印的hello world字符串取出来打印出来。 这就是一个极简的helloworld的示例，当我们执行它时，它就会跟踪所有的execve的系统调用，每次触发这个系统调用时，就会打印上述helloworld字符串信息。</description></item><item><title>eBPF原理及实践：什么是eBPF</title><link>/blog/2023-09-15-ebpf%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E8%B7%B5%E4%BB%80%E4%B9%88%E6%98%AFebpf/</link><pubDate>Fri, 15 Sep 2023 11:57:13 +0800</pubDate><guid>/blog/2023-09-15-ebpf%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E8%B7%B5%E4%BB%80%E4%B9%88%E6%98%AFebpf/</guid><description>eBPF是一项革命性的内核技术，它允许开发人员编写自定义的代码，然后被内核动态加载后执行，以此来改变内核的执行行为。它的这个特点能够帮助实现高性能网络、更好的可观测性、更细致的安全分析工具。
eBPF的前身：bpf # 1、eBPF的前身是bpf（BSD Packet Filter），最早它在1993年论文中有Lawrence Berkeley National Laboratory的Steven McCanne和Van Jacobson提出，它是一种类似字节码虚拟机的东西，有自己的指令集，你可以通过它来编写程序然后交给这个小的虚拟机去执行，这个指令集非常像汇编。比如你可以用它来写包过滤的逻辑（接受或者拒绝一个网络分组）。在这篇论文中可以找到其他一些更加复杂的示例程序，论文名：The BSD Packet Filter: A New Architecture for User-level Packet Capture。
2、1997年，Linux内核版本2.1.75首次引入了BPF，BPF也就开始成为了Berkeley Packet Filter的简称，主要用在tcpdump这些工具中来实现高效的网络包的跟踪。
3、时间快进到2012年，Linux内核版本3.5中引入了seccomp-bpf，它能够控制是否允许用户态应用程序执行系统调用，举个例子，我们启动一个docker容器，如果不添加特殊的选项控制，在docker容器内部去调试程序的时候是执行不了的，因为Linux系统中程序调试需要利用系统调用ptrace，但是ptrace往往都是被默认不允许的，发挥作用的就是seccomp-bpf，这里有一篇文章介绍了seccomp+ptrace调试原理的文章：https://zhuanlan.zhihu.com/p/606317619。seccomp-bpf是首次开始将bpf从包过滤这个范畴开始向其他范畴扩展。到今天发展到eBPF这个阶段，其实与最早的“包过滤”已经没有多大关系了。
从BPF到eBPF # 随着BPF在Linux内核中的演进，到了2014年，从版本3.18开始可以使用eBPF将来称呼这项技术，全程就是extended BPF，这包含了几个比较明显的改变：
BPF指令级对64位机器做了高度的优化，解释器也基本上重写了； eBPF中增加了maps，BPF程序执行时可以访问它记录一些数据，这些数据可以在BPF程序间共享，也可以允许用户态程序访问它获取结果； 增加了bpf()系统调用，用户态程序通过它可以和eBPF程序进行交互，比如加载到内核、从内核卸载、访问maps数据等； 增加了bpf_这样的一些helper函数； 增加了eBPF程序验证器，验证安全的程序才可以被执行； 这是eBPF首次正式放出，但是不是结束，此后就开始了它的快速发展之路。
eBPF到生产系统 # 这里介绍下eBPF技术演进过程中的一些关键事件：
2005年Linux中就引入了特性kprobe，它允许在任意指令地址处设置trap，当执行到此处时允许回调用户自定义的函数。开发人员可以编写内核模块，将其中的函数设置为kprobe的回调以执行调试。 ps: 调试器一般也是使用这种指令patch的方式，区别在于kprobe回调函数是内核处理的，而调试器tracee执行时触发断点是内核通过信号通知tracer由tracer来执行的。 2015年的时候允许将eBPF程序连接到kprobe，kprobe可以回调eBPF程序了，这使得在Linux中tracing变得简单，为了更好的追踪Linux内核网络栈的各类事件，Linux中开始增加各种hooks允许eBPF程序进行更细致的观测。 2016年，Netflix的工程师Gregg大佬公开了他和团队在eBPF基础上的大量性能观测工具及实践，让基础设施、运维领域认识到了eBPF在这方面的巨大潜力。 2017年，Facebook开源了Katran这个基于eBPF的高性能L4负载均衡器，也是这一年，Liz Rice这位女强人对此也产生了浓厚的兴趣，并开始研究。 ps: Liz Rice 经常做些技术方面的分享，目前是 the chief open source officer with eBPF specialists at Isovalent, 也是 the Cilium cloud native networking, security and observability project 的创建者.</description></item><item><title>Linux性能问题排查60s</title><link>/blog/2023-09-08-linux%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A560s/</link><pubDate>Fri, 08 Sep 2023 16:30:30 +0800</pubDate><guid>/blog/2023-09-08-linux%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A560s/</guid><description>简介 # 最近在阅读Gregg大佬著作《BPF Performance Tools》，其中一小节作者提到了其在Netflix工程团队中践行的一个性能排查checklist，当遇到Linux性能问题时，前60s往往是借助这个checklist来进行排查，如果有必要，缩小范围后再借助其他工具进行进一步排查。我觉得这个简短的checklist还挺实用的，特地摘录出来分享下。
问题背景 # 这个checklist可以用来指导排查任意Linux性能问题，当我们知道有台机器性能（疑似）有问题时，我们就可以登录这台机器，按照这个checklist来进行前60s的快速分析。这也是Gregg自己以及Netflix工程团队实践中总结出来的。
对于很多刚入行后台开发的同学而言，我觉得这个还是比较有价值的，应该在日常工作中不断实践、不断加深对性能影响因素的理解。有位技术扎实的同事曾经这样说，一切都是可计算的、可量化的，比如判断对特定工作负载瓶颈是什么，cpu、内存、网卡？链路长短，网络延迟，然后大致的系统吞吐量是什么样的？他大致就能推算出来。
其实，Jeff Dean曾经在论文里给出过一些开发人员应该知晓的latency数据：
L1 cache reference ......................... 0.5 ns Branch mispredict ............................ 5 ns L2 cache reference ........................... 7 ns Mutex lock/unlock ........................... 25 ns Main memory reference ...................... 100 ns Compress 1K bytes with Zippy ............. 3,000 ns = 3 µs Send 2K bytes over 1 Gbps network ....... 20,000 ns = 20 µs SSD random read ........................ 150,000 ns = 150 µs Read 1 MB sequentially from memory .</description></item><item><title>Netflix自适应过载保护算法</title><link>/blog/2023-04-18-netflix%E8%87%AA%E9%80%82%E5%BA%94%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4/</link><pubDate>Tue, 18 Apr 2023 23:38:09 +0800</pubDate><guid>/blog/2023-04-18-netflix%E8%87%AA%E9%80%82%E5%BA%94%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4/</guid><description>思路简介 # Netflix/concurrency-limits，基于Java实现的，star 2.9k+，也有go语言的第三方实现platinummonkey/go-concurrency-limits。
平时大家评估服务负载、容量、最佳qps是如何做的，往往是先压测看服务能抗多少qps，然后请求方取个75%*qps作为一个阈值，然后请求方通过令牌桶、漏洞之类的来进行控制。但是对于很多个节点、需要动态扩缩容场景，这个固定值很快就会失效……当然有分布式频控的搞法……netflix的思路是与其将重点放在如何告知客户端设置qps，还不如让客户端能根据rtt自动算出下阶段的最大请求量来，这个是借鉴了little’s law以及tcp的拥塞控制。
它这里vegas算法估计的limit是这么算的 L * (1-minRTT/sampleRTT)， 然后还有个gradient2优化，来平滑下 详细设计 # 这个库提供了很多的limiter实现：
fixed，固定值，并发请求的时刻量不能超过这个fixedlimiter的值，这个值不变
aimd，基于loss的，请求成功就加性增，遇到失败就乘性减
windowed，基于滑动窗口实现的，每个窗口期内有一个limiter（成员delegate）,可以是前面提到的fixed、aimd等limiter
vegas，是基于测量的rtt的，另外也会考虑丢包。它实际上是确定了这么几个负载阶段：请求没有排队、请求有少量排队、请求有多一点排队、请求有很多排队。每次采样后会更新最新的limit，更新时会首先根据当前minRTT和sampleRTT以及当前limit来算一下接下里的queueSize，然后检查queueSize处于上面哪个阶段，然后使用对数操作进行平滑对当前的limit进行增大、缩小的调整。
gradient，它这里和vegas的实现思想上是一致的，只是对于inflight*2≥estimatedLimit时的处理逻辑不一样，vegas是将排队严重情况分成了几个阶段用不同的函数来调整limit，gradient是用了一个“梯度”的方法来调整，大致上是当前estimatedLimit * gradient + queueSize…这个算法的平滑处理能理解，但是不是那么“想象“象其效果。
仔细看下，多揣摩几遍还是可以想象的出来的 😂
gradient2，它这里是对gradient的一个优化，什么优化呢？gradient是基于测量minRTT的，这会有个问题，minRTT还是比较敏感的，对于测量tcp的包（因为通常都会分片、分片大小往往都是确定的）没啥问题还挺好的。
但是使用minRTT来测量RPC就不是特别好，因为RPC请求，不同接口的请求可能大小变化挺大的，即使是相同接口的请求可能变化也比较明显的。所以使用avgRTT要比minRTT更友好些，不至于limit的“抖动”，可能会导致过度的load shedding，造成不必要的请求被拦截。
然后这里的avgRTT怎么算呢？从开始到现在的请求RTT的平均值？这里其实用的一个指数平均，一方面有平均值的作用能避免minRTT的上述问题；另一方面，使用的指数平均，0.8longtermRTT + 0.2sampleRTT，这样也能尽可能反映当前时刻的负载信息。
另外这里的tolerance=2.0是说，如果遇到sampleRTT=tolerance*longtermRTT时，可以容忍这么长耗时的请求而不降低limit，仍然可以按照原速率发送，如果超了tolerance下的设置，那么梯度gradient就会小于1.0，此时limit就会被调低。limit调低时也会被smooth参数进一步平滑下。
当从过载中恢复时，因为longtermRTT也被搞大了，如果不加处理，可能会有较长一段时间才能恢复到≤sampleRTT，这会有个问题，如果不能尽快恢复longtermRTT，则有可能持续增加发包速率再次导致过载。为了尽快恢复longtermRTT到正常值让发包速率处于steady状态，会判断longrtt / shortrtt&amp;gt;2时会给longrtt*0.95尽快调低longrtt。
调查总结 # 总结一下，vegas、gradient都是基于minRTT进行测量的，对于RPC场景而言可能并非最佳选择。相比之下gradient2是基于longtermRTT指数平均代替了minRTT，对RPC场景适应性可能更好。
除了RTT，它们都考虑了负载steady、overload情况下的不同阶段以及调整策略（主要是increase limit、decrease limit时如何做到平滑）。可以测试下gradient2先有个直观认识。
一点后话 # 当你的系统是一个大型的分布式系统，集群也需要动态扩缩容，系统中的负载类型不同，同一个服务的不同接口处理耗时不同，即便是相同接口不同请求处理耗时也有明显不同，这个时候常规的基于“请求配额”的传统过载保护机制是不怎么有效的。
最初有这种想法，是在看点做内容处理链路的时候，注意到有些服务是计算密集型的（如OCR模块），有些是IO密集型的，有些图文发表请求里面只有一张图片，有的有多张图片，有的文章比较短，有的文章比较长，这都会影响你的系统负载、处理耗时，如何科学的评估负载进而确定合理的请求配额，是一件比较困难的事情。
后面开始思考如何评估“负载”这样的问题，可能会想CPU使用率、内存使用率高、IO利用率高、网卡利用率高，实际上不同workload类型对资源的使用情况不同，这些指标高还真不一定就是负载高。如果涉及到具体语言，可能会去想Java、Go GC STW问题……
预期纠结这些，不如更高屋建瓴地站在宏观角度看看，如果负载高了会发生什么？系统负载开始变高之后，是可以把其当做一个黑盒通过外部观测来观察出来的。Netflix的过载保护算法正是从这里触发，看似简单的实现，但是并不是不着边际。整个网络世界得以正常运转的TCP拥塞控制也是建立在RTT、Loss观测基础上的，Netflix也将其Vegas Limiter命名成了Vegas，正是因为它借鉴了TCP vegas拥塞控制算法（TCP Reno的替代算法）。</description></item><item><title>压测之接口lo的妙用</title><link>/blog/2023-04-14-%E5%8E%8B%E6%B5%8B%E4%B9%8B%E6%8E%A5%E5%8F%A3lo%E7%9A%84%E5%A6%99%E7%94%A8/</link><pubDate>Fri, 14 Apr 2023 03:10:08 +0000</pubDate><guid>/blog/2023-04-14-%E5%8E%8B%E6%B5%8B%E4%B9%8B%E6%8E%A5%E5%8F%A3lo%E7%9A%84%E5%A6%99%E7%94%A8/</guid><description>问题背景 # 前一篇文章介绍了本地开发机压测时如何为每个待压测分配CPU资源（其实是taskset进行绑核，由于没有其他负载可以近似为分配CPU资源），本文继续介绍下如何让压测变得更真实一点，那就是网络IO这块，在本地通信时往往使用的是loopback接口，但是loopback并不是一个真实的网卡设备，它基本没有什么硬件网卡设备的传输速率的限制，也没有网络传输过程中的传输延迟。
这样的话，我们在压测的时候，网络方面的开销就几乎体现不出来，比如说，你想看下在4g网络下客户端、服务器之间网络通信数据包多大时打开数据压缩更有价值……
在我的测试过程中我希望能尽可能简化测试工作的同时，也能保证该有的环境的真实性，于是就有了本文对loopback接口的一点探索。
认识本地lo # Linux中的Loopback接口是一个虚拟网络接口，允许在同一主机上运行的应用程序之间通信。它通常被称为“lo”接口，具有IP地址127.0.0.1。
Loopback接口在内核中使用Loopback驱动程序实现，创建一个虚拟网络接口，并将所有传入的数据转发到本地协议栈。当一个应用程序将数据发送到loopback接口时，数据会被回送到协议栈，并像从另一个网络接口到达一样转发。 在Linux中，Loopback接口的一个重要用例是用于测试和调试网络应用程序。通过通过Loopback接口发送和接收数据，应用程序可以模拟网络流量，而不实际发送或接收来自物理或虚拟网络接口的数据。
Loopback接口还由一些网络协议使用，例如Kubernetes kube-proxy IPVS，OSPF和其他需要在同一主机上的进程之间通信的网络相关软件。
总之，Linux中的Loopback接口是一个虚拟网络接口，为在同一主机上运行的应用程序提供了一种通信通道。它在内核中使用Loopback驱动程序实现，并且在测试、调试和网络相关软件中具有许多实际用例。
认识netem # 在 Linux 中，ip 命令中的 netem 是一个网络模拟工具。它允许您对网络连接进行各种修改，例如，添加延迟、丢包以及增加噪声等，以便在网络环境下测试应用程序的性能和稳定性。使用 netem 工具，您可以模拟各种不同的网络条件，包括高延迟、高带宽和低带宽等，以便更好地测试和优化应用程序在各种网络条件下的行为。
Netem 已经成为 Linux 网络模拟和测试工具的标准选择之一，同时也是在诸如交换机、路由器和 WAN 加速器等网络设备上进行隔离测试和仿真时的一个有用工具。通过使用 netem，您可以更好地了解您的应用程序在不同网络条件下的行为，并且能够更好地进行演示和培训。
利用本地lo # 如何使用netem让本地loopback接口更好地模拟真实网络情况呢？下面就来简单说一下。
启用netem # 首先，需要启用内核模块netem：
sudo yum install -y kmod sudo modprobe sch_netem 模拟网络延迟 # 然后，如果loopback接口的每次的收、发操作模拟一定的网络延迟：
sudo yum install iproute-tc or sudo yum install iproute sudo tc qdisc add dev lo root netem delay 1ms 这样的话就相当于一个rtt增加了2ms，为了验证这个，你可以在执行上述模拟前后，分别看下ping localhost的延迟。</description></item><item><title>压测之taskset的妙用</title><link>/blog/2023-04-13-%E5%8E%8B%E6%B5%8B%E4%B9%8Btaskset%E7%9A%84%E5%A6%99%E7%94%A8/</link><pubDate>Thu, 13 Apr 2023 03:22:15 +0800</pubDate><guid>/blog/2023-04-13-%E5%8E%8B%E6%B5%8B%E4%B9%8Btaskset%E7%9A%84%E5%A6%99%E7%94%A8/</guid><description>问题背景 # 想测试下gRPC框架的性能，设计了如下服务拓扑来对gRPC框架各组件、特性、参数配置下的性能进行探索。
压力源程序 perfclient ---请求-&amp;gt; perfserver1 ---请求-&amp;gt; perfserver2 压力源程序perfclient会并发发送请求给服务perfserver1，perfserver1则会继续请求perfserver2，然后perfserver2回包给perfserver1，perfserver1收到响应后内部完成处理逻辑后继续回包给perfclient。
perfclient每隔一段时间会打印下请求的请求量、成功量、失败量，以及qps、耗时信息。需要注意的事，这里再统计耗时信息的时候，除了avg、min、max耗时，还需要percentile(or quantile）百分位耗时，后者更具有说服力。
现在呢？遇到点问题，正常我需要将上述压力源程序、被压测服务perfserver1、perfserver2尽力部署到不同的机器上，让它们之间避免相互影响，同时部署的机器上也应该注意没有其他负载会干扰到我们的测试，但是问题来了：
可能有机器，但是部署起来太麻烦了，可能每调整下测试点就要要操作多台机器 可能有机器，但是云平台存在超卖的情况，母机负载大影响到了虚拟机负载稳定性 可能有机器，但是ci/cd流水线执行耗时太久了 可能没机器，只有一台本地开发机 有没有什么其他简单好用的办法呢？我觉得有，资源隔离下啊。
认识taskset # taskset，是linux下用来进行绑核设置的一个工具，我们可以借助它对上述不同的3个进程的cpu资源进行限定，如压力源程序perfclient需要能多生成些请求，我们给它分配7~10 4个cpu core，perfserver1负载会稍微比perfserver2高点，但如果是纯echo的话也多不了读少，给perfserver1分配2个cpu core，给perfserver2也分2个。
taskset -a -p 7,8,9,10 `pidof perfclient` taskset -a -p 3,4 `pidof perfserver1` taskset -a -p 5,6 `pidof perfserver2` 这样上述几个进程就被分别设置到了不同的cpu cores上执行，意味着当他们把cpu跑满时，他们能抗的负载大致就是这个比例。
解释下选项-a：
taskset如果不指定选项-a，则知会对当前进程名对应的主进程进行绑核设置，不会对进程中的其他线程进行设置，当然也不会对后续新创建的线程进行设置。
加了-a，taskset就会对执行命令时，该进程pid下的所有线程进行统一的绑核设置，但是如果后续创建了新线程，新线程不会被绑核。
那么如果一个程序是多线程程序，且线程数不是固定的，会在以后新创建、销毁动态变化的，这种该怎么解决呢？
go天然多线程 # go程序天然是多线程程序，那应该如何进行绑核设置呢？如果只是为了限制进程使用的cpu资源，直接使用runtime.GOMAXPROC(x)进行设置不行吗？不行！
该函数只是说限制同时在运行的线程数，并没有像taskset那样将线程绑到核上，这意味着这些go程序线程的执行有可能会在cpu core上迁移，这样的话通过top命令查看cpu core负载情况，就不好判断哪个core的负载是因为哪个进程引起的…对吧。
另一个问题，go程序的GMP调度模型会在必要时自动创建新的线程出来，用来执行goroutines，这里问题就来了，我需要动态感知当前进程下的所有线程。go语言或者标准库都没有提供线程层面的东西来获取，那我们怎么获取呢？
go如何绑核 # Linux下面每个进程都有一个pid，对应的虚拟文件系统/proc//tasks下面就是该进程pid下的所有线程信息。理论上可以定时获取里面的pid，然后再去taskset -p绑核，或者说go启动一个协程定时调用下taskset -a -p &amp;lt;pid&amp;gt;，可以简洁明了搞定。</description></item><item><title>从排队论到过载保护算法</title><link>/blog/2023-04-12-%E4%BB%8E%E6%8E%92%E9%98%9F%E8%AE%BA%E5%88%B0%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 12 Apr 2023 10:38:09 +0800</pubDate><guid>/blog/2023-04-12-%E4%BB%8E%E6%8E%92%E9%98%9F%E8%AE%BA%E5%88%B0%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4%E7%AE%97%E6%B3%95/</guid><description>排队论简介 # 排队论的精髓是通过数学模型和分析方法来研究排队系统的性能和行为。排队系统是指由一些服务设施和一些顾客组成的系统，顾客需要排队等待服务。排队论的目标是研究如何优化排队系统的性能，以提高服务质量和效率。
排队论的核心是建立数学模型来描述排队系统的行为。这些模型通常基于随机过程和概率论，用于描述顾客到达的随机性、服务时间的随机性以及服务设施的数量和性能等因素。通过分析这些模型，可以得出排队系统的性能指标，如平均等待时间、平均逗留时间、系统利用率等等。
排队论的应用非常广泛，涉及到许多领域，如交通运输、通信网络、制造业、医疗保健等等。在这些领域中，排队论可以用来优化资源利用、提高服务质量、降低成本等等。
总之，排队论的精髓是通过数学模型和分析方法来研究排队系统的性能和行为，以优化系统的性能和效率。
公式及应用 # 排队论是一个非常广泛的领域，其中涉及到许多不同的理论公式和应用。以下是一些常见的排队论理论公式和计算机系统中的应用：
Little&amp;rsquo;s Law：L = λW，其中L表示系统中平均顾客数，λ表示到达率，W表示平均逗留时间。这个公式表明，系统中平均顾客数等于到达率乘以平均逗留时间。在计算机系统中，这个公式可以用来计算系统中的平均并发请求数，以及系统的响应时间和吞吐量。 Kendall&amp;rsquo;s Notation：A/B/C/K/N，其中A表示到达过程的类型，B表示服务时间的分布类型，C表示服务设施的数量，K表示服务设施的排队规则，N表示系统容量。这个符号表示了排队系统的基本特征，可以用来描述和比较不同的排队系统。 M/M/1队列：这是一个经典的排队模型，其中到达过程和服务时间都是指数分布，服务设施数量为1。这个模型可以用来计算系统的平均等待时间、平均逗留时间、系统利用率等指标。在计算机系统中，这个模型可以用来分析单个服务器的性能。 M/M/m队列：这是一个扩展的M/M/1队列模型，其中服务设施数量为m。这个模型可以用来计算系统的平均等待时间、平均逗留时间、系统利用率等指标。在计算机系统中，这个模型可以用来分析多个服务器的性能。 网络队列模型：这是一个用于分析计算机网络性能的排队模型，其中网络节点被建模为队列，数据包被建模为顾客。这个模型可以用来计算网络的吞吐量、延迟、丢包率等指标。 负载均衡算法：这是一种用于优化计算机系统性能的算法，通过将负载均衡地分配到多个服务器上，以提高系统的吞吐量和可靠性。排队论可以用来分析负载均衡算法的性能和效率。 这些理论公式和应用只是排队论中的一部分，排队论还涉及到许多其他的理论和应用，如排队网络、排队模拟、排队优化等等。
浅谈Little&amp;rsquo;s Law # 简要说明 # Little&amp;rsquo;s Law是一个基本的排队论原理，它描述了在一个稳定的系统中，平均顾客数、平均等待时间和平均服务率之间的关系。该原理最初由美国数学家John Little在1961年提出，被广泛应用于各种排队系统的性能分析和优化。
Little&amp;rsquo;s Law的问题背景是排队系统，例如银行、超市、餐厅等等。在这些系统中，顾客到达、等待和离开的过程构成了一个排队模型。Little&amp;rsquo;s Law的目的是通过分析这个模型，来解决如何优化排队系统的问题。
Little&amp;rsquo;s Law的核心公式是：L = λW，其中L表示平均顾客数，λ表示平均到达率，W表示平均等待时间。这个公式告诉我们，如果我们知道了平均到达速率和平均等待时间，就可以计算出平均顾客数。反之亦然，如果我们知道了平均顾客数和平均等待时间，就可以计算出平均到达速率。这个公式可以帮助我们更好地理解排队系统的性能，并且指导我们如何优化排队系统。
应用案例 # 以下是一些应用Little&amp;rsquo;s Law的案例：
在一个银行分行，平均每小时有100名顾客到达，平均等待时间为10分钟，那么该分行的平均顾客数是多少？根据Little&amp;rsquo;s Law，L = λW = 100 * 10 / 60 = 16.67，因此该分行的平均顾客数为16.67人。 一家餐厅想要提高服务质量，他们决定增加服务员的数量。根据Little&amp;rsquo;s Law，如果他们想要减少平均等待时间，他们需要增加服务员的数量，以提高服务率。如果他们想要减少平均顾客数，他们需要减少到达率，例如通过减少广告宣传或者提高价格等方式。 在线服务领域 # 理解little&amp;rsquo;s law # 将其应用到我们熟悉的在线服务领域的话，可以达到稳态的前提下，调整下相关参数：
队列平均长度可视为同时被服务的请求个数，即服务并发度Concurrency， 队列人数到达(速)率可视为服务吞吐Throughput， 平均服务时间可视为服务平均处理延迟Latency（可细分为等待延迟+处理延迟）， 这样可以得到另一个版本的Little&amp;rsquo;s Law，Concurrency=Throughput * Latency。</description></item></channel></rss>