<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分布式锁 on</title><link>/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link><description>Recent content in 分布式锁 on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 25 Sep 2022 15:31:53 +0800</lastBuildDate><atom:link href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/index.xml" rel="self" type="application/rss+xml"/><item><title>分布式锁方案的思考</title><link>/blog/2022-09-25-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Sun, 25 Sep 2022 15:31:53 +0800</pubDate><guid>/blog/2022-09-25-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>问题背景 # 在微服务架构下，经常面临一些事务性处理需要考虑一致性的场景，笔者工作过程中，很多场景最终都采取了可用性优先、数据最终一致的方案。最后我们可能也会结合一些对账之类的手段来发现异常并修复不一致的数据。
归根结底，是因为微服务架构下上述事务性处理方案没有保证刚性事务的ACID原则，其弱化了对A、C的控制，OK，这种方案并非不可接受的，只要业务场景适用即可。这里还有一点就是I（Isolation）的原则，如何保证微服务架构下事务处理的隔离性呢？
分布式锁，就是大家常用的方案，只不过关于对分布式锁的认识，可能大家认识的程度并没有自己认为的那么到位。
常见方案分类 # 分布式锁，归根究底是为了保证任务的排他性执行。但是为了排他性执行的初衷却可能是不同的，所以我们接下来会先按照要解决问题进行分类。然后呢，考虑到可用性、正确性，实现分布式锁的具体方法也是不同的，然后我们也可以按照实现方式进行分类。
按解决问题分类 # 解决效率类问题：为了避免资源浪费，如每天统计下业务下所有服务接口成功率数据，这类定时任务也是多机部署的避免单点问题，但是只要一台机器执行就行了，属于解决效率类问题。没有必要多台机器执行，但是即便都执行了也没啥影响，只是后面执行的覆盖掉前面的执行了，仅此而已。 解决正确性问题：任务必须排他性执行，如果并发执行则存在正确性问题。比如用户购买游戏道具时需要读取玩家金币数、扣金币、写回，这里涉及到典型的Read、Modify、Write的问题，如果这个操作时序不是排他性的，就掺杂着重置、送礼等各种可能修改金币的操作时序，则会导致正确性问题。 按实现方式分类 # 基于缓存实现：比如利用redis、memcache等实现，分布式缓存一般提供了get、set的能力，允许给key、value指定版本、排他性、过期时间属性，来实现分布式锁。 基于共识算法实现：比如etcd、zk这类底层有raft、zab共识算法支撑的组件，借助他们可以比较可靠的实现分布式锁，至少能保证分配锁时不会导致client错误持有锁。 在实际实现、使用分布式锁时，我们多数时候是冲着正确性去的，但是方案本身其实是不完备的，但是我们却将其当做了“正确的”。
常见方案介绍 # 基于redis单机版 # 比较常见的就是单机版的redis实现版本，如下所示：
# 加锁操作 SET resource_name my_random_value NX PX 30000 # 解锁操作 if redis.call(&amp;quot;get&amp;quot;,KEYS[1]) == ARGV[1] then return redis.call(&amp;quot;del&amp;quot;,KEYS[1]) else return 0 end 实际情况是单机版redis存在单点问题，为了解决这个问题，通常又会给redis master挂个slave来备份数据，但是redis的备份机制是异步的，所以仍然存在主备切换时丢失锁数据而导致的错误加锁情况，解决不了正确性。
基于redis集群版 # 集群版的redis，用的比较多的是redlock算法。redlock算法主要是解决单点故障问题，它的主要思想是，假设集群中有N个（建议值5）master节点，这些master节点及其replicas各自维护一些不相干的keyslots。加锁时，client先获取本地时间，然后串行地向N个节点发起请求，是串行的。
至于详细的实现，redis官网上有这些推荐实现的github repo链接，可以自己去找找看。
其实这个算法也解决不了网络波动、分区极端场景下，依然会导致client错误持有锁的情况，比如1、2、3、4、5个节点，一开始网络正常client1持有了1、2、3上的锁，后面网络波动导致client只能访问3、4、5，并且3发生了主备切换而备份上缺少数据，client依然能在3、4、5上获得相同锁。
尽管redlock算法提出了一些可以缓解正确性被破坏的想法，但是仍然不能保证分配锁时的正确性。
基于etcd实现 # etcd本身是基于raft算法实现的副本的状态复制，是有可靠的共识理论支撑的工程实现，另外etcd号称其raft算法实现有着比paxos算法更好的性能（这个没求证，多数情况下paxos算法可能性能更优点，也不一定非得有master节点），感兴趣的可以自行了解。
基于etcd的分布式锁实现，已经内置在etcd中了，直接使用即可。
因为示例代码的篇幅原因、go.mod设置等，我们就不在这里占用太多篇幅了，感兴趣的可以直接查看这里的示例代码：https://github.com/hitzhangjie/codemaster/blob/master/distlock/main.go。
思考下 # 我们从解决效率问题的分布式锁，到解决正确性问题的分布式锁，对锁分配正确性的要求提升了一档，关于其实现方案，也从基于redis、redis集群版的方案，也过渡到了基于raft算法的etcd实现（其他的基于paxos、zab共识算法的类似就不介绍了），我们解决了锁服务分配锁时的正确性问题，但是这样就能保证任务排他性执行了吗？
不能！client在使用锁的时候，可能会出现如下情况：
client出现了崩溃、逻辑bug，导致锁没有被正确地释放掉，如果锁没有过期时间，将导致其他client加锁时出现死锁； client加锁时设置过期时间，但是过期时间可能设置的过短，锁过期被etcd清理然后又重新分配给了其他client，然后旧client还以为自己别锁保护的临界区内还可以肆意妄为，导致并发执行错误； client加锁时设置了合理的过期时间，但是自身因为其他原因出现了一定时间阻塞，恢复后继续执行，但是锁实际已经过期被释放； client执行操作时锁确实是有效的，但是在其发起对下游的请求后，下游继续处理期间锁过期，其他client持有了锁并发继续执行操作； 看到没，即便是锁分配是正确的，client使用锁时依然无法100%保证正确性，这个问题能100%解决吗？不能，但是可以尽可能缓解，比如合理设置锁过期时间，比如请求方调用下游服务时，把锁信息带给下游让下游能够去锁服务校验锁有效性。</description></item></channel></rss>