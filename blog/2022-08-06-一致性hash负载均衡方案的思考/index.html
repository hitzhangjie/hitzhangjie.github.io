<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=/main.b078dd132968e74ef07e62c6e31f9d49bf2ad5872952ed627ba8326f3f712b45939261733971dbb4c667dc64caab32b7cdbe7f7de210f13c751699db36993537.css integrity="sha512-sHjdEylo507wfmLG4x+dSb8q1YcpUu1ie6gybz9xK0WTkmFzOXHbtMZn3GTKqzK3zb5/feIQ8Tx1FpnbNpk1Nw==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>一致性hash负载均衡方案的思考 - MySpace</title><meta name=description content="本文从一致性hash负载均衡策略触发，深入了解了下不同的方案变体，最后针对普适应更好的一致性hash方案、go-zero实现、实践中的引用改进进行了探索介绍。"><link rel=canonical href=/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="一致性hash负载均衡方案的思考"><meta property="og:description" content="本文从一致性hash负载均衡策略触发，深入了解了下不同的方案变体，最后针对普适应更好的一致性hash方案、go-zero实现、实践中的引用改进进行了探索介绍。"><meta property="og:url" content="/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/"><meta property="og:site_name" content="MySpace"><meta property="article:published_time" content="2022-08-06T17:16:57+08:00"><meta property="article:modified_time" content="2022-08-06T17:16:57+08:00"><meta property="og:image" content="/doks.png"><meta property="og:image:alt" content="MySpace"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@hitzhangjie"><meta name=twitter:creator content="@hitzhangjie"><meta name=twitter:title content="一致性hash负载均衡方案的思考"><meta name=twitter:description content="本文从一致性hash负载均衡策略触发，深入了解了下不同的方案变体，最后针对普适应更好的一致性hash方案、go-zero实现、实践中的引用改进进行了探索介绍。"><meta name=twitter:image content="/doks.png"><meta name=twitter:image:alt content="一致性hash负载均衡方案的思考"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"/#/schema/person/1","name":"","url":"/","sameAs":["https://twitter.com/hitzhangjie","https://www.linkedin.com/in/hitzhangjie/","https://github.com/hitzhangjie"],"image":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/\u003cnil\u003e","width":null,"height":null,"caption":""}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"MySpace","description":"MySpace is a hitzhangjie\u0027s personal space, for blogs, books, journey, thinkings.","publisher":{"@id":"/#/schema/person/1"}},{"@type":"WebPage","@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/","url":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/","name":"一致性hash负载均衡方案的思考","description":"本文从一致性hash负载均衡策略触发，深入了解了下不同的方案变体，最后针对普适应更好的一致性hash方案、go-zero实现、实践中的引用改进进行了探索介绍。","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/person/1"},"datePublished":"2022-08-06T17:16:57CET","dateModified":"2022-08-06T17:16:57CET","breadcrumb":{"@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/#/schema/image/2"},"inLanguage":"","potentialAction":[{"@type":"ReadAction","target":["/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/"]}]},{"@type":"BreadcrumbList","@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/blog2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"/#/schema/article/1","headline":"一致性hash负载均衡方案的思考","description":"本文从一致性hash负载均衡策略触发，深入了解了下不同的方案变体，最后针对普适应更好的一致性hash方案、go-zero实现、实践中的引用改进进行了探索介绍。","isPartOf":{"@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/"},"mainEntityOfPage":{"@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/"},"datePublished":"2022-08-06T17:16:57CET","dateModified":"2022-08-06T17:16:57CET","author":{"@id":"/#/schema/person/2"},"publisher":{"@id":"/#/schema/person/1"},"image":{"@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"/#/schema/person/2","name":null,"sameAs":[]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/blog/2022-08-06-%E4%B8%80%E8%87%B4%E6%80%A7hash%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/#/schema/image/2","url":"/doks.png","contentUrl":"/doks.png","caption":"一致性hash负载均衡方案的思考"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest><script type=text/javascript src="https://platform-api.sharethis.com/js/sharethis.js#property=607868a58d7101001829a8df&product=sop" async></script><style>[alt~=sharing]{border:0;box-shadow:none}div#st-1{text-align:unset}div#st-1 .st-btn{height:24px;padding:0 4px}div#st-1 .st-btn>img{top:4.2px}div#st-2 .st-btn{height:24px;padding:0 4px}div#st-2 .st-btn>img{top:4.2px}</style><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168027530-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-168027530-1')</script></head><body class="blog single d-flex flex-column min-vh-100"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=MySpace>MySpace</a>
<button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>MySpace</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/blog/>Blog</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/books/>Books</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"><li class=nav-item><a class="nav-link social-link" href=https://twitter.com/hitzhangjie><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-twitter"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg><small class="ms-2 d-lg-none">Twitter</small></a></li><li class=nav-item><a class="nav-link social-link" href=https://github.com/hitzhangjie><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-lg-none">GitHub</small></a></li></ul></div></div></nav></header></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar"><nav class=docs-links aria-label="Main navigation"><h3>Tag List</h3><ol><li><a href=/tags/hash/>hash</a></li><li><a href=/tags/consistent-hash/>consistent hash</a></li><li><a href=/tags/loadbalance/>loadbalance</a></li><li><a href=/tags/rendezvous-hash/>rendezvous hash</a></li><li><a href=/tags/jump-consistent-hash/>jump consistent hash</a></li><li><a href=/tags/consistent-hash-with-bounded-load/>consistent hash with bounded load</a></li><li><a href=/tags/multi-probe-consistent-hash/>multi-probe consistent hash</a></li></ol></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#常见的负载均衡策略>常见的负载均衡策略</a></li><li><a href=#调研一致性hash策略及其可替代方案>调研一致性hash策略及其可替代方案</a><ul><li><a href=#余数hash>余数hash</a></li><li><a href=#一致性hash>一致性hash</a><ul><li><a href=#简要介绍>简要介绍</a></li><li><a href=#关于实现>关于实现</a></li></ul></li><li><a href=#一致性hash替代方案rendezvous-hashing>一致性hash替代方案：Rendezvous hashing</a></li><li><a href=#一致性hash变体jump-consistent-hash>一致性hash变体：jump consistent hash</a></li><li><a href=#一致性hash变体consistent-hash-with-bounded-load>一致性hash变体：consistent hash with bounded load</a></li></ul></li><li><a href=#参考资料>参考资料</a><ul><li><a href=#参考文献>参考文献：</a></li><li><a href=#实际应用>实际应用：</a></li></ul></li><li><a href=#影响一致性hash评估结果的因素>影响一致性hash评估结果的因素</a></li><li><a href=#一致性hash实际测试结果>一致性hash实际测试结果</a></li><li><a href=#go-zero中一致性hash实现源码阅读>go-zero中一致性hash实现源码阅读</a><ul><li><a href=#定义>定义</a></li><li><a href=#addaddwithreplicasaddwithweight>Add/AddWithReplicas/AddWithWeight</a></li><li><a href=#get>Get</a></li><li><a href=#继续优化>继续优化</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>一致性hash负载均衡方案的思考</h1><div style=display:flex><div>分享:&nbsp;&nbsp;</div><div><div class=sharethis-inline-share-buttons></div></div></div><hr><p class=lead></p><h1 id=常见的负载均衡策略>常见的负载均衡策略 <a href=#%e5%b8%b8%e8%a7%81%e7%9a%84%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e7%ad%96%e7%95%a5 class=anchor aria-hidden=true>#</a></h1><p>客户端完成被调服务的服务发现后，获得了一批实例节点列表，现在要借助合适的负载均衡算法来选择一个实例完成请求处理。</p><p>常见的负载均衡算法包括：</p><ul><li>轮询：每一次网络请求按照顺序发放给下节点列表中的下一个节点，这种情况适用于节点配置相同并且平均服务请求相对均衡的情况</li><li>加权轮询：考虑了不同节点的硬件配置情况，如节点a、b、c性能有低到高，权重设置为1、3、6，则按照权重分配10%、30%、60%的请求给到节点，这种可以避免高性能机器负载低、避免性能差机器过载</li><li>随机：随机选择一个节点来处理请求，这种在请求量比较大的情况下能达到相对均衡的分布，同样适用于机器配置相同的情况</li><li>加权随机：考虑了不同节点的硬件配置情况，类似加权轮询，只不过选择下一个节点时是基于随机选择，而非轮询的方式</li><li>余数hash：根据某个key对节点数做取模运算，比如节点数为n，根据请求中的m = uid % n，表示用节点列表中第m个节点来作为服务节点。当key分布范围比较广能达到相对均衡，选择key字段的时候要考虑下key分布情况。使用hash的场景，一般是因为后端节点有状态可复用（或者希望借此减少并发冲突），但真实环境中，节点故障是常态，尤其是在容器化部署场景下自动化扩缩容，hash会导致集群中所有节点状态无法被复用。一般会用一致性hash代替hash。</li><li>一致性hash：一致性hash是对hash的优化，一致性这里强调的就是节点加入、离开后尽量保证大多数请求仍然能路由到该路由的节点，而不是新加入的节点，同时为了避免新加入、离开节点导致的负载不均衡问题，引入了虚拟节点的概念，每个物理节点都对应着hash换上一定数量的虚拟节点，这些节点混在一起，能实现各个节点负载的相对均衡。节点数量该选择多少呢？一个比较直观的认识是可能虚拟节点越多越均衡，但是数量过多也会有开销，这与虚拟节点的hash计算、存储有关系，本文后面讨论。</li><li>按响应速度：有些负载均衡设备，会根据与后端服务节点间的ping延时来选择一个响应时间最短的。类似的也可以根据client、server之间的ping延时或者请求处理响应时间来选择。</li><li>按最少连接数：对于某些请求处理时间比较长的场景，如ftp传输等，一个tcp连接存在的时间可能比较长，连接数比较多的可能代表该节点负载比较重，因此会开率选择连接数比较少的来提供服务。</li><li>其他</li></ul><p>负载均衡算法有很多，之所以这么多也是因为应用场景的差异，根据合适的场景选择适用的负载均衡算法。</p><h1 id=调研一致性hash策略及其可替代方案>调研一致性hash策略及其可替代方案 <a href=#%e8%b0%83%e7%a0%94%e4%b8%80%e8%87%b4%e6%80%a7hash%e7%ad%96%e7%95%a5%e5%8f%8a%e5%85%b6%e5%8f%af%e6%9b%bf%e4%bb%a3%e6%96%b9%e6%a1%88 class=anchor aria-hidden=true>#</a></h1><p>对一致性hash方案及其可替代方案进行调研、对比。</p><h2 id=余数hash>余数hash <a href=#%e4%bd%99%e6%95%b0hash class=anchor aria-hidden=true>#</a><a href=#余数hash class=anchor aria-hidden=true>#</a></h2><p>余数hash，简单讲就是那个key去算出hash值，然后对节点数量取模，m = hash(key) % n，用节点列表中的第m个节点去做请求处理。
如果节点数变化非常不频繁，或者说key remapping（rebalancing）过程中带来的开销不大、影响不大，那用余数hash也无所谓。</p><p>但是现实场景中，比如一些有状态服务，如果remapp后映射到了与先前不同的节点，或者容器化部署时节点数经常变更，不满足适用余数hash的条件。</p><p>比较常见的对策，就是采用一致性hash。</p><h2 id=一致性hash>一致性hash <a href=#%e4%b8%80%e8%87%b4%e6%80%a7hash class=anchor aria-hidden=true>#</a><a href=#一致性hash class=anchor aria-hidden=true>#</a></h2><h3 id=简要介绍>简要介绍 <a href=#%e7%ae%80%e8%a6%81%e4%bb%8b%e7%bb%8d class=anchor aria-hidden=true>#</a><a href=#简要介绍 class=anchor aria-hidden=true>#</a></h3><p>一致性hash能够缓解节点加入、离开时rebalancing导致的一些hash节点改变的问题，在以下场景中就更有优势：</p><ul><li><p>服务是有状态的，这样大多数路由仍然能路由到原来的节点，状态可以复用；</p></li><li><p>即使服务不是有状态的，将原来路由到节点n的请求及其后续请求继续路由到该节点，也可能存在更好的局部性处理（locality），</p><blockquote><p>举个例子（可能不很恰当哈）：
比如有个个人展示页要展示头像昵称、最近游戏记录，假设之前有个什么客户端请求uid=xxx的请求路由到了节点n拉取过了昵称头像并cache，后面该展示页也路由到该节点的话就可以复用该cache。</p></blockquote></li></ul><p>假设key空间中值数量为k，节点数为n，那么当发生remapping时，笼统估算有k/n不命中原来的节点。</p><h3 id=关于实现>关于实现 <a href=#%e5%85%b3%e4%ba%8e%e5%ae%9e%e7%8e%b0 class=anchor aria-hidden=true>#</a><a href=#关于实现 class=anchor aria-hidden=true>#</a></h3><p>关于一致性hash的实现：</p><ul><li>构建一个一致性hash环，一个数组就可以实现</li><li>选定节点的key，如ip，hash(key)，然后再hash换上对应位置存储该节点信息，考虑到hash环大小需要适当取模</li><li>考虑到各节点的负载平衡，引入虚节点，每个物理节点对应为k各虚节点（k多大？），各个虚节点的hash值计算就分不同方法：<ul><li>key多大？兼顾计算效率和负载均衡性，因为节点数提前无法预估，可能要选择一个更好的经验值</li><li>引入k个hash函数，hash1(key), hash2(key), hash3(key)&mldr;.hashK(key)，分别设置到hash环上</li><li>针对key，构造key_1, key_2, key_3..，keyK，使用同一个hash函数分别计算上述key的hash，并在hash环上设置其节点信息</li><li>TODO 这里的计算方式的选择，虚节点数多大（过少还是会不均衡），过大计算效率慢（多次计算hash），另外多个hash还是构造多个key也可能会影响到负载的均衡性，需要针对性的测试。</li></ul></li><li>现在有个请求，比如用玩家userid作key，hash(key)得到值之后，因为一致性hash环是个收尾相接的有序数组实现的，可通过二分查找（查找第一个大于等于该<code>hash(key)</code> )的节点，复杂度O(logn)</li></ul><p>一致性hash，对于带权重的也能支持到：比如a机器比b机器性能高一倍，希望其处理两倍于b的请求，那么就可以让a机器的虚节点多一倍。但是如果管理的节点数量成千上万之后，hash环上存储这些虚节点的开销就不能忽略了。</p><h2 id=一致性hash替代方案rendezvous-hashing>一致性hash替代方案：Rendezvous hashing <a href=#%e4%b8%80%e8%87%b4%e6%80%a7hash%e6%9b%bf%e4%bb%a3%e6%96%b9%e6%a1%88rendezvous-hashing class=anchor aria-hidden=true>#</a><a href=#一致性hash替代方案rendezvous-hashing class=anchor aria-hidden=true>#</a></h2><p>Rendezvous hashing，也叫Highest Random Weight hashing。它比一致性hash提出来早一年，用了一种不同的方式来解决余数hash中key remapping的问题，也能实现一致性hash中 “需要remmap的keys数量=k/n” 的这种效果。</p><p>它是怎么做的呢？将请求key和机器节点的key（比如ip），合在一起做hash（不像一致性hash那样分开做hash），然后选择hash值最大的那个机器节点。</p><pre><code class=language-go>type router struct {
  endpoints []*Endpoint
}

func (r *router) Get(key string) *Endpoint {
  var ep *Endpoint
  hashVal := -INF  for _, e := range r.endpoints {
    h = hash(key, e)
    if h &gt; hashVal {
      ep = e
      hashVal = h
    }
  }
  return ep
}
</code></pre><p>这种方案的最大问题是O(n)的计算复杂度，一致性hash是O(logn)查找复杂度，不过如果节点数不是很多的话，这个开销可以接受。</p><p>ps：测试了下，rendezvous hash到各个节点一次记load+1，那么100w请求时，各节点load负载标准差387，最大、最小节点负载占总负载（100w）比例为1/1000。</p><p>go-zero实现的经典的一致性hash算法，虚节点数量100个，默认的hash函数（不一致哈），100w请求时，各节点负载标准差1w+，最大、最小节点负载占总负载（100w）比例为5/100。</p><h2 id=一致性hash变体jump-consistent-hash>一致性hash变体：jump consistent hash <a href=#%e4%b8%80%e8%87%b4%e6%80%a7hash%e5%8f%98%e4%bd%93jump-consistent-hash class=anchor aria-hidden=true>#</a><a href=#一致性hash变体jump-consistent-hash class=anchor aria-hidden=true>#</a></h2><p>相比传统的环形一致性哈希，空间复杂度更低，根本无内存占用，而且算法非常简洁，C++的实现不到10行代码就可以搞定。</p><pre><code class=language-c>int32_t JumpConsistentHash(uint64_t key, int32_t num_buckets) {
    int64_t b = -1, j = 0;
    while (j &lt; num_buckets) {
        b = j;
        key = key * 2862933555777941757ULL + 1;
        j = (b + 1) * (double(1LL &lt;&lt; 31) / double(key &gt;&gt; 33) + 1);
    }
    return b;
}
</code></pre><p>但是jump consistent hash存在它的局限性，使用场景受限：</p><ul><li>服务器名不是任意的，而是按照数字递增，它更适合应用于数据存储场景，如随着时间增长、数据量变化有创建出更多的shards之类的场景。</li><li>jump consistent hash只能在节点列表末端增加、删除节点，不能从中间任意删除节点，所以才说它适合用于存储类场景，比如数据容量大了，我们增加一个shard，或者说一个中间的shard崩溃了我们通过replicas复制来应对等。</li></ul><p><strong>在rpc场景下，后面任意一个节点都可能故障，我们需要从节点列表中删除任意一个节点的灵活性，所以说jump consistent hash不适用。</strong></p><h2 id=一致性hash变体consistent-hash-with-bounded-load>一致性hash变体：consistent hash with bounded load <a href=#%e4%b8%80%e8%87%b4%e6%80%a7hash%e5%8f%98%e4%bd%93consistent-hash-with-bounded-load class=anchor aria-hidden=true>#</a><a href=#一致性hash变体consistent-hash-with-bounded-load class=anchor aria-hidden=true>#</a></h2><p>这里的bounded load是啥意思呢？也是为了保证集群中各个节点的负载相对均衡，怎么做到呢，一个简单的思路就是：<strong>返回一个可以处理这个key的负载还ok的节点</strong>。</p><p><strong>1. 返回一个能处理这个key的节点，怎么理解呢？</strong></p><p>还是根据经典一致性hash的思路，计算key的hash从一致性hash环上找到第一个>=这个hash的虚节点，然后找到对应的物理节点信息。按经典一致性hash算法，此时就准备返回了。但是这里的方案变体还有其他事情要考量。</p><p>ps：在这个方案变体，一致性是要考虑的，但是负载均匀也是要考虑的，而且重视程度更重。经典一致性hash算法中，无论我们怎么设置虚节点数量、选择hash函数，包括给性能高的物理节点分配更多看似合理的虚节点等等。总有可能会出现负载不均衡的情况，负载均衡是一个理想值。我们在跑测试的时候也可以看到节点的最大、最小负载（hash一次load+1）相差很明显。怎么针对负载做优化呢？</p><p><strong>2、如何做到负载相对均匀?</strong></p><p>假设我们规定，返回一个节点时更新这个节点的load（load+1）、同时更新总的totalload，这样我们就能计算各个节点的avg load。如果第一步中待返回的load超过了avg load，我们就不返回该节点，而是从当前hash环当前虚节点位置继续向下遍历，直到找到下一个负载小于avg load的节点。</p><p><strong>有没有两全其美的方案？</strong></p><p>简单对比下，经典的一致性hash 及 jump一致性hash：</p><ul><li>ring-based consistent hash，以较大内存为代价，提供了增删任意node的灵活性，但是呢它的负载不够均衡。经典的实现里各个节点的负载是有偏差的，这给我们进行系统容量评估带来了些挑战，除非我们把虚节点加大，比如1000、2000。</li><li>jump consistent hash，以极低的内存消耗，提供了高效的负载均衡，负载均衡均匀性也比较好，但是损失的是灵活增删节点的灵活性，这导致它在存储类shards路由场景中比较适用，其他场景则不适用。</li></ul><p><strong>那有没有两全其美的方案呢？（实际上没有）</strong></p><ul><li>Multi-Probe Consistent Hash（简写为MPCH），就是为了解决这里的问题的，也是google提出的。<ul><li>优点：它支持O(n)的空间复杂度（胜过ring-based一致性hash），支持O(1)的插入、删除时间复杂度（胜过jump一致性hash），支持增删任意节点（胜过jump一致性hash）</li><li>缺点：它的查询复杂度下降了，假设我们追求的均匀性，比方说负载的peak-to-mean为1.05%，那么需要做21轮hash（有公式可以算，略），
            达到相同负载偏差，ring-based一致性hash需要700*ln(n)，n为100个节点时hash环存储时就要1m内存。</li></ul></li><li>Maglev Hash方案，Maglev是google的网络负载均衡器，内部也用了一致性hash方案，我们简称maglev hash方案。maglev在google类似我司tgw这层，通过vip转发外部数据包给内部服务器时，希望尽量复用以前的tcpconn并在后端节点变化时做最少机器迁移：<ul><li>优点：和ring-based一致性hash和rendezvous hash方案比，有不错的低内存开销、查询速度</li><li>缺点：maglev hash依赖一张查询表，当后端节点出现失败时构建这个查询表开销比较大，这也限制了后端节点的最大数量。</li></ul></li></ul><p><strong>我们期望的完美的hash方案应该是什么样的?</strong></p><p>调研了这些hash方案后，我们希望有这样的完美的hash方案：</p><ul><li>Only 1/n percent of the keys would be remapped on average where n is the number of nodes.</li><li>A O(n) space complexity where n is the number of nodes.</li><li>A O(1) time complexity per insertion/removal of a node and per key lookup.</li><li>A minimal standard deviation to make sure that a node is not overloaded compared to another one.</li><li>It would allow associating a weight to a node to cope with different node sizing.</li><li>It would allow arbitrary nodes name (not numbered sequentially) to support both load balancing and sharding.</li></ul><p>但是实际情况是，没有这样完美的hash方案!</p><ul><li>Rendezvous has a linear time complexity per lookup.</li><li>Ring consistent hash has a poor minimal standard deviation without the concept of virtual nodes. With virtual nodes, is space complexity is O(n*v) with n the number of nodes and v the number of virtual nodes per node.</li><li>Jump consistent hash does not have a constant time complexity and it does not support arbitrary nodes name.</li><li>Multi-Probe Consistent Hash也存在问题，虽然空间、时间、灵活性不错，但是查询效率大大下降了</li></ul><p>其实还有很多hash方案，它们都极力去<strong>平衡“一致性”和“均匀性”</strong>，但是实际情况就是没有完美的可以适用于所有场景的方案，下面是个hash方案的对比（<strong>展示了随着shards数增加查询的耗时 nanoseconds</strong>）：</p><p><img src=https://miro.medium.com/max/1400/1*fl7F4cFSXEcFilGt5-NvFw.png alt="benchmark with consistent hash and alternatives"></p><p><strong>除了单次查询耗时，其实还需要考虑内存开销、构建开销、插入删除节点开销、最大支持节点数等，没有完美的方案。</strong></p><p>所以，我们只能结合实际场景进行各种“<strong>权衡</strong>”，这也是为什么<strong>一致性hash方案尽管负载偏差比较差，但是它目前仍然应用范围比较广的原因，因为它对大多数场景都还ok</strong>。</p><p>## 负载均衡最大努力交付</p><p>现在回到我们现在的mesh框架的负载均衡场景，我们再重新评估下我们关切的点：</p><ul><li>节点选择的一致性</li><li>节点负载的均匀性</li><li>尽最大努力交付</li></ul><p><strong>现在只考虑ring-based一致性hash方案，它好理解、适用范围更广，而且可以结合ring值域、key值域、虚节点数、hash函数选择来做些优化来满足需要：</strong></p><ul><li>一致性：根据理论值如果节点数n，那么新加入一个节点最多迁移1/n</li><li>均匀性：通过增加虚节点数量，hash函数也比较好，那么也可以改善均匀性，且能在我们接受范围内，ring占用的内存空间在可接受范围内</li><li>尽最大努力交付：如果选中的一个节点，是一个失败的节点，我们可以借助重试（replication），使用hash环选择第2个或更多个节点出来供使用，howto?</li></ul><p>ring-based一致性hash，最大努力交付howto？</p><ul><li>比如，hash出的一个节点，是一个失败的节点，直接取hash环上这个节点的下一个节点（不能是相同的物理节点），这种好实现点，虚节点记录下在环上的位置即可</li><li>比如，借鉴一些存储系统replication的思路，允许取出多个节点</li></ul><h1 id=参考资料>参考资料 <a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 class=anchor aria-hidden=true>#</a></h1><h2 id=参考文献>参考文献： <a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=anchor aria-hidden=true>#</a><a href=#参考文献 class=anchor aria-hidden=true>#</a></h2><ul><li>介绍一致性hash，https://itnext.io/introducing-consistent-hashing-9a289769052e</li><li>redezvous hash，https://medium.com/i0exception/rendezvous-hashing-8c00e2fb58b0</li><li>经典一致性hash算法paper：Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</li><li>jump一致性hash算法paper：A Fast, Minimal Memory, Consistent Hash Algorithm</li><li>jump一致性hash算法paper推导：https://zhuanlan.zhihu.com/p/104124045</li><li>一致性hash算法tradeoff：https://dgryski.medium.com/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8</li><li>Multi-Probe一致性hash算法：https://arxiv.org/abs/1505.00062</li><li>一致性hash方案tradeoffs：https://itnext.io/introducing-consistent-hashing-9a289769052e</li><li>Maglev hash方案，https://blog.acolyer.org/2016/03/21/maglev-a-fast-and-reliable-software-network-load-balancer/</li></ul><h2 id=实际应用>实际应用： <a href=#%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8 class=anchor aria-hidden=true>#</a><a href=#实际应用 class=anchor aria-hidden=true>#</a></h2><ul><li>dapr采用了google consistent hash with bounded load, https://cloud.tencent.com/developer/article/1799300?from=article.detail.1340095</li><li>go-zero rpc框架采用了经典的一致性hash算法</li><li>twitter eventbus采用了rendezvous hash (最大随机权重hash）</li><li>memcache client采用了jump consistent hash, https://sourcegraph.com/github.com/grafana/loki/-/blob/pkg/storage/chunk/cache/memcached_client.go?L100</li><li>go-redis client默认采用了rendezvous hash，https://sourcegraph.com/github.com/go-redis/redis@v8/-/blob/ring.go?L39</li></ul><h1 id=影响一致性hash评估结果的因素>影响一致性hash评估结果的因素 <a href=#%e5%bd%b1%e5%93%8d%e4%b8%80%e8%87%b4%e6%80%a7hash%e8%af%84%e4%bc%b0%e7%bb%93%e6%9e%9c%e7%9a%84%e5%9b%a0%e7%b4%a0 class=anchor aria-hidden=true>#</a></h1><p>我们主要关注负载均衡算法的 ”一致性“、”均匀性“ 这两点，我们的测试也围绕着这两点展开。为了使得测试更有价值，更有可信度，需要说明下接下来的测试方案。</p><p><strong>影响各负载均衡算法测试结果的，可能有以下几点：</strong></p><ol><li><p>服务物理节点数，固定为10</p><ul><li>涉及到一致性hash算法及其变体时，我们会分别对比虚节点数为5、10、20、50、100、1000时的数量（直观理解，虚节点越多越均匀，内存开销可能越大）</li><li>jump一致性hash，虚节点数量对内存没影响，但是该算法使用场景受限于节点只add不减少的场景（如只增加shards的存储场景）</li><li>rendezvous hash，非一致性hash，没有虚节点概念，但是时间复杂度从一致性hash的O(logn)下降到O(n)，当节点数很多时开销不可忽视</li></ul></li><li><p>待测试的userid数量足够多（将userid mapping到物理节点上去处理），要远多于节点数，比如100w</p></li><li><p>待测试的userid生成算法是否均匀，go标准库rand.Int()默认的source是均匀的，我们用这个方法来生成userid</p></li><li><p>各基于hash的负载均衡算法采用的hash函数是否一致，在从key计算hash value时，不一致的hash函数可能会导致分布不均匀，这样会导致难以评估各类负载均衡算法本身的差异性
可以把常见实现的代码clone下来，统一调节下hash函数来验证下，控制变量下。</p></li><li><p>一致性hash算法中，虚节点对应的hash value的计算，为了平衡负载均匀和开销，通常虚节点数量n可调整，这种情况下就没法按照经典一致性hash算法中那样提供n个hash函数了
一般是对物理节点host做下处理，比如加前缀1、2、3或者后缀9、10、11后表示虚节点，然后再用同一个hash函数做计算。
这种做法是否能让各个虚节点在ring上分布均匀呢？这个跟hash函数有关，但是直观感受是不见得能均匀。</p></li><li><p>用户userid在采用与hash(host)时相同的hash函数，userid的值域与其hash值，是否能在hash环上均匀分布呢？直观感受是，不见得。</p></li></ol><p>上述这些都是影响我们评估算法质量的影响因素，在进行测试对比时要多关注。</p><p><strong>另外，关于“一致性”方面，通过算法本身的理论描述是可以给出一个理论值的：</strong></p><ul><li>一致性hash，假设k个key，n个nodes，那么节点加入、离开后，需要remapping的key大约为k/n（均衡的前提下）</li><li>一致性hash with bounded load，这个虽然负载比较均衡，但是直观感受是“一致性”不如经典的“一致性hash”，因为它会在负载偏高时选择下一个节点</li><li>jump consistent hash，这个算法只考虑存储场景data shards增加的情况，我们可以先延迟测试这个</li><li>rendezvous hash，理论上来说，只要新加入的节点host不会导致hash(userid, host)最大，就对原来的userid没影响，但是有多少userid会受影响呢？待理解</li></ul><p>但是实际使用时到底怎么样，就跟key本身以及hash函数选择的优劣很有关系了。</p><h1 id=一致性hash实际测试结果>一致性hash实际测试结果 <a href=#%e4%b8%80%e8%87%b4%e6%80%a7hash%e5%ae%9e%e9%99%85%e6%b5%8b%e8%af%95%e7%bb%93%e6%9e%9c class=anchor aria-hidden=true>#</a></h1><ul><li>选择一个一致性hash实现，比如采用go-zero的一致性hash实现</li><li>测试一致性，这个有算法理论支撑，我们其实可以不用测试</li><li>测试均匀性，这个有必要亲自测试下</li><li>修改以支持最大努力交付，比如失败之后该如何重试，以go-zero中定义的一致性hash环为例：<ul><li>先计算key从consistenthash.keys找到下一个虚节点的hashvalue，然后从ring[hashvalue]得到nodes，遍历这些nodes看是否有可用节点</li><li>上面不成，直接索引值+1顺着consistenthash.keys找下一个索引位置的hashvalue，再从ring[hashvalue]得到nodes，遍历看是否有可用节点，</li><li>如果转了一圈了还没有合适的，就应该退出了，当然也可以限制最大重试次数</li><li>另外要注意，之前遍历到的失败的节点，下次从虚节点找到对应物理节点时，应检查物理节点是否是已经排除过的，是的话就没必要重试了。</li></ul></li></ul><h1 id=go-zero中一致性hash实现源码阅读>go-zero中一致性hash实现源码阅读 <a href=#go-zero%e4%b8%ad%e4%b8%80%e8%87%b4%e6%80%a7hash%e5%ae%9e%e7%8e%b0%e6%ba%90%e7%a0%81%e9%98%85%e8%af%bb class=anchor aria-hidden=true>#</a></h1><h2 id=定义>定义 <a href=#%e5%ae%9a%e4%b9%89 class=anchor aria-hidden=true>#</a><a href=#定义 class=anchor aria-hidden=true>#</a></h2><pre><code class=language-go>type ConsistentHash struct {
hashFunc Func
replicas int
keys     []uint64
ring     map[uint64][]interface{}
nodes    map[string]lang.PlaceholderType
lock     sync.RWMutex
}
</code></pre><ul><li>hashFunc是自定义的hash函数</li><li>replicas表示每个物理节点对应的虚节点数量</li><li>keys其实就是一致性hash环的表示，记录了虚节点对应的hash值，有序</li><li>ring其实是hash值到一组虚节点的映射，它其实是为了解决hash冲突来的<ul><li>准确地说，keys+ring构成了一致性hash环，查找hash(key)对应的虚节点时，先在keys中找到>=hash(key)的虚节点对应的hash值，</li><li>然后，通过hash值到ring中找对应的虚节点</li><li>因为可能有冲突，所以map[k]v这里的v是一个slice</li></ul></li><li>nodes中记录了当前添加了哪些物理节点，但是这里的map[k]v，k是节点描述信息，可以简单理解成node.String()，v是struct{}</li></ul><p>这个一致性hash的设计还是不错的。</p><h2 id=addaddwithreplicasaddwithweight>Add/AddWithReplicas/AddWithWeight <a href=#addaddwithreplicasaddwithweight class=anchor aria-hidden=true>#</a><a href=#addaddwithreplicasaddwithweight class=anchor aria-hidden=true>#</a></h2><p>添加新节点的时候，大致就是这几个函数，逻辑是什么呢？</p><ul><li>先获取待添加节点的一个描述信息，如String()，然后记录到nodes中，表示记录了这个节点</li><li>然后呢，根据设置的虚节点数量，for循环，每次在描述信息后面添加数字后缀，计算hash，然后记录到keys、ring里面
前面提过了，keys+ring共同构成了一致性hash环</li></ul><h2 id=get>Get <a href=#get class=anchor aria-hidden=true>#</a><a href=#get class=anchor aria-hidden=true>#</a></h2><p>根据key获取节点的时候呢？</p><ul><li>先计算key的hash，然后从keys中找第一个>=hash(key)的位置，这个位置对应的hash即为候选虚节点的hash，</li><li>然后通过虚节点的hash去ring里面找，ring里面是个slice，是为了解决散列冲突的，</li><li>怎么从这个slice中取呢，重新hash一次，从这个slice里面选一个</li></ul><h2 id=继续优化>继续优化 <a href=#%e7%bb%a7%e7%bb%ad%e4%bc%98%e5%8c%96 class=anchor aria-hidden=true>#</a><a href=#继续优化 class=anchor aria-hidden=true>#</a></h2><p>ring-based一致性hash的均匀性还可以继续优化，比如从hash函数的选择方面，虚节点数量的选择方面。</p><p>以下是10个物理节点，均匀的100w userid，在不同虚节点数量、hash函数选择情况下的测试情况：</p><pre><code class=language-bash>case: replicas:50+hash:murmur3.Sum64 标准方差: 14628.560790453721  max: 126737  min: 76395 (max-min)/times: 0.050342 peak/mean: 1.26737
case: replicas:100+hash:murmur3.Sum64 标准方差: 14555.295022774357  max: 127129  min: 76438 (max-min)/times: 0.050691 peak/mean: 1.27129 *
case: replicas:200+hash:murmur3.Sum64 标准方差: 6902.00454940447  max: 110178  min: 85121 (max-min)/times: 0.025057 peak/mean: 1.10178
case: replicas:500+hash:murmur3.Sum64 标准方差: 2285.3205902017335  max: 105277  min: 97136 (max-min)/times: 0.008141 peak/mean: 1.05277
case: replicas:1000+hash:murmur3.Sum64 标准方差: 2069.765928794848  max: 104603  min: 97606 (max-min)/times: 0.006997 peak/mean: 1.04603
case: replicas:2000+hash:murmur3.Sum64 标准方差: 2618.900303562547  max: 104628  min: 94870 (max-min)/times: 0.009758 peak/mean: 1.04628

case: replicas:50+hash:xxhash.Sum64 标准方差: 8627.559643375409  max: 119229  min: 91110 (max-min)/times: 0.028119 peak/mean: 1.19229
case: replicas:100+hash:xxhash.Sum64 标准方差: 8918.29840272235  max: 120236  min: 90692 (max-min)/times: 0.029544 peak/mean: 1.20236 *
case: replicas:200+hash:xxhash.Sum64 标准方差: 5913.828556865679  max: 111947  min: 89811 (max-min)/times: 0.022136 peak/mean: 1.11947
case: replicas:500+hash:xxhash.Sum64 标准方差: 4256.551350565384  max: 107631  min: 93326 (max-min)/times: 0.014305 peak/mean: 1.07631
case: replicas:1000+hash:xxhash.Sum64 标准方差: 3148.5766943176086  max: 106134  min: 95150 (max-min)/times: 0.010984 peak/mean: 1.06134
case: replicas:2000+hash:xxhash.Sum64 标准方差: 1664.1786562746202  max: 103375  min: 96885 (max-min)/times: 0.00649 peak/mean: 1.03375

case: replicas:100+hash:crc32.ChecksumIEEE 标准方差: 16188.201024202783  max: 121890  min: 69629 (max-min)/times: 0.052261 peak/mean: 1.2189
case: replicas:200+hash:crc32.ChecksumIEEE 标准方差: 11440.727826497754  max: 126050  min: 82970 (max-min)/times: 0.04308 peak/mean: 1.2605 *
case: replicas:500+hash:crc32.ChecksumIEEE 标准方差: 17259.726985094523  max: 130659  min: 69507 (max-min)/times: 0.061152 peak/mean: 1.30659
case: replicas:1000+hash:crc32.ChecksumIEEE 标准方差: 21791.261533009052  max: 137256  min: 72892 (max-min)/times: 0.064364 peak/mean: 1.37256
case: replicas:2000+hash:crc32.ChecksumIEEE 标准方差: 12953.256825987819  max: 120299  min: 73664 (max-min)/times: 0.046635 peak/mean: 1.20299
</code></pre><p>不难看出： xxhash的均匀性首先比较好，在100个虚节点（这个一般是比较常用的经验值）时，最大最小负载偏差2.9%，peak/mean比为1.20</p><h1 id=总结>总结 <a href=#%e6%80%bb%e7%bb%93 class=anchor aria-hidden=true>#</a></h1><p>本文从一致性hash负载均衡策略触发，深入了解了下不同的方案变体，最后针对普适应更好的一致性hash方案、go-zero实现、实践中的引用改进进行了探索介绍。</p><div class=edit-page><a href=https://github.com/hitzhangjie/myspace/blob/master/content/blog/2022-08-06-%e4%b8%80%e8%87%b4%e6%80%a7hash%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e6%96%b9%e6%a1%88%e7%9a%84%e6%80%9d%e8%80%83.md><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-edit-2"><path d="M17 3a2.828 2.828.0 114 4L7.5 20.5 2 22l1.5-5.5L17 3z"/></svg>Edit this page on GitHub</a></div><div class="docs-navigation d-flex justify-content-between"><a href=/blog/2022-09-25-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88%E7%9A%84%E6%80%9D%E8%80%83/><div class="card my-1"><div class="card-body py-2">&larr; 分布式锁方案的思考</div></div></a><a class=ms-auto href=/blog/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%A3%80%E6%9F%A5/><div class="card my-1"><div class="card-body py-2">内核中的内存检查工具 &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted mt-auto"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a class=text-muted href=https://www.netlify.com/>Netlify</a>, <a class=text-muted href=https://gohugo.io/>Hugo</a>, and <a class=text-muted href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div><div class=col-lg-8 align=right><p><font size=-1>站点构建版本：v0.2.3</font></p></div></div></div></footer><script src=/js/bootstrap.min.fdbe9b9ba88a036135318f3c721784d684ac9e3280fe282cc80d7d49f7f9c82780cd54228c1608685a230c09984300d7946fc471734d566fcebc148b65bd16db.js integrity="sha512-/b6bm6iKA2E1MY88cheE1oSsnjKA/igsyA19Sff5yCeAzVQijBYIaFojDAmYQwDXlG/EcXNNVm/OvBSLZb0W2w==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.b64f1e7517e5839396950ceee4ef937fbbd3ff20aa1fdd261ce87fa457863404f35a6e5239dd57b20b37f39c2401b933deeef60af180195b16941c88f10e948d.js integrity="sha512-tk8edRflg5OWlQzu5O+Tf7vT/yCqH90mHOh/pFeGNATzWm5SOd1Xsgs385wkAbkz3u72CvGAGVsWlByI8Q6UjQ==" crossorigin=anonymous defer></script>
<script src=/main.min.f16ffd9b364013a1df84be9cd7a45c470cb48639766bf5551e05bba845fe4ab150cb6436de43609bfed9af47c23bd5e395e9a10b932fa6fb2c4ee8f6cc78d7a3.js integrity="sha512-8W/9mzZAE6HfhL6c16RcRwy0hjl2a/VVHgW7qEX+SrFQy2Q23kNgm/7Zr0fCO9XjlemhC5MvpvssTuj2zHjXow==" crossorigin=anonymous defer></script>
<script src=/index.min.3a5f103954273721d4b7666f38b9fb7cff911198d2f182fde191c07e6b8a2ffc08e9d8b3ba001af0751acf51f828167a83e8d61811805e49b60e707e0c59c09a.js integrity="sha512-Ol8QOVQnNyHUt2ZvOLn7fP+REZjS8YL94ZHAfmuKL/wI6dizugAa8HUaz1H4KBZ6g+jWGBGAXkm2DnB+DFnAmg==" crossorigin=anonymous defer></script></body></html>