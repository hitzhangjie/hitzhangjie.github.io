---
layout: post
title: "Linux任务调度(7)"
description: "前面几篇文章介绍了Linux下调度器的演进过程，本文在CFS基础上继续探讨几个比较实际的问题。当线程数多了之后，线程切换频率会上升吗？调度器又如何平衡多cpu多核上的负载。"
date: 2024-06-27 12:36:00 +0800
tags: ["scheduler","cfs","fair","vruntime","gmp"]
categories: ["linux内核"]
toc: true
reward: true
---
## Linux内核CFS调度器核心概念与逻辑总结

以 v5.12 版本内核为例进行说明。

### 核心概念

1. **完全公平调度(Completely Fair Scheduling, CFS)**

   - 目标是为所有任务提供公平的CPU时间分配，甚至还支持不同层级的公平（通过组调度来实现）
   - 取代了传统的O(1)调度器，成为Linux默认的进程调度器，尽管它不完美也已经相当优秀了
2. **虚拟运行时间(vruntime)**

   - sched_entity：每个任务维护一个vruntime值，记录其"虚拟"运行时间。对于组调度的话，这个组也有vruntime。实际上都是通过sched_entity来维护
     ```c
      struct sched_entity {
        	/* For load-balancing: */
        	struct load_weight	load;
        	struct rb_node			run_node;
        	struct list_head		group_node;
        	unsigned int			on_rq;
        
        	u64				exec_start;
        	u64				sum_exec_runtime;
        	u64				vruntime;
        	u64				prev_sum_exec_runtime;
        
        	u64				nr_migrations;
        
        	struct sched_statistics		statistics;
        
        #ifdef CONFIG_FAIR_GROUP_SCHED
        	int				depth;
        	struct sched_entity		*parent;
        	/* rq on which this entity is (to be) queued: */
        	struct cfs_rq			*cfs_rq;
        	/* rq "owned" by this entity/group: */
        	struct cfs_rq			*my_q;
        	/* cached value of my_q->h_nr_running */
        	unsigned long			runnable_weight;
        #endif
        
        #ifdef CONFIG_SMP
        	/*
        	 * Per entity load average tracking.
        	 *
        	 * Put into separate cache line so it does not
        	 * collide with read-mostly values above.
        	 */
        	struct sched_avg		avg;
        #endif
    };
    ```
   - 考虑任务优先级：通过查表将nice值（静态优先级）转换为静态权重，然后再通过动态优先级计算额外的附加权重，得到总的权重
   - 公式：vruntime += delta_exec × (NICE_0_LOAD / weight)，即虚拟运行时间=真实运行时间*权重系数

   ```c
   /*
    * delta /= w
    */
   static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
   {
       if (unlikely(se->load.weight != NICE_0_LOAD))
   	    delta = __calc_delta(delta, NICE_0_LOAD, &se->load);
       return delta;
   }

   curr->vruntime += calc_delta_fair(delta_exec, curr);
   ```
3. **红黑树(RB-Tree)**

   - 用于管理可运行任务，按vruntime排序
   - 最左侧节点是vruntime最小的任务(最需要CPU时间)
4. **调度周期(sched_latency)**

   - 默认6ms，表示CFS尝试在这个时间窗口内为所有可运行任务提供公平运行机会
   - 计算公式：sched_latency = min(sysctl_sched_latency, nr_running × sysctl_sched_min_granularity)/*

     ```c
     /*
      * Targeted preemption latency for CPU-bound tasks:
      *
      * NOTE: this latency value is not the same as the concept of
      * 'timeslice length' - timeslices in CFS are of variable length
      * and have no persistent notion like in traditional, time-slice
      * based scheduling concepts.
      *
      * (to see the precise effective timeslice length of your workload,
      *  run vmstat and monitor the context-switches (cs) field)
      *
      * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds)
      */
     unsigned int sysctl_sched_latency			= 6000000ULL;

     /*
      * Minimal preemption granularity for CPU-bound tasks:
      *
      * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)
      */
     unsigned int sysctl_sched_min_granularity			= 750000ULL;

     /*
      * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity
      */
     static unsigned int sched_nr_latency = 8;

      * The idea is to set a period in which each task runs once.
      *
      * When there are too many tasks (sched_nr_latency) we have to stretch
      * this period because otherwise the slices get too small.
      *
      * p = (nr <= nl) ? l : l*nr/nl
      */
     static u64 __sched_period(unsigned long nr_running)
     {
     	if (unlikely(nr_running > sched_nr_latency))
     		return nr_running * sysctl_sched_min_granularity;
     	else
     		return sysctl_sched_latency;
     }
     ```

     容易看出，当可运行任务数<=8时，调度周期为6ms；当>8时，调度周期=可运行任务数*0.75ms。现在还需要为任务计算一个执行的时间片。
     进程执行完系统调用返回时、系统定时器中断服务程序执行时，会检查进程vruntime是否最小是否可切换？如果这样的话岂不是要频繁切换了。实际上是调度跟任务之前根据sched_slice算好了可以执行的时间片，然后调度到该任务执行的时候，任务执行期间遇到系统调用返回、定时器中断处理时，会先比较当前进程运行时间是否已经超过了刚才的时间片，如果是就准备执行切换。此时cfs调度器会选择下一个vruntime最小的任务来调度。

     ```c
     /*
      * We calculate the wall-time slice from the period by taking a part
      * proportional to the weight.
      *
      * s = p*P[w/rw]
      */
     static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se)
     {
     	u64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);

     	for_each_sched_entity(se) {
     		struct load_weight *load;
     		struct load_weight lw;

     		cfs_rq = cfs_rq_of(se);
     		load = &cfs_rq->load;

     		if (unlikely(!se->on_rq)) {
     			lw = cfs_rq->load;

     			update_load_add(&lw, se->load.weight);
     			load = &lw;
     		}
     		// slice = delta_exec * weight / lw.weight
     		slice = __calc_delta(slice, se->load.weight, load);
     	}
     	return slice;
     }
     ```
5. **调度粒度(sched_entity)**

   - 每个任务维护一个sched_entity，表示其调度粒度
   - 组调度，任务组、同一个用户等，可以创建对应的sched_entity，其中包含了粒度更小的sched_entity，比如包含了任务对应的entity
6. 调度节拍(scheduler_tick)

   ```c

   /*
    * This function gets called by the timer code, with HZ frequency.
    * We call it with interrupts disabled.
    */
   void scheduler_tick(void)
   {
   	int cpu = smp_processor_id();
   	struct rq *rq = cpu_rq(cpu);
   	struct task_struct *curr = rq->curr;
   	struct rq_flags rf;
   	unsigned long thermal_pressure;

   	arch_scale_freq_tick();
   	sched_clock_tick();

   	rq_lock(rq, &rf);

   	update_rq_clock(rq);
   	thermal_pressure = arch_scale_thermal_pressure(cpu_of(rq));
   	update_thermal_load_avg(rq_clock_thermal(rq), rq, thermal_pressure);

   	// task_tick -> task_tick_fair
   	curr->sched_class->task_tick(rq, curr, 0);
   	calc_global_load_tick(rq);
   	psi_task_tick(rq);

   	rq_unlock(rq, &rf);

   	perf_event_task_tick();

   #ifdef CONFIG_SMP
   	rq->idle_balance = idle_cpu(cpu);
   	trigger_load_balance(rq);
   #endif
   }

   /*
    * scheduler tick hitting a task of our scheduling class.
    *
    * NOTE: This function can be called remotely by the tick offload that
    * goes along full dynticks. Therefore no local assumption can be made
    * and everything must be accessed through the @rq and @curr passed in
    * parameters.
    */
   static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
   {
   	struct cfs_rq *cfs_rq;
   	struct sched_entity *se = &curr->se;

   	for_each_sched_entity(se) {
   		cfs_rq = cfs_rq_of(se);
         // tick sched_entity
   		entity_tick(cfs_rq, se, queued);
   	}

   	if (static_branch_unlikely(&sched_numa_balancing))
   		task_tick_numa(rq, curr);

   	update_misfit_status(curr, rq);
   	update_overutilized_status(task_rq(curr));
   }

   // 更新进程统计信息（运行时间等），检查是否需要抢占该进程，如果需要抢占则设置标记位之类的
   static void
   entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
   {
   	/*
   	 * Update run-time statistics of the 'current'.
   	 */
   	update_curr(cfs_rq);

   	/*
   	 * Ensure that runnable average is periodically updated.
   	 */
   	update_load_avg(cfs_rq, curr, UPDATE_TG);
   	update_cfs_group(curr);

   #ifdef CONFIG_SCHED_HRTICK
   	/*
   	 * queued ticks are scheduled to match the slice, so don't bother
   	 * validating it and just reschedule.
   	 */
   	if (queued) {
   		resched_curr(rq_of(cfs_rq));
   		return;
   	}
   	/*
   	 * don't let the period tick interfere with the hrtick preemption
   	 */
   	if (!sched_feat(DOUBLE_TICK) &&
   			hrtimer_active(&rq_of(cfs_rq)->hrtick_timer))
   		return;
   #endif

   	if (cfs_rq->nr_running > 1)
   		check_preempt_tick(cfs_rq, curr);
   }

   /*
    * Preempt the current task with a newly woken task if needed:
    * 1) 如果运行足够久了，超过了sched_slice返回的时间片（时间配额），则直接标记为抢占然后返回
    * 2) 如果运行任务时间很短，还不到最小的0.75ms，那么无需抢占、继续执行
    * 3) 如果运行时间超过了0.75ms，但是还小于sched_slice返回的时间片（时间配额），
    *    则从当前cpu的cfs_rq（任务队列）中取出vruntime最小的一个se，进行比较。
    *    如果se.vruntime比当前任务的小，则标记为抢占，否则继续执行。
    */
   static void
   check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
   {
   	unsigned long ideal_runtime, delta_exec;
   	struct sched_entity *se;
   	s64 delta;

   	ideal_runtime = sched_slice(cfs_rq, curr);
   	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
   	if (delta_exec > ideal_runtime) {
   		resched_curr(rq_of(cfs_rq));
   		/*
   		 * The current task ran long enough, ensure it doesn't get
   		 * re-elected due to buddy favours.
   		 */
   		clear_buddies(cfs_rq, curr);
   		return;
   	}

   	/*
   	 * Ensure that a task that missed wakeup preemption by a
   	 * narrow margin doesn't have to wait for a full slice.
   	 * This also mitigates buddy induced latencies under load.
   	 */
   	if (delta_exec < sysctl_sched_min_granularity)
   		return;

   	se = __pick_first_entity(cfs_rq);
   	delta = curr->vruntime - se->vruntime;

   	if (delta < 0)
   		return;

   	if (delta > ideal_runtime)
   		resched_curr(rq_of(cfs_rq));
   }
   ```
7. 任务抢占
   当前正在执行的任务，在时钟中断到来时schedule_tick函数被执行，它里面会检查当前任务是否应该被抢占，如果需要被抢占，则会调用resched_cur
   来标记任务的一些抢占标记为。检查当前任务是否运行足够久应该被抢占的函数，前面我们已经分析过了，就是check_preempt_tick函数。

   ```c
   /*
    * resched_curr - mark rq's current task 'to be rescheduled now'.
    *
    * On UP this means the setting of the need_resched flag, on SMP it
    * might also involve a cross-CPU call to trigger the scheduler on
    * the target CPU.
    */
   void resched_curr(struct rq *rq)
   {
   	struct task_struct *curr = rq->curr;
   	int cpu;

   	lockdep_assert_held(&rq->lock);

   	if (test_tsk_need_resched(curr))
   		return;

   	cpu = cpu_of(rq);

   	if (cpu == smp_processor_id()) {
   		set_tsk_need_resched(curr);
   		set_preempt_need_resched();
   		return;
   	}

   	if (set_nr_and_not_polling(curr))
   		smp_send_reschedule(cpu);
   	else
   		trace_sched_wake_idle_without_ipi(cpu);
   }
   ```
8. 调度时机
   注意前面的schedule_tick只是定时器触发调度器执行相关的检查，如检查当前执行中的任务是否应该被抢占，如果要被抢占就设置标记位返回。
   我们知道此时任务还是在执行中的，顶多被抢占设置了个标记位而已。那么调度器什么时候执行了真正的切换呢？

   ```c
   /*
    * __schedule() is the main scheduler function.
    *
    * The main means of driving the scheduler and thus entering this function are:
    *
    *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
    *
    *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
    *      paths. For example, see arch/x86/entry_64.S.
    *
    *      To drive preemption between tasks, the scheduler sets the flag in timer
    *      interrupt handler scheduler_tick().
    *
    *   3. Wakeups don't really cause entry into schedule(). They add a
    *      task to the run-queue and that's it.
    *
    *      Now, if the new task added to the run-queue preempts the current
    *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
    *      called on the nearest possible occasion:
    */
   static void __sched notrace __schedule(bool preempt)
   {
   	struct task_struct *prev, *next;
   	unsigned long *switch_count;
   	unsigned long prev_state;
   	struct rq_flags rf;
   	struct rq *rq;
   	int cpu;

   	cpu = smp_processor_id();
   	rq = cpu_rq(cpu);
   	prev = rq->curr;

      ...

      // 寻找下一个可执行任务
   	next = pick_next_task(rq, prev, &rf);
      // 清理旧任务的抢占标记位
   	clear_tsk_need_resched(prev);
   	clear_preempt_need_resched();

      // 如果next!=prev，更新上下文切换计数器，并执行上下文切换
      // 如果next==prev，说明当前runq上没有其他任务，需要检查下负载均衡了
   	if (likely(prev != next)) {
   		rq->nr_switches++;

         // 执行上下文切换，从prev到next，切换完就开始执行next这个任务了
   		rq = context_switch(rq, prev, next, &rf);
   	} else {
   		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

         // 负载均衡相关的操作，rq.lock dropped之后会执行balance_callbacks
   		rq_unpin_lock(rq, &rf);
   		__balance_callbacks(rq);
   		raw_spin_unlock_irq(&rq->lock);
   	}
   }
   ```
9. 负载均衡
   see: __balance_callbacks(rq)，因为我们了解go workstealing相关的迁移负载的做法，对linux cfs调度器做到负载均衡的思路并不好奇，
   我们这里直接跳过它。如果您感兴趣，可以看下相关的代码。

10. 组调度
   组调度，或者说一个一起调度的组，也是用sched_entity来表示这个组，然后组内又细分为具体的组或者直接就是任务，它们也是由sched_entity表示。
   这里其实已经不是cfs的设计实现部分了，而是考虑如何将cfs与cgroups控制相结合。
   即，在cgroups中增加哪些配置项来影响调度，如cpu.shares表示这个调度组的CPU开销的配额。
   对应地cfs调度器需要读取这里的这些配置项，来让sched_entity中的字段设置以及后续的各种优先级计算、权重计算、时间片计算、抢占逻辑的效果，
   能与cgroups中的配置对齐。
   ok，这里先不讲组调度，我们将另开一节进行介绍。

## 再来回答下面的问题

### 线程数多了之后，线程切换会更频繁吗? 不见得

讨论一个问题，进程数、线程数多了之后，CFS调度器下线程的切换频率是否会升高呢？

根据前面的介绍，任务切换的时机是定时器触发scheduler_tick->task_tick->check_preempt_tick，这里面会检查当前任务是否应该被抢占。
发生抢占才会发生上下文切换，而抢占的判断条件是：
- 当前任务运行时间足够久，超过了计算出的ideal_runtime，这个时间由sched_slice计算出，大致计算方式是: 
   ```
   u64 slice = __sched_period(cfs_rq->nr_running + !se->on_rq);
   slice = delta_exec * weight / lw.weight
   ```
  什么意思呢？就是，按照权重（优先级折算为了这里的权重）将一个调度周期的时长进行按比例瓜分，
  瓜分来的这部分时间就是运行当前任务的最大时间片，最小时间片是sysctl_sched_min_granularity=0.75ms。
  这里要看下sysctl_sched_min_granularity和sched_slice的大小关系了，通过sched_pierod的函数定义得知，
  实际上当运行任务数量<=8时，一个调度周期就是6ms；如果任务数量>8，那就=任务数量*0.75ms，意味着：超出sched_slice时长的任务基本上只有在任务数量<=8时才会出现，任务数量多时，每个任务运行超过0.75ms就会切换。

  所以可以得知，随着任务数量增多，调度周期变长，而每个任务的运行时间片基本上是确定的，就是0.75ms，所以上下文切换频率并不会变大，而是相对恒定的。
  这样设计其实是对的，可以避免过多的上下文切换开销带来的CPU资源的浪费，反而保证了吞吐量，只是可能牺牲了点调度的及时性，引入了一点延迟，但是这对于稳定性来说是值得的。

### CPU1上的P1的vruntime比CPU2上的P2的vruntime更小，CPU1负载高，会将P1迁迁移到CPU2上吗？

注意，尽管CFS调度器会为每个cpu维护独立的任务调度数据结构（rbtree），但是CFS调度器确实有多cpu的负载均衡机制。

假设有两个CPU（CPU1和CPU2），每个CPU有自己的调度队列和红黑树：

- CPU1上的任务P1的vruntime较大，暂时不被CPU1调度。
- CPU2上的任务P2的vruntime最小，但P1的vruntime比P2更小。

在这种情况下，是否会将P1迁移到CPU2取决于负载均衡机制的具体实现和当前系统的负载情况：

- 如果CPU1的负载较高，而CPU2的负载较低，负载均衡机制可能会将P1迁移到CPU2，以平衡负载。
- 如果CPU1和CPU2的负载相对均衡，调度器可能不会进行任务迁移，因为任务迁移本身也有一定的开销。

### cfs调度器负载均衡的时机，以及考虑因素

**负载均衡的时机:**
负载均衡通常在以下几种情况下触发：

1. 周期性负载均衡：调度器会定期检查各个CPU的负载，并在必要时进行任务迁移。
2. 任务唤醒：当一个任务从睡眠状态被唤醒时，调度器会检查当前CPU的负载情况，并可能将任务分配到负载较轻的CPU。
3. 任务创建：当一个新任务被创建时，调度器会选择一个负载较轻的CPU来运行该任务。

**任务迁移的考虑因素:**
在决定是否迁移任务时，调度器会考虑多个因素，包括：

1. CPU负载：调度器会比较各个CPU的负载，选择负载较轻的CPU进行任务迁移。
2. 任务的vruntime：调度器会比较任务的vruntime，选择合适的任务进行迁移。
3. 任务的亲和性：某些任务可能对特定的CPU有亲和性（例如，缓存亲和性），调度器会尽量避免迁移这些任务。

## 为何会想到这个问题呢

一个直接原因时因为go程序中GOMAXPROCS设置不合理，母机上有128个CPU核心，但是虚拟化技术下容器里分配的只有2个cpus。

此时go进程检测到GOMAXPROCS=128（go不会自动感知到实际上只分配了2个cpus），此时runtime会误认为最多可以创建128个P（GMP中的P，Processor），后果就是进程中最多会创建128个P。比如随着goroutines增多如果当前P处理不过来，就会激活更多的空闲P，对应的创建更多的线程M并轮询绑定的P上的的localrunq、全局的globalrunq以及定时器事件、网络IO事件就绪的goroutines并调度。

> ps: 严格来说，go运行时是这样创建GMP的
>
> 1. 进程启动的时候会根据GOMAXPROCS先创建出对应数量的P，详见schedinit()->procresize()，但是线程数M还是没有创建的
> 2. 上述创建出来的一堆P，除了当前g.m.p是在用状态，其他都是idle状态；M也不会预先创建出来，而是根据设计负载情况动态去创建、去激活P去执行的；
> 3. 具体来说就是当创建一堆goroutines后，这些goroutine会先往p.runq放，放不下了就会考虑injectglist，这个其实就是放到全局队列sched.runq，放的时候：
>    - 如果当前M有关联一个P，就先放npidle个G到sched.runq，并且启动npdile个M去激活npdile个P，去尝试从goroutine抢G然后执行。然后剩下的放到p.runq
>    - 如果当前M没有关联一个P，这种情况下怎么会发生呢（有多种情况可能会发生，比如GC、系统调用阻塞、初始化阶段等）？这种情况下会全部放到sched.runq，然后启动最多npidle个（即 `min(goroutineQSize, npdile)`）个M去激活P并执行；
>
> 有些细节就不过分展开，大家知道这一点就好了，“**如果短时间内创建大量goroutine，当前p.runq full就会往sched.runq放，并且会启动最多npidle个M去抢P执行。**”
>
> 如果这种情况出现了，并且GOMAXPROCS设置的不合理（如远大于虚拟化技术分配的CPU配额，如docker run --cpus=2，但是GOMAXPROCS=128），太大在运行时初始化时就会创建大量的P，尽管它此时可能没被激活，但是当后面goroutines创建太快时就会激活这些空闲P并创建大量的线程M，M在执行一些轮询 p.runq、sched.runq、netpoller、stealing、contextswitch 过程中就容易推高CPU占用，如果GOMAXPROCS 远大于 --cpus，那么这个CPU开销就更容易达到CPU配额限制，进而被虚拟化管理软件给节流（CPU throttling），进而导致程序性能出现整体性的下降 (程序逻辑还没怎么执行就没限制使用CPU了)。

这样的话就会个不好的影响：

前面的学习我们知道，即使M多了之后，CFS调度层面也不会因为线程数增加就导致切换频率增高。这里主要是go runtime的原因，线程数多了后会一直在那里轮询localp.runq, sched.globq, timers, netpoller, steal G from other P，这些轮询操作会消耗cpu、推高cpu占用。当然128个M肯定不算太多，实际上母机的核数可能比这个多的多，那问题就是会有更多的M被创建出来，go runtime空轮询的问题就会更明显。

我们由go服务中常见的问题联想到了CFS调度器这里的工作机制，进而引申出了对cfs调度器的一系列探索。我们知道了，随着线程数增加CFS不会导致上下文切换频率升高、不会浪费掉很多CPU资源。go服务GOMAXPROCS参数设置不当时，go runtime创建了一些线程，而这些线程轮询的轮询操作导致了cpu占用高、节流。ok，搞清楚这点。

go程序中解决这个问题，可以直接 `import _ "github.com/uber-go/automaxprocs"` 来解决。这不是一个新问题，只是思考了下和CFS调度器、goruntime调度的一点联系。
